{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction to Monte Carlo Methods",
   "id": "2016621e41575a5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Monte Carlo\n",
    "Monte Carlo Techniques are particularly useful in such situation where traditional deterministic approach are impractical or impossible to apply. Monte Carlo is computational technique which rely on random sampling to obtain complex numerical results. They are very commonly used across various fields of Physics, Finance, Gaming, Engineering, Chemistry etc.\n",
    "\n",
    "As said above, the Monte Carlo method approximates the expectation of a random variable from sampling. And, when the sampling size is greater, the approximate value becomes better. Say, X be the random variable and we need to compute the expected value of X, such that we compute the sum of values of X multiplied by the respective probabilities and is given, matematically, as:\n",
    "\n",
    "\\begin{equation}\n",
    "E(X) = \\sum_{i=1}^{N}{x_ip(x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "However, we are trying to approximate it using Monte Carlo method by sampling the values of X for some large number N times and compute the expected value of X as:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{x \\sim p(x)}[X] \\approx 1/N\\sum_{i=1}^{N}{x_i}\n",
    "\\end{equation}\n"
   ],
   "id": "37f3af2c618b429f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Monte Carlo Methods for RL\n",
    "\n",
    "Monte Carlo is a model-free methods in Reinforcement Learning and model free methods do not require the model dynamics of the environment. They provide ways of solving the RL problem based on averaging sample returns. Unlike Dynamic Programming technique, here we consider a learning method for estimating value functions and discovering optimal policies as we also do not assume complete knowledge of environment dynamics. Due to the lack of knowledge on dynamics of environment, this technique solely rely on experience.\n",
    "\n",
    "Here, We take sample sequences of states, actions and rewards from actual or simulated interaction with an environment by agent to predict on value function and finding optimal policies. Having said that, we still gonna need a model which needs only the sample distribution instead of complete probability distributions of all possible transitions.\n",
    "\n",
    "Monte Carlo methods are particularly suited for episodic tasks, where an episode is a sequence of states, actions, and rewards starting from an initial state and ending in a terminal state. The value of states or state-action pairs is estimated based on the average return (sum of rewards) received over many episodes.\n",
    "\n",
    "The fundamental concept underlying Monte Carlo methods revolves around estimating the value of states or state-action pairs by averaging the returns gained from various trajectories or episodes. This process involves computing the anticipated return for each state or state-action pair and adjusting the value estimates as more experience is accumulated.\n",
    "\n",
    "When using Monte Carlo in RL, we sample and take average returns for each state-action pair as there are multiple states in any given problems. Then the return after taking an action in preceding state depends on the actions taken in the "
   ],
   "id": "847ad75a6086f9fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advantages of Monte Carlo Methods Over Dynamic Programming\n",
    "* MC are useful to learn optimal behaviour directly from interaction with the environment without the need of environment dynamics.\n",
    "* MC can be used with simulation or sample models or real environment whereas DP requires explicit model of transition probabilities.\n",
    "* It is easy and efficient to focus MC on small required subset of the states without evaluating on the rest of the states set."
   ],
   "id": "dc2c2c0f98b5f474"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## On Policy Learning vs Off Policy Learning\n",
    "Policy learning is the process of learning an optimal policy or decision making strategy in RL. \n",
    "\n",
    "1. **On-Policy Learning:** In this, the agent learns the optimal policy by following the same policy that is being updated during the learning process. This method directly evaluate and improve the policy which is in use to make decision i.e. the learning agent learns from the experience generated by its own current policy. Thus, the behaviour policy and target policy are the same policy of different version. However, this type of learnign ensures the target behaviour is atleast better than or as good as the behavioural policy. \n",
    "\n",
    "\n",
    "2. **Off-Policy Learning:** In this, the agent learns the optimal policy from the experience generatedby some other policy. This method evaluate and improve a different policy from the one used to make current decisions i.e. the learning algorithm learns from any other policy. Thus, the beaviour policy and target policy are different in this type of learning. These can be more sample efficient because they can reuse the data collected from various policies. But, as a cons, this may lead to the complexity or distributional shift.  \n"
   ],
   "id": "589f39d8952f653a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d8dcedce8790c31e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
