{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Bellman Equation\n",
    "\n",
    "The Bellman Equation provides a technique on solving Markov Decision Process (MDP) in reinforcement learning and is common for finding the optimal value and Q functions recursively. Knowing optimal value or optimal Q function helps us to derive the optimal policy."
   ],
   "id": "7693439f802acf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deterministic Policy\n",
    "Using deterministic policy, an agent performs only one particular action in a state. It is denoted by $\\mu$ and is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "a_t = \\mu(s_t)\n",
    "\\end{equation}\n",
    "\n",
    "where, t is the time step and a is the action available in state for an agent. "
   ],
   "id": "5ac6ef629b6f0be1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Bellman equation in Deterministic Environment\n",
    "The Bellman equation for the deterministic environment gives that the value of a current state can be obtained as a sum of the immediate reward and the discounted value of th next state.\n",
    "\n",
    "\\begin{equation}\n",
    "V(s) = R(s,a,s') + \\gamma V(s')\n",
    "\\end{equation}\n",
    "\n",
    "Where, \n",
    "* $R(s,a,s')$ is the immediate reward obtained,\n",
    "* $\\gamma$ is the discount factor, and,\n",
    "* $V(s')$ is the value of the next state.\n",
    "\n",
    "Thus, using the policy $\\pi$, we have the bellman equation for the deterministic environment as:\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = R(s,a,s') + \\gamma V^\\pi(s')\n",
    "\\end{equation}\n",
    "\n",
    "*Note: the RHS term of the Bellman equation is also known as the **Bellman Backup**."
   ],
   "id": "460971b2ee541339"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Bellman equation in Stochastic Environment\n",
    "If our environment is stochastic instead of being deterministic, we have situation of landing into different states when we perform an action a in state s. Say, we are in state s1 and taking action a1 will land into s2 with 10% of probability and s3 with 90% probability. In such situation we need to update the Bellman equation incorporating the probability as:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = \\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $P(s'|s,a)$ is the transition probability of reaching various s' by taking action a in state s."
   ],
   "id": "90a3f982ee772a79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stochastic Policy\n",
    "Unlike a deterministic policy, with stochastic policy there are multiple actions available for agent to take. Thus, stochastic policy returns a probability distribution over an action space for each state s in a given state space $S$. This means, an agent doesn't take same action in particular state as it is determined probabilistically through the distribution."
   ],
   "id": "dc6166e6f653dddb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "`",
   "id": "d291dc0ad7012112"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
