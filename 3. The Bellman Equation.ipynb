{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Bellman Equation\n",
    "\n",
    "The Bellman Equation provides a technique on solving Markov Decision Process (MDP) in reinforcement learning and is common for finding the optimal value and $Q$ functions recursively. Knowing optimal value or optimal $Q$ function is useful to derive the optimal policy."
   ],
   "id": "7693439f802acf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deterministic Policy\n",
    "Using deterministic policy, an agent performs only one particular action in a state. It is denoted by $\\mu$ and is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "a_t = \\mu(s_t)\n",
    "\\end{equation}\n",
    "\n",
    "where, $t$ is the time step and $a$ is the action available in state for an agent. "
   ],
   "id": "5ac6ef629b6f0be1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Bellman equation in Deterministic Environment\n",
    "The Bellman equation for the deterministic environment gives that the value of a current state can be obtained as a sum of the immediate reward and the discounted value of the next state.\n",
    "\n",
    "\\begin{equation}\n",
    "V(s) = R(s,a,s') + \\gamma V(s')\n",
    "\\end{equation}\n",
    "\n",
    "Where, \n",
    "* $R(s,a,s')$ is the immediate reward obtained,\n",
    "* $\\gamma$ is the discount factor, and,\n",
    "* $V(s')$ is the value of the next state.\n",
    "\n",
    "Thus, using the policy $\\pi$, we have the bellman equation for the deterministic environment as:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = R(s,a,s') + \\gamma V^\\pi(s')\n",
    "\\end{equation}\n",
    "\n",
    "And, in terms of $Q$ function, we can write as:\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, a) = R(s, a, s') + \\gamma Q^\\pi(s', a')\n",
    "$$\n",
    "\n",
    "*Note: the RHS term of the Bellman equation is also known as the **Bellman Backup**."
   ],
   "id": "460971b2ee541339"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Bellman equation in Stochastic Environment\n",
    "If our environment is stochastic instead of being deterministic, we have situation of landing into different states when we perform an action $a$ in state $s$. Say, we are in state $s1$ and taking action $a1$ will land into $s2$ with 10% of probability and $s3$ with 90% probability. In such situation we need to update the Bellman equation incorporating the probability as:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = \\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]\n",
    "\\end{equation}\n",
    "\n",
    "Here, $P(s'|s,a)$ is the transition probability of reaching various $s'$ by taking action $a$ in state $s$.\n",
    "\n",
    "In terms of $Q$ function, we can write the above equation as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^\\pi(s, a) = \\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma Q^\\pi(s', a')]\n",
    "\\end{equation}"
   ],
   "id": "90a3f982ee772a79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`## Stochastic Policy\n",
    "Unlike a deterministic policy, with stochastic policy there are multiple actions available for agent to take in any particular state. Thus, stochastic policy returns a probability distribution over an action space for each state $s$ in a given state space $S$. This means, an agent may not take same action in particular state as it is determined probabilistically through the distribution. To incorporate this stochastic nature of the policy, we use the form as below:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = \\sum_{a}{{\\pi(a|s)}\\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]}}\n",
    "\\end{equation}\n",
    "\n",
    "which uses the weighted average using probability of action that agent can take in that particular state. This is known as the **Bellman expectation equation of the value function** and can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = \\mathbb{E}_{{a \\sim \\pi} {s' \\sim \\pi}}{[R(s,a,s') + \\gamma V^\\pi(s')]}\n",
    "\\end{equation}\n",
    "\n",
    "In terms of $Q$ function, we can write the above form as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^\\pi(s, a) = \\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma \\sum_{a'}{{\\pi(a'|s')} Q^\\pi(s', a')]}}\n",
    "\\end{equation}\n",
    "\n",
    "*Note the difference in $Q$ function from the value function here."
   ],
   "id": "dc6166e6f653dddb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
