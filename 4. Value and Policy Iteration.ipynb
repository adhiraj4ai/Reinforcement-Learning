{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Value Iteration\n",
    "Value Iteration is a dynamic programming technique used in reinforcement learning to find value function by iteratively solving the bellman equation. \n",
    "\n",
    "In value iteration, we first compute the optimal value function and then use it to extract the optimal policy. The optimal policy is the one through which agent takes best action with best reward possible in every state to transit from initial to final state.\n",
    "\n",
    "Using the bellman equation, we can obtain the optimal value function as:\n",
    "\n",
    "\\begin{equation}\n",
    "V^*(s) = max_a\\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]}\n",
    "\\end{equation}\n",
    "\n",
    "Or, the Q function as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^*(s, a) = \\sum_{s'}{P(s'|s,a)[R(s,a,s') + \\gamma Q^*(s', a')]}\n",
    "\\end{equation}\n",
    "\n",
    "which means, \n",
    "\\begin{equation}\n",
    "V^*(s) = max _a Q^*(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "i.e. we can compute the optimal value function by just taking the maximum over the optimal $Q$ function. Thus, in order to compute the optimal value of any state over state space $S$, we can take the maximum of $Q$ values obtained form all of the state-action pairs."
   ],
   "id": "ae7d73247c07459a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
