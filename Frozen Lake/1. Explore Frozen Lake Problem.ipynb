{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Explore Frozen Lake Environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a0aff65a5901a05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Frozen Lake is the stochastic environment i.e. agent reach the next states with some probability known as **Transition probability**. The transition probability is obtained using `env[state][action]`.\n",
    "\n",
    "\n",
    "Detail documentation on Frozen Lake environment [https://gymnasium.farama.org/environments/toy_text/frozen_lake/] "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be94a91f2a2e7c5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:44:21.923898Z",
     "start_time": "2024-06-01T05:44:21.650912Z"
    }
   },
   "source": [
    "# Import necessary library\n",
    "import gymnasium as gym\n",
    "\n",
    "# define seed\n",
    "SEED = 42"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='ansi')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:44:22.022530Z",
     "start_time": "2024-06-01T05:44:22.017948Z"
    }
   },
   "id": "4cf6566aafba6c14",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# Reset an environment to its initial internal state\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# Print the initial position of agent in the environment\n",
    "print(\"The initial observation is: {}\".format(obs))\n",
    "print(\"The information is : {}\".format(info))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:44:23.008060Z",
     "start_time": "2024-06-01T05:44:23.004982Z"
    }
   },
   "id": "cb957ae0c430ed2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is: 0\n",
      "The information is : {'prob': 1}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's render the environment and observe the complete frozen lake environment\n",
    "print(env.render())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:44:23.867016Z",
     "start_time": "2024-06-01T05:44:23.863911Z"
    }
   },
   "id": "a06d4d37cb26cdca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, \n",
    "\n",
    "**S** refers to the starting state,\n",
    "**F** refers to the frozen state,\n",
    "**H** refers to the hole state, and,\n",
    "**G** refers to the goal state\n",
    "\n",
    "Our goal, here is to reach goal state **G** from the starting state **S** without visiting any hole state **H**. By any chance, if agent visits the hole state **H**, the agent will fall into the hole and die."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b99bff5315b92ab"
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the Observation space (or state space) and action space \n",
    "print(\"The observation space: {}\".format(env.observation_space))\n",
    "print(\"The action space: {}\".format(env.action_space))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:44:38.733379Z",
     "start_time": "2024-06-01T05:44:38.729905Z"
    }
   },
   "id": "263fe55e78940356",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Discrete(16)\n",
      "The action space: Discrete(4)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "The observation space is also called **State Space**. And there are 16 states in our environment starting from 0 to 15 (from left to right and top to bottom). And, in each state, an agent can take four action as defined by action space. They are:\n",
    "\n",
    "0 => Left\n",
    "\n",
    "1 => Down\n",
    "\n",
    "2 => Right\n",
    "\n",
    "3 => Up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da71a85188931744"
  },
  {
   "cell_type": "code",
   "source": [
    "# map numbers to action\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"down\",\n",
    "    2: \"right\",\n",
    "    3: \"up\"\n",
    "}\n",
    "\n",
    "# Let's take a random action now from the action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "print(\"Action: {}\".format(action_map[random_action]))\n",
    "print(\"Next State: {}\".format(next_state))\n",
    "print(\"Reward: {}\".format(reward))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:46:16.000075Z",
     "start_time": "2024-06-01T05:46:15.995771Z"
    }
   },
   "id": "4b9d2b53f7d581f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's render the environment and confirm the same in the environment\n",
    "print(env.render())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:46:16.867117Z",
     "start_time": "2024-06-01T05:46:16.864817Z"
    }
   },
   "id": "63eecd55d971e214",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One Episode Random Walk\n",
    "Episode refers to the transition of agent from initial state to the final state. This contains the path of the transition from initial to final state for agent. Hence, also known as trajectory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe74f3f50860d50a"
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of times the agent moves\n",
    "num_timestep = 20\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "for i in range(num_timestep):\n",
    "    print(\"------- Step: {} --------\".format(i+1))\n",
    "        # Let's take a random action now from the action space\n",
    "    # Random action means we are taking random policy at the moment.\n",
    "    random_action = env.action_space.sample()\n",
    "    \n",
    "    # # Take the action and get the new observation space\n",
    "    next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "    \n",
    "    print(\"Action: {}\".format(action_map[random_action]))\n",
    "    print(\"Next State: {}\".format(next_state))\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    \n",
    "    print(env.render())\n",
    "    \n",
    "    # if the agent moves to hole state, then terminate\n",
    "    if done: \n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:49:58.390417Z",
     "start_time": "2024-06-01T05:49:58.385235Z"
    }
   },
   "id": "8885ce86ef59cbd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Step: 1 --------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 2 --------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 3 --------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 4 --------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 5 --------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 6 --------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 7 --------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 8 --------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "This completes one episode in the environment and we can run for a series of episodes as below:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd9032570f515389"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-episode Random Walk\n",
    "Multiple episodes are important as it enables agent to find optimal policy. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30888c2443ef18e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of times the agent moves\n",
    "num_timestep = 20\n",
    "\n",
    "# Number of episodes\n",
    "num_episodes = 10\n",
    "\n",
    "for e in range(num_timestep):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Episode {}/{}\".format(e, num_episodes))\n",
    "    print(\"-----------------------------------\")\n",
    "    # Reset the environment\n",
    "    env.reset(seed = SEED)\n",
    "    # We make 10 random walk in environment\n",
    "    for t in range(num_timestep):\n",
    "        print(\"timestep: {}\".format(t+1))\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        # Let's take a random action now from the action space\n",
    "        # Random action means we are taking random policy at the moment.\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        # # Take the action and get the new observation space\n",
    "        next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "        \n",
    "        print(\"Action: {}\".format(action_map[random_action]))\n",
    "        print(\"Next State: {}\".format(next_state))\n",
    "        print(\"Reward: {}\".format(reward))\n",
    "        print(\"\")\n",
    "        \n",
    "        print(env.render())\n",
    "        \n",
    "        # if the agent moves to hole state, then terminate\n",
    "        if done: \n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T05:50:19.043082Z",
     "start_time": "2024-06-01T05:50:19.026606Z"
    }
   },
   "id": "464c056cc88d4bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Episode 0/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001B[41mF\u001B[0mFH\n",
      "HFFG\n",
      "\n",
      "timestep: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 10\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "\n",
      "timestep: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 11\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFF\u001B[41mH\u001B[0m\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 1/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 2/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 7\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001B[41mH\u001B[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 3/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 6\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 7\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001B[41mH\u001B[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 4/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 5/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 6/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 7/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 8/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 15\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 9/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 13\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 14\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 15\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 16\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 17\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 18\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 20\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 10/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 11/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 12\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001B[41mH\u001B[0mFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 12/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 13\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 14\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 13/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 14/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 3\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFF\u001B[41mF\u001B[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 13\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 7\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001B[41mH\u001B[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 15/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 16/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 17/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 6\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 7\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001B[41mH\u001B[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 18/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 19/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Through random walk, as explored above, our agent mostly landed on Hole. Even if not and have reached the goal successfully, there is no policy learned that guarantees the agent will reach the goal. This is where we need a reinforcement learning to learn optimal policy that guides our agent in each state and helps it to reach the goal state. This optimal policy is learnt through various RL algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cba6a8cc004a0231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d7c1d927f852a20"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
