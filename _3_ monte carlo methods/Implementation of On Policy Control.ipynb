{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementation of On Policy Control",
   "id": "d085e65762de384a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T16:37:16.312705Z",
     "start_time": "2024-06-20T16:37:16.310570Z"
    }
   },
   "source": [
    "# import necessary libraries\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set SEED\n",
    "SEED = 106"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Black Jack Environment",
   "id": "19ecab4b9a903fd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href = \"https://gymnasium.farama.org/environments/toy_text/blackjack/\">Black Jack Environment</a>",
   "id": "70747585764aaa29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:37:16.332321Z",
     "start_time": "2024-06-20T16:37:16.328425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('Blackjack-v1', natural = False, sab = False)\n",
    "env.reset(seed=SEED)"
   ],
   "id": "c0fde955b0e10d8e",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Epsilon Greedy Policy",
   "id": "e8e30d35bca9d2d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:37:16.336600Z",
     "start_time": "2024-06-20T16:37:16.333790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def epsilon_greedy_policy(env, state, q_values):\n",
    "    \"\"\"\n",
    "    Define epsilon-greedy policy for action selection.\n",
    "    \n",
    "    :param state: The current state of the agent.\n",
    "    :param q_values: A dictionary mapping (state, action) pairs to Q-values.\n",
    "    :param env: The environment object, which has the action space information.\n",
    "    :return : The action selected by the epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    #set epsilon\n",
    "    epsilon = 0.5\n",
    "\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # Initialize the best action and its value\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Iterate over all possible actions\n",
    "        for action in range(env.action_space.n):\n",
    "            # Get the Q-value for the current state-action pair\n",
    "            q_value = q_values[(state, action)]\n",
    "            \n",
    "            # Update the best action if the current Q-value is greater than the best value found so far\n",
    "            if q_value > best_value:\n",
    "                best_value = q_value\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "        \n",
    "        # One line for the whole else block\n",
    "        # return max(list(range(env.action_space.n)), key = lambda x: q_values[(state, x)])"
   ],
   "id": "3866138636f1a7b9",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Episodes",
   "id": "327f834a06ac6737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:37:46.518626Z",
     "start_time": "2024-06-20T16:37:46.513482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the number of timesteps\n",
    "num_of_timesteps = 100\n",
    "\n",
    "def generate_episode(env, q_values):\n",
    "    \"\"\"\n",
    "    Generate episodes using the epsilon-greedy policy.\n",
    "    \n",
    "    :param env: The environment object.\n",
    "    :param q_values: A dictionary mapping (state, action) pairs to Q-values.\n",
    "    :return episode: A list of tuples (state, action, reward) representing the episode.\n",
    "    \"\"\"\n",
    "    # Initialize the list for storing the episode\n",
    "    episode = []\n",
    "    \n",
    "    # Initialize the state using the reset function\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(num_of_timesteps):\n",
    "        # Select the action using the epsilon-greedy policy\n",
    "        action = epsilon_greedy_policy(env, state, q_values)\n",
    "        \n",
    "        # Execute the action\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        # Append the state, action, and reward to the episode list\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        # If next state is the terminating state, exit\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        # Update the current state to the next state\n",
    "        state = next_state\n",
    "        \n",
    "    return episode"
   ],
   "id": "591ff4b703a4d917",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:37:47.087712Z",
     "start_time": "2024-06-20T16:37:47.071914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define the number of episode\n",
    "num_of_episode = 1000\n",
    "\n",
    "# Initialize q values\n",
    "q_values = defaultdict(float)\n",
    "\n",
    "# Initialize total return\n",
    "total_return = defaultdict(float)\n",
    "\n",
    "# Initialize the dictionary for storing the count of state-action visit\n",
    "N = defaultdict(int)\n",
    "\n",
    "# Iterate over each episode\n",
    "for _ in range(num_of_timesteps):\n",
    "    # Generate episode\n",
    "    episode = generate_episode(env, q_values)\n",
    "    \n",
    "    # Initialize a set to track visited state-action pairs in the episode\n",
    "    visited_state_action_pairs = set()\n",
    "    \n",
    "    # Get all rewards from the episode\n",
    "    rewards = [r for (s, a, r) in episode]\n",
    "    \n",
    "    # Iterate for each step in the episode\n",
    "    for t, (state, action, _) in enumerate(episode):\n",
    "        # Only update the state-action pair if it has not been seen before in this episode\n",
    "        if (state, action) not in visited_state_action_pairs:\n",
    "            visited_state_action_pairs.add((state, action))\n",
    "\n",
    "            # Compute the return (sum of rewards from time step t to the end of the episode)\n",
    "            R = sum(rewards[t:])\n",
    "\n",
    "            N[(state, action)] += 1\n",
    "            \n",
    "            # Compute the average of return and assign to Q value\n",
    "            q_values[(state, action)] = total_return[(state, action)] / N[(state, action)]"
   ],
   "id": "43e738e156df213b",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T16:37:37.269970Z",
     "start_time": "2024-06-20T16:37:37.266908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract Q value dictionary into dataframe\n",
    "df = pd.DataFrame(q_values.items(), columns= ['state-action', 'q value'])"
   ],
   "id": "82a5b5cd3e18c15f",
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "161a22c6c6e50cc5",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
