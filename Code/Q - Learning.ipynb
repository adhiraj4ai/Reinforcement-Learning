{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T20:22:59.950481Z",
     "start_time": "2024-06-24T20:22:59.947468Z"
    }
   },
   "source": [
    "#import required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set seed\n",
    "SEED = 106"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:22:59.955216Z",
     "start_time": "2024-06-24T20:22:59.952004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\n",
    "    'FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode = 'ansi'\n",
    ")\n",
    "env.reset(seed=SEED)\n",
    "print(env.render())"
   ],
   "id": "6785088fc7bde621",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:22:59.963468Z",
     "start_time": "2024-06-24T20:22:59.956590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env, alpha = 0.01, gamma = 0.9, epsilon=0.9):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Learning Agent agent.\n",
    "        \n",
    "        :param env: The environment to learn from.\n",
    "        :type env: gym.Env\n",
    "        :param alpha: The learning rate.\n",
    "        :type alpha: float\n",
    "        :param gamma: The discount factor.\n",
    "        :type gamma: float\n",
    "        :param epsilon: The exploration rate for the epsilon-greedy policy.\n",
    "        :type epsilon: float\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = self.init_q_values()\n",
    "        \n",
    "    def init_q_values(self):\n",
    "        \"\"\"\n",
    "        Initialize the Q-values to zero for all state-action pairs.\n",
    "        \n",
    "        :return: A dictionary with state-action pairs as keys and Q-values as values.\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        q = {}\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                q[(state, action)] = 0.0\n",
    "        return q\n",
    "    \n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using the epsilon-greedy policy.\n",
    "        \n",
    "        :param state: The current state.\n",
    "        :type state: int\n",
    "        :return: The selected action.\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(\n",
    "                range(self.env.action_space.n), key=lambda x: self.q_values[(state, x)]\n",
    "            )\n",
    "    \n",
    "    def compute_policy(self, num_of_timesteps, num_of_episodes):\n",
    "        \"\"\"\n",
    "        Learn the policy by running SARSA for a specified number of episodes and timesteps.\n",
    "        \n",
    "        :param num_of_timesteps: The number of timesteps in each episode.\n",
    "        :type num_of_timesteps: int\n",
    "        :param num_of_episodes: The number of episodes to run.\n",
    "        :type num_of_episodes: int\n",
    "        :return: DataFrame containing Q-values for each episode.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        q_values_df = pd.DataFrame()\n",
    "\n",
    "        for i in range(num_of_episodes):\n",
    "            # Initialize states\n",
    "            state, _ = self.env.reset()\n",
    "            \n",
    "            for t in range(num_of_timesteps):\n",
    "                # Select the action using epsilon-greedy policy\n",
    "                action = self.epsilon_greedy(state)\n",
    "                \n",
    "                # Take the action and observe the next state and reward\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                \n",
    "                # Select next action for the next state using -greedy policy\n",
    "                next_action = np.argmax(\n",
    "                    [self.q_values[(next_state, a)] for a in range(env.action_space.n)]\n",
    "                )\n",
    "                \n",
    "                # Update Q-value\n",
    "                self.q_values[(state, action)] += self.alpha * (\n",
    "                    reward + self.gamma * self.q_values[(next_state, next_action)] - self.q_values[(state, action)]\n",
    "                )\n",
    "                \n",
    "                # Move to the next state and action\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Store Q-values in DataFrame\n",
    "            q_values_df = pd.concat([q_values_df, pd.DataFrame(self.q_values, index=[i])])\n",
    "        \n",
    "        return q_values_df"
   ],
   "id": "e98c34f141524baa",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:23:09.147939Z",
     "start_time": "2024-06-24T20:22:59.964364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "alpha = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.5\n",
    "num_of_episodes = 100000\n",
    "num_of_timesteps = 100\n",
    "\n",
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)\n",
    "env.reset()\n",
    "\n",
    "# Create SARSA model\n",
    "model = QLearning(env, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "# Compute policy and store Q-values\n",
    "q_values_df = model.compute_policy(num_of_timesteps, num_of_episodes)"
   ],
   "id": "dd90336ce92fc4de",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m model \u001B[38;5;241m=\u001B[39m QLearning(env, alpha\u001B[38;5;241m=\u001B[39malpha, gamma\u001B[38;5;241m=\u001B[39mgamma, epsilon\u001B[38;5;241m=\u001B[39mepsilon)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Compute policy and store Q-values\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m q_values_df \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_of_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_of_episodes\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 92\u001B[0m, in \u001B[0;36mQLearning.compute_policy\u001B[0;34m(self, num_of_timesteps, num_of_episodes)\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;66;03m# Store Q-values in DataFrame\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m     q_values_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([q_values_df, \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m])\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m q_values_df\n",
      "File \u001B[0;32m~/Documents/Projects/Reinforcement-Learning/venv/lib/python3.9/site-packages/pandas/core/frame.py:778\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    772\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[1;32m    773\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[1;32m    774\u001B[0m     )\n\u001B[1;32m    776\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[0;32m--> 778\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[0;32m~/Documents/Projects/Reinforcement-Learning/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py:503\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[0;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    500\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[1;32m    501\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[0;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Projects/Reinforcement-Learning/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py:119\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[1;32m    116\u001B[0m         index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001B[39;00m\n\u001B[0;32m--> 119\u001B[0m     arrays, refs \u001B[38;5;241m=\u001B[39m \u001B[43m_homogenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# _homogenize ensures\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001B[39;00m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    125\u001B[0m \n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    127\u001B[0m     index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n",
      "File \u001B[0;32m~/Documents/Projects/Reinforcement-Learning/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py:630\u001B[0m, in \u001B[0;36m_homogenize\u001B[0;34m(data, index, dtype)\u001B[0m\n\u001B[1;32m    627\u001B[0m         val \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mfast_multiget(val, oindex\u001B[38;5;241m.\u001B[39m_values, default\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan)\n\u001B[1;32m    629\u001B[0m     val \u001B[38;5;241m=\u001B[39m sanitize_array(val, index, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 630\u001B[0m     \u001B[43mcom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequire_length_match\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m     refs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    633\u001B[0m homogenized\u001B[38;5;241m.\u001B[39mappend(val)\n",
      "File \u001B[0;32m~/Documents/Projects/Reinforcement-Learning/venv/lib/python3.9/site-packages/pandas/core/common.py:568\u001B[0m, in \u001B[0;36mrequire_length_match\u001B[0;34m(data, index)\u001B[0m\n\u001B[1;32m    564\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m condition:\n\u001B[1;32m    565\u001B[0m             \u001B[38;5;28msetattr\u001B[39m(obj, attr, old_value)\n\u001B[0;32m--> 568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequire_length_match\u001B[39m(data, index: Index) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    569\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    570\u001B[0m \u001B[38;5;124;03m    Check the length of data matches the length of the index.\u001B[39;00m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(index):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate average Q-values over episodes\n",
    "avg_q_values = q_values_df.mean(axis=1)\n",
    "\n",
    "# Plot the average Q-values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(avg_q_values.index, avg_q_values.values, label='Average Q-value', color='r')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q-value')\n",
    "plt.title('Average Q-value Evolution Over Episodes through Q-Learning')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c4eea1799bc99d19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "57c63afbc2248eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the learned policy\n",
    "policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "for state in range(env.observation_space.n):\n",
    "    policy[state] = np.argmax([model.q_values[(state, a)] for a in range(env.action_space.n)])\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "print(policy.reshape((4, 4)))  # Reshape according to FrozenLake's 4x4 grid"
   ],
   "id": "701bcc2ef84f5f19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the learned Q-values\n",
    "print(\"Learned Q-values:\")\n",
    "for state in range(env.observation_space.n):\n",
    "    for action in range(env.action_space.n):\n",
    "        print(f\"Q[{state}, {action}] = {model.q_values[(state, action)]:.2f}\")"
   ],
   "id": "c1667340c56cb80d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get learned Policy\n",
    "policy = policy.reshape((4, 4))\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "\n",
    "# Function to run an episode\n",
    "def run_episode(env, policy):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    \n",
    "    while not done:\n",
    "        frames.append(env.render())\n",
    "        action = policy[state]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Run an episode\n",
    "frames = run_episode(env, policy)\n",
    "\n",
    "# Display the animation\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(frames[0])\n",
    "\n",
    "def animate(i):\n",
    "    img.set_data(frames[i])\n",
    "    return (img,)\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "anim = FuncAnimation(fig, animate, frames=len(frames), interval=500, blit=True)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ],
   "id": "3ca18268b496cbe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fab476e782e668c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
