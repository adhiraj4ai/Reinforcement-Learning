{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cart Pole Environment\n",
    "The goal of the Cart Pole problem is to balance the pole placed upright on cart where a cart moves along a frictionless track. \n",
    "\n",
    "Detail documentation on Cart Pole environment [https://gymnasium.farama.org/environments/classic_control/cart_pole/] "
   ],
   "id": "d997048a9a8e45a6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T06:41:44.644983Z",
     "start_time": "2024-06-01T06:41:44.386042Z"
    }
   },
   "source": [
    "# Import gymnasium library\n",
    "import gymnasium as gym\n",
    "\n",
    "SEED = 42"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T06:42:34.918585Z",
     "start_time": "2024-06-01T06:42:34.911975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('CartPole-v1')"
   ],
   "id": "ee429cb1ed642afb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T06:42:48.308229Z",
     "start_time": "2024-06-01T06:42:48.303364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reset an environment to its initial internal state\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# Print the initial position of agent in the environment\n",
    "print(\"The initial observation is: {}\".format(obs))\n",
    "print(\"The information is : {}\".format(info))"
   ],
   "id": "a145a45f18af7794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "The information is : {}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:36:02.691190Z",
     "start_time": "2024-06-01T07:36:02.688373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the Observation space (or state space) and action space \n",
    "print(\"The observation space: {}\".format(env.observation_space))"
   ],
   "id": "9ab36939efbb3045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The observation space consists of minimum and maximum value for \n",
    "1. Cart position ( -4.8 to 4.8 ), \n",
    "2. Cart Velocity ( -Inf to Inf ), \n",
    "3. Pole Angle ( ~-0.418 rad to 0.418 rad ), and, \n",
    "4. Pole Angular Velocity ( -Inf to Inf )\n",
    "\n",
    "Box implies that our state space contains continuous values and not discrete values. We can obtain the maximum and minimum values as below:"
   ],
   "id": "b5a75cdf16ecc610"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T06:56:28.886716Z",
     "start_time": "2024-06-01T06:56:28.882161Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.observation_space.high, env.observation_space.low)",
   "id": "48b2b0d5abcab449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38] [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And the action space in Cart Pole is discrete and contains two discrete values as:\n",
    "1. 0: Push cart to the left\n",
    "2. 1: Push cart to the right"
   ],
   "id": "9bebd97896f8fd05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:36:50.844435Z",
     "start_time": "2024-06-01T07:36:50.840547Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"The action space: {}\".format(env.action_space))\n",
   "id": "670cbe92fbea95db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space: Discrete(2)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cart Pole Balancing with Random Policy in multiple episode",
   "id": "80d14cbc97182b65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:18:21.207Z",
     "start_time": "2024-06-01T08:18:21.026058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define action map\n",
    "action_map = {\n",
    "    0: 'left',\n",
    "    1: 'right',\n",
    "}\n",
    "\n",
    "# Number of times the agent moves\n",
    "num_timestep = 50\n",
    "\n",
    "# Number of episodes\n",
    "num_episodes = 100\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Episode {}/{}\".format(e, num_episodes))\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # Initialize the return\n",
    "    RETURN = 0\n",
    "    \n",
    "    # Initialize the state by resetting the environment\n",
    "    state = env.reset(seed=SEED)\n",
    "    \n",
    "    # We take a random step for each episode\n",
    "    for t in range(num_timestep):\n",
    "        print(\"timestep: {} of episode: {}\".format(t+1, e))\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        # select random action\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action and get the new observation space\n",
    "        next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "        \n",
    "        print(\"Action: {}\".format(action_map[random_action]))\n",
    "        print(\"Next State: {}\".format(next_state))\n",
    "        print(\"Reward: {}\".format(reward))\n",
    "        print(\"\")\n",
    "        \n",
    "        RETURN = RETURN + reward\n",
    "        \n",
    "        # if the agent moves to hole state, then terminate\n",
    "        if done: \n",
    "            break\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print(\"****************************************************\")\n",
    "        print(\"Episode: {}, Return: {}\".format(e, RETURN))\n",
    "        print(\"****************************************************\")\n",
    "    \n",
    "    env.close()\n",
    "    "
   ],
   "id": "7684e0833d100dfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Episode 0/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0579058   0.18717085 -0.00685163 -0.23240055]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164922 -0.00785253 -0.01149964  0.0581133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06149217 -0.20280772 -0.01033737  0.34714594]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05743602 -0.00754027 -0.00339445  0.05122127]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05728521 -0.20261338 -0.00237003  0.34283128]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05323294 -0.39770153  0.0044866   0.6347659 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04527891 -0.20264246  0.01718191  0.34349927]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04122606 -0.0077691   0.0240519   0.05628363]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04107068  0.18699987  0.02517757 -0.22871475]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04481068 -0.00847266  0.02060328  0.07180254]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04464123 -0.20388384  0.02203933  0.37091404]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04056355 -0.00908182  0.02945761  0.08526102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04038191 -0.20461337  0.03116283  0.38709038]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03628964 -0.00994733  0.03890464  0.10439347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0360907  -0.2056046   0.04099251  0.40909237]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03197861 -0.01108709  0.04917435  0.12960964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03175686 -0.20687772  0.05176655  0.43739203]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02761931 -0.40269274  0.06051439  0.74593365]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01956546 -0.5985952   0.07543306  1.0570298 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00759355 -0.40454942  0.09657366  0.7889452 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-4.9743714e-04 -6.0085571e-01  1.1235256e-01  1.1103810e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01251455 -0.7972599   0.13456018  1.4360921 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02845975 -0.60402906  0.16328202  1.1883074 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04054033 -0.8008468   0.18704817  1.5273991 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05655727 -0.608409    0.21759616  1.2984493 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 0, Return: 29.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 1/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106766 -0.5943515   0.06362975  0.96259123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-8.1936747e-04 -7.9026806e-01  8.2881577e-02  1.2745659e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01662473 -0.9863437   0.1083729   1.5920091 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0363516  -0.79266214  0.14021307  1.3349905 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05220484 -0.98924476  0.16691288  1.6680583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07198974 -1.1858681   0.20027405  2.0077393 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0957071  -1.3824346   0.24042884  2.3551834 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 2/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01234624 -0.594763    0.09879284  0.9741247 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0242415  -0.7910615   0.11827533  1.2961347 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04006273 -0.98747045  0.14419802  1.6233817 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05981214 -0.79431033  0.17666565  1.3788961 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07569835 -0.6017788   0.20424357  1.1462637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08773392 -0.409823    0.22716886  0.9239459 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 3/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06945409  0.38238686 -0.02320687 -0.5272419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07710183  0.5778275  -0.0337517  -0.8271461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08865838  0.77339435 -0.05029463 -1.1302503 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10412627  0.5789658  -0.07289963 -0.8537567 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11570558  0.77500165 -0.08997477 -1.168443  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13120562  0.5811579  -0.11334363 -0.90527046]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14282878  0.7776172  -0.13144904 -1.2313203 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15838112  0.97416174 -0.15607545 -1.5621284 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17786436  1.1707683  -0.18731801 -1.8991536 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20127971  0.9781039  -0.22530109 -1.6699625 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 4/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06571045  0.5774066  -0.01855621 -0.817635  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07725858  0.77277756 -0.03490891 -1.1160963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09271413  0.5781308  -0.05723084 -0.83456504]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10427675  0.773986   -0.07392213 -1.1446835 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11975647  0.96999156 -0.09681581 -1.4596024 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1391563   1.1661583  -0.12600785 -1.7808938 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16247946  0.9726592  -0.16162573 -1.5298947 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18193264  1.1693188  -0.19222362 -1.868351  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20531902  1.3659544  -0.22959064 -2.2140381 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 5/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01888104 -0.593896    0.05171815  0.95197964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00700312 -0.39950672  0.07075775  0.67598397]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00098702 -0.20543562  0.08427742  0.40639117]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00509573 -0.01160355  0.09240524  0.14142324]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0053278   0.18208183  0.09523371 -0.12073685]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00168616  0.3757195   0.09281898 -0.38192075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00582823  0.5694094   0.08518056 -0.6439553 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01721641  0.37321     0.07230145 -0.32571054]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02468061  0.1771371   0.06578724 -0.01113146]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02822336  0.37125692  0.06556461 -0.2823538 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03564849  0.5653854   0.05991754 -0.55365866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0469562   0.75961703  0.04884436 -0.8268781 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06214854  0.5638624   0.0323068  -0.5192418 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07342579  0.758515    0.02192197 -0.8015718 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08859609  0.5630994   0.00589053 -0.50207424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09985808  0.3678949  -0.00415095 -0.20754077]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10721598  0.56307596 -0.00830177 -0.50153023]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11847749  0.7583139  -0.01833237 -0.7968178 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13364378  0.56344825 -0.03426873 -0.50995785]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14491273  0.7590358  -0.04446789 -0.81323993]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16009346  0.56455016 -0.06073269 -0.53486913]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17138445  0.3703325  -0.07143006 -0.26192415]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1787911   0.56639755 -0.07666855 -0.5762543 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19011906  0.37242925 -0.08819363 -0.30867508]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19756764  0.5686899  -0.09436714 -0.6278172 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20894144  0.7649934  -0.10692348 -0.94866514]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.22424132  0.57146084 -0.12589678 -0.69139975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23567052  0.76808393 -0.13972478 -1.0209161 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2510322   0.57507217 -0.1601431  -0.77516556]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.26253363  0.77199185 -0.17564641 -1.1136466 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.27797347  0.579556   -0.19791935 -0.88080907]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2895646   0.38759318 -0.21553552 -0.65629524]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 6/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02667528 -0.20403126  0.04023969  0.37436375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259465 -0.00950332  0.04772697  0.09463533]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02240459 -0.20527568  0.04961967  0.40198588]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01829907 -0.40106502  0.05765939  0.7098906 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01027777 -0.5969361   0.0718572   1.0201515 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00166095 -0.40284148  0.09226023  0.7508687 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00971778 -0.5991065   0.1072776   1.0711002 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02169991 -0.79547065  0.12869962  1.3954324 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03760932 -0.9919374   0.15660825  1.7254285 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05744807 -1.188466    0.19111682  2.0624685 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08121739 -1.3849555   0.23236619  2.4076834 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 7/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03039867  0.38069376  0.03602977 -0.49021873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03801255  0.18508255  0.0262254  -0.18640189]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0417142  -0.01040461  0.02249736  0.1144374 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0415061   0.18438788  0.02478611 -0.17106372]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04519386 -0.0110799   0.02136483  0.12933426]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04497226 -0.20650129  0.02395152  0.4286802 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04084224 -0.01172659  0.03252512  0.14364304]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04060771  0.18291482  0.03539798 -0.13860397]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.044266    0.37751237  0.0326259  -0.4199126 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05181625  0.5721572   0.02422765 -0.70213413]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06325939  0.7669351   0.01018497 -0.98709303]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0785981   0.96191925 -0.00955689 -1.2765597 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09783648  1.1571617  -0.03508809 -1.5722197 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12097972  0.96247554 -0.06653249 -1.290684  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14022923  0.7682597  -0.09234616 -1.0195509 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15559442  0.57448167 -0.11273718 -0.75723356]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16708405  0.38107902 -0.12788185 -0.50204575]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17470564  0.5777496  -0.13792276 -0.8321376 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18626063  0.38475475 -0.15456551 -0.58581525]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19395572  0.19209692 -0.16628182 -0.3455352 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19779766  0.3891456  -0.17319253 -0.68569326]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20558058  0.19679703 -0.1869064  -0.45215124]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20951651  0.3940022  -0.19594942 -0.79743826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21739656  0.5911952  -0.21189818 -1.1448085 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 8/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05010155  0.18701302  0.00484587 -0.22891912]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3841807e-02  3.8206539e-01  2.6748964e-04 -5.2006954e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06148311  0.18693967 -0.0101339  -0.22730236]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06522191  0.38220495 -0.01467995 -0.5231646 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07286601  0.18729267 -0.02514324 -0.23514338]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07661186  0.38276467 -0.02984611 -0.53565   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08426715  0.5782933  -0.04055911 -0.8375858 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09583302  0.38374805 -0.05731082 -0.55792904]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10350798  0.57962567 -0.06846941 -0.86810327]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1151005   0.77560914 -0.08583147 -1.1815039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13061267  0.5816996  -0.10946155 -0.9169138 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14224666  0.7781178  -0.12779982 -1.2418952 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15780902  0.5848467  -0.15263774 -0.99182385]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16950595  0.781645   -0.1724742  -1.3282907 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18513885  0.9784722  -0.19904003 -1.6696073 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20470831  1.1752731  -0.23243216 -2.0171127 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 9/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106766 -0.20418042  0.06362975  0.378082  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00698405 -0.4001456   0.07119139  0.69012946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00101886 -0.20607999  0.08499398  0.420681  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00514046 -0.01225866  0.0934076   0.15595664]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00538563  0.1814104   0.09652673 -0.10585862]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00175742 -0.01495279  0.09440956  0.21564984]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00205648 -0.21128874  0.09872255  0.53655744]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00628225 -0.01768337  0.10945371  0.27654073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00663592 -0.21418282  0.11498452  0.6016413 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01091958 -0.02084113  0.12701735  0.34727484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0113364  -0.21751933  0.13396284  0.6771583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01568679 -0.02448791  0.14750601  0.42947274]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01617655 -0.22135715  0.15609546  0.7647795 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02060369 -0.02868966  0.17139105  0.52499795]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02117748 -0.22575645  0.18189101  0.8664098 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02569261 -0.03351373  0.19921921  0.6359843 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02636288 -0.23077437  0.21193889  0.98420674]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 10/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03041884 -0.3990535   0.03558456  0.6648789 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02243777 -0.20444414  0.04888214  0.38360932]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01834889 -0.4002248   0.05655432  0.69129515]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01034439 -0.59608394  0.07038023  1.0012323 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00157729 -0.4019695   0.09040488  0.73145616]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00961668 -0.5982168   0.10503399  1.0511677 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02158101 -0.79456306  0.12605736  1.3748873 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03747227 -0.9910147   0.1535551   1.7041894 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05729257 -0.7979576   0.18763888  1.4629792 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07325172 -0.9948171   0.21689847  1.807928  ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 10, Return: 15.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 11/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06926793  0.9669101  -0.0214023  -1.3866944 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08860613  0.7720614  -0.0491362  -1.1007801 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10404736  0.57761925 -0.07115179 -0.82390916]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11559974  0.7736386  -0.08762998 -1.1380953 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13107252  0.9697904  -0.11039189 -1.4569241 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15046832  0.7761826  -0.13953036 -1.2006695 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16599198  0.9728058  -0.16354376 -1.5336269 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1854481  0.7799881 -0.1942163 -1.2961276]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20104785  0.58778954 -0.22013885 -1.0699911 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 12/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08490668  0.5778198  -0.0454587  -0.82727844]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09646308  0.7735329  -0.06200428 -1.133905  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11193374  0.96940905 -0.08468238 -1.4453722 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13132192  0.77542496 -0.11358982 -1.1803056 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14683041  0.5819459  -0.13719593 -0.925281  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15846933  0.7786274  -0.15570155 -1.2577403 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17404188  0.5858025  -0.18085636 -1.0175933 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18575794  0.7828139  -0.20120822 -1.3611729 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20141421  0.97980726 -0.22843169 -1.7094585 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 13/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04587036 -0.20363224  0.01334879  0.36532268]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04179772 -0.3989413   0.02065525  0.66218466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03381889 -0.5943445   0.03389894  0.96129906]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.021932   -0.39969411  0.05312492  0.67945564]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01393812 -0.5955122   0.06671403  0.9883798 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00202788 -0.4013438   0.08648163  0.71737444]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.005999   -0.5975494   0.10082912  1.0359769 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01794999 -0.40390185  0.12154865  0.7765744 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02602803 -0.21064253  0.13708015  0.5244716 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03024088 -0.40740037  0.14756957  0.85701513]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03838888 -0.21456386  0.16470988  0.6141324 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04268016 -0.02207992  0.17699252  0.37752014]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04312176 -0.21921587  0.18454292  0.72036904]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04750608 -0.02706138  0.1989503   0.49097958]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0480473   0.1647802   0.2087699   0.26699975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0447517   0.35640645  0.2141099   0.04673513]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 14/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03448966  0.1866822   0.02831649 -0.22171655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0382233   0.3813882   0.02388216 -0.50533456]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04585107  0.18593797  0.01377547 -0.20522213]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04956983  0.38086024  0.00967103 -0.49352795]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05718703  0.18560325 -0.00019953 -0.19781289]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06089909 -0.00951585 -0.00415579  0.09480708]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06070878 -0.20457798 -0.00225965  0.38617596]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05661722 -0.3996678   0.00546387  0.6781456 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04862386 -0.20462216  0.01902678  0.3871879 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04453142 -0.0097754   0.02677054  0.10056419]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04433591  0.18495286  0.02878182 -0.18355395]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04803497 -0.01056885  0.02511074  0.11806782]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04782359 -0.20604141  0.0274721   0.41856602]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04370276 -0.40154168  0.03584342  0.7197815 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03567193 -0.5971407   0.05023905  1.0235274 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02372912 -0.79289454  0.0707096   1.3315517 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00787122 -0.9888334   0.09734064  1.6454968 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01190544 -0.79497594  0.13025057  1.3846601 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02780496 -0.6016965   0.15794377  1.1353823 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03983889 -0.4089533   0.18065141  0.8961088 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04801796 -0.21667957  0.19857359  0.66521466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05235155 -0.41392824  0.21187788  1.0132712 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 15/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08490668  0.5778198  -0.0454587  -0.82727844]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09646308  0.383348   -0.06200428 -0.5492323 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10413004  0.1891493  -0.07298892 -0.2767116 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10791302  0.38523257 -0.07852315 -0.59149307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11561768  0.19129267 -0.09035301 -0.32454175]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11944353  0.38757718 -0.09684385 -0.64429516]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12719508  0.5839059  -0.10972975 -0.9658359 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13887319  0.39041513 -0.12904647 -0.7095412 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14668149  0.19729422 -0.1432373  -0.46010298]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15062737  0.00445656 -0.15243936 -0.21577665]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15071651 -0.18819475 -0.15675488  0.02520544]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14695261 -0.38076195 -0.15625077  0.26461723]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13933738 -0.5733487  -0.15095843  0.5042271 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1278704  -0.37645766 -0.1408739   0.16803586]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12034124 -0.1796298  -0.13751318 -0.16556124]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11674865  0.01716527 -0.14082439 -0.4982688 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11709195 -0.17571959 -0.15078978 -0.25307134]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11357756 -0.36840287 -0.1558512  -0.01149044]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1062095  -0.5609862  -0.156081    0.22824836]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09498978 -0.7535733  -0.15151605  0.46791565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07991832 -0.556672   -0.14215773  0.13157189]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06878488 -0.7495017  -0.1395263   0.37624502]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05379484 -0.94239473 -0.13200139  0.621885  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03494695 -1.1354505  -0.11956369  0.8702535 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01223794 -1.3287609  -0.10215862  1.123081  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01433728 -1.1324588  -0.07969701  0.80018073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03698646 -0.93633944 -0.06369339  0.48352927]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05571324 -0.7403791  -0.0540228   0.17147161]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07052083 -0.9346879  -0.05059337  0.4466346 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08921459 -1.129059   -0.04166068  0.72295004]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.11179576 -0.9333863  -0.02720168  0.41745102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.1304635  -0.73788965 -0.01885266  0.1163182 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.14522128 -0.93273646 -0.01652629  0.40299413]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.16387601 -0.7373841  -0.00846641  0.10514705]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.17862369 -0.5421418  -0.00636347 -0.19019493]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.18946654 -0.3469294  -0.01016737 -0.48487845]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.19640511 -0.5419064  -0.01986494 -0.19541723]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.20724325 -0.7367387  -0.02377328  0.09093358]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.22197802 -0.54128414 -0.02195461 -0.209154  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.2328037  -0.7360854  -0.02613769  0.07652333]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.24752541 -0.5405987  -0.02460722 -0.22429019]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.25833738 -0.34513387 -0.02909303 -0.5246325 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.26524007 -0.14961484 -0.03958568 -0.82633907]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.26823235 -0.34417373 -0.05611246 -0.5463645 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 16/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03039867 -0.00950393  0.03602977  0.09460401]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03020859  0.18508358  0.03792185 -0.18649738]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03391026  0.37964302  0.0341919  -0.46698037]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04150312  0.18405509  0.0248523  -0.16371949]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04518422  0.3788126   0.02157791 -0.44845974]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05276048  0.1833922   0.01260871 -0.14905393]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05642832 -0.01190802  0.00962763  0.14757995]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05619016 -0.20716651  0.01257923  0.4432846 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05204683 -0.40246418  0.02144492  0.73990613]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04399755 -0.20764478  0.03624305  0.45404854]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03984465 -0.40325996  0.04532402  0.75793177]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03177945 -0.59897625  0.06048265  1.0645254 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01979993 -0.7948443   0.08177316  1.3755614 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00390304 -0.60083383  0.10928439  1.1095326 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00811363 -0.40730417  0.13147503  0.85303557]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01625972 -0.21419588  0.14853574  0.6044154 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02054363 -0.02142912  0.16062406  0.36195827]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02097222 -0.2184261   0.16786322  0.70067066]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02534074 -0.41542807  0.18187663  1.0411404 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0336493  -0.22312641  0.20269944  0.8106231 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03811183 -0.03127236  0.2189119   0.5879202 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 17/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05010155  0.18701302  0.00484587 -0.22891912]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3841807e-02  3.8206539e-01  2.6748964e-04 -5.2006954e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06148311  0.57718354 -0.0101339  -0.8126682 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07302678  0.38220188 -0.02638726 -0.52318996]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08067083  0.57768506 -0.03685106 -0.8240697 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09222452  0.77329123 -0.05333246 -1.1281114 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10769035  0.5789069  -0.07589468 -0.85262185]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11926848  0.38489705 -0.09294713 -0.5847364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12696643  0.58118963 -0.10464185 -0.9051915 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13859022  0.7775611  -0.12274569 -1.2288461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15414144  0.9740299  -0.14732261 -1.5573287 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17362204  1.170577   -0.17846918 -1.8921121 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19703358  0.9777851  -0.21631142 -1.6597093 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 18/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00454311 -0.5947744   0.087106    0.9734366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01643859 -0.79095024  0.10657474  1.2921615 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0322576  -0.5973322   0.13241796  1.0346559 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04420424 -0.7939424   0.15311109  1.3658066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06008309 -0.6010329   0.18042722  1.1246666 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07210375 -0.40867454  0.20292054  0.893571  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08027724 -0.6058844   0.22079197  1.2425663 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 19/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03039867 -0.00950393  0.03602977  0.09460401]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03020859 -0.20512328  0.03792185  0.39843303]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02610613 -0.01055924  0.04589051  0.11794317]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02589494 -0.20630765  0.04824938  0.4247437 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02176879 -0.01190119  0.05674425  0.14765322]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02153076  0.1823642   0.05969732 -0.126602  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02517805 -0.0135599   0.05716527  0.18430124]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02490685 -0.20945121  0.0608513   0.49445567]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02071783 -0.01523786  0.07074041  0.22155407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02041307  0.1788054   0.07517149 -0.04800251]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02398918 -0.01730946  0.07421144  0.26741862]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02364299  0.17667924  0.07955982 -0.00096537]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02717657  0.37057537  0.07954051 -0.26752365]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03458808 0.17441367 0.07419004 0.04914886]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03807635  0.3683977   0.07517301 -0.21923493]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04544431  0.5623691   0.07078832 -0.48729023]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05669169  0.36632347  0.06104251 -0.17316389]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.06401816 0.17038341 0.05757923 0.13813417]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06742582 -0.02551395  0.06034191  0.44841218]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.06691555 0.16870485 0.06931016 0.17534395]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07028965 -0.02733705  0.07281704  0.48906162]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.0697429  0.16668612 0.08259827 0.22018741]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07307663  0.36053622  0.08700202 -0.04534011]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.08028735 0.16428143 0.08609521 0.273475  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08357298 -0.03195672  0.09156471  0.59202266]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.08293384 0.16177213 0.10340517 0.32952914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08616929 -0.03465817  0.10999575  0.6529477 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.08547612 0.15877408 0.12305471 0.39682642]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0886516  -0.03785928  0.13099124  0.72563297]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.08789442 0.15523154 0.1455039  0.47687948]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09099905 -0.04161286  0.15504149  0.8116533 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.09016679 0.15108405 0.17127454 0.57147485]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09318847 -0.04597331  0.18270405  0.91284484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09226901 -0.24303389  0.20096095  1.2569325 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08740833 -0.4400788   0.2260996   1.6052374 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 20/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06926793  0.57666624 -0.0214023  -0.80132866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08080126  0.38184425 -0.03742888 -0.5154544 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08843814  0.57747275 -0.04773797 -0.81969315]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0999876   0.3830355  -0.06413183 -0.5423992 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10764831  0.18887073 -0.07497981 -0.27059263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11142572 -0.00510563 -0.08039167 -0.002469  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11132361 -0.19898812 -0.08044105  0.26380578]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10734385 -0.00281563 -0.07516493 -0.0531256 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10728753 -0.19678384 -0.07622744  0.2149275 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10335185 -0.39073792 -0.0719289   0.48262462]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0955371 -0.1946784 -0.0622764  0.1681669]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09164353 -0.3888562  -0.05891306  0.44057155]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0838664  -0.19295216 -0.05010163  0.12991485]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08000736  0.00285037 -0.04750333 -0.1781441 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08006437 -0.19156073 -0.05106622  0.09918286]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07623316 -0.38591504 -0.04908256  0.37532732]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06851485 -0.5803067  -0.04157601  0.6521392 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05690872 -0.77482575 -0.02853323  0.9314458 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0414122  -0.57933056 -0.00990431  0.6299348 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02982559 -0.7743129   0.00269438  0.9194822 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01433934 -0.5792275   0.02108403  0.6276472 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00275478 -0.3844061   0.03363697  0.34167826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00493334 -0.18977845  0.04047053  0.05978936]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00872891 -0.3854566   0.04166632  0.36496133]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01643804 -0.58114517  0.04896555  0.67048573]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02806094 -0.38673693  0.06237526  0.39361307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03579568 -0.58268595  0.07024752  0.70529145]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0474494  -0.7787073   0.08435336  1.0192342 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06302354 -0.97484607  0.10473803  1.3371665 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08252046 -0.7811877   0.13148136  1.0790077 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.09814422 -0.5880238   0.15306152  0.8303069 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.1099047  -0.7848694   0.16966766  1.1669478 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.12560208 -0.5923117   0.19300662  0.93190205]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.13744831 -0.40024403  0.21164466  0.7055423 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 20, Return: 40.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 21/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04587036 -0.20363224  0.01334879  0.36532268]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04179772 -0.3989413   0.02065525  0.66218466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03381889 -0.20411275  0.03389894  0.37607634]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02973664 -0.3996994   0.04142046  0.6792521 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02174265 -0.20517655  0.05500551  0.39989224]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01763912 -0.40103388  0.06300335  0.7093974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00961844 -0.59696925  0.0771913   1.0212281 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00232094 -0.4029559   0.09761586  0.75374633]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01038006 -0.2093056   0.11269078  0.49330723]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01456617 -0.40582153  0.12255693  0.8192724 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02268261 -0.60238856  0.13894238  1.147853  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03473037 -0.40932664  0.16189943  0.9017687 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04291691 -0.21672449  0.17993481  0.66403425]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0472514  -0.02450123  0.1932155   0.43297023]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04774142 -0.22175844  0.2018749   0.77980274]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05217659 -0.02989908  0.21747096  0.5568069 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 22/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00082953 -0.01053289  0.08311619  0.11762639]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00104019  0.1833059   0.08546872 -0.14772001]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00262593 -0.01292938  0.08251432  0.17065568]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00236734  0.18092054  0.08592743 -0.09489819]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00598575 -0.01532116  0.08402947  0.22360991]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00567933 -0.21153732  0.08850166  0.54157126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00144858 -0.01776347  0.09933309  0.27803358]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00109331 0.17581142 0.10489376 0.01825848]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00460954 -0.02064622  0.10525893  0.3421075 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00419661 0.17283301 0.11210108 0.08438392]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00765327  0.36618444  0.11378875 -0.17093405]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01497696  0.5595094   0.11037008 -0.42566442]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02616715  0.7529091   0.10185678 -0.6816159 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04122533  0.55653113  0.08822447 -0.35868144]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05235596  0.36027297  0.08105084 -0.03953486]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05956142  0.5541448   0.08026014 -0.305585  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07064431  0.74803674  0.07414845 -0.5719155 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08560505  0.55195767  0.06271013 -0.2568254 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0966442   0.7461308   0.05757362 -0.52908796]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11156681  0.5502482   0.04699187 -0.21883354]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12257178  0.744668    0.04261519 -0.4963308 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13746513  0.54897183  0.03268858 -0.19052802]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.14844458 0.35339788 0.02887802 0.11228496]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15551253  0.5480944   0.03112372 -0.17114908]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.16647442 0.35254112 0.02770074 0.13118768]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17352524  0.5472555   0.03032449 -0.15262893]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.18447036 0.3517128  0.02727191 0.14946435]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.19150461 0.15621114 0.0302612  0.45062473]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19462883 -0.03932545  0.03927369  0.75269073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.19384232 0.15523359 0.05432751 0.47262073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.196947   0.34954786 0.06377992 0.1975438 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.20393795 0.15357436 0.0677308  0.5096449 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20700943 -0.04243314  0.07792369  0.8228797 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.20616078 0.15154122 0.09438129 0.5556875 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2091916  -0.04477032  0.10549504  0.87655115]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2082962  -0.24115576  0.12302607  1.2004497 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20347308 -0.04782093  0.14703506  0.9487178 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20251666 -0.24458365  0.16600941  1.2837499 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19762498 -0.05191828  0.19168441  1.0473078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.19658662 0.14021508 0.21263057 0.820397  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 23/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04600646  0.77108794  0.01264031 -1.0788078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06142822  0.5758014  -0.00893584 -0.78218526]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07294425  0.771045   -0.02457955 -1.077666  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08836515  0.9664829  -0.04613287 -1.37796   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1076948   0.7719664  -0.07369207 -1.1000539 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12313413  0.57788754 -0.09569315 -0.8313716 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13469188  0.38419458 -0.11232058 -0.5702523 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14237577  0.58069766 -0.12372562 -0.8961024 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15398973  0.77726024 -0.14164767 -1.224975  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16953494  0.9738931  -0.16614717 -1.5584756 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1890128   0.7811043  -0.19731669 -1.3218975 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20463488  0.97809505 -0.22375464 -1.6692854 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 24/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06926793  0.57666624 -0.0214023  -0.80132866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08080126  0.38184425 -0.03742888 -0.5154544 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08843814  0.57747275 -0.04773797 -0.81969315]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0999876   0.3830355  -0.06413183 -0.5423992 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10764831  0.5789974  -0.07497981 -0.8545796 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11922825  0.38497302 -0.09207141 -0.5863842 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12692772  0.58125573 -0.10379909 -0.90659195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13855283  0.3876807  -0.12193093 -0.6482544 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14630644  0.19444944 -0.13489601 -0.39631835]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15019543  0.39120162 -0.14282238 -0.72830474]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15801947  0.5879787  -0.15738848 -1.0623122 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16977903  0.39525077 -0.17863472 -0.8228749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17768405  0.5923078  -0.19509222 -1.1659967 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18953021  0.4001845  -0.21841216 -0.94027257]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 25/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08490668  0.9680484  -0.0454587  -1.4124637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10426765  0.7735184  -0.07370798 -1.1343304 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11973801  0.9695234  -0.09639458 -1.4491901 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13912848  0.7757095  -0.12537839 -1.188115  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15464267  0.5824158  -0.14914069 -0.9372152 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16629098  0.77919984 -0.16788499 -1.2727995 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18187499  0.5865701  -0.19334099 -1.0370439 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19360639  0.78366226 -0.21408187 -1.3836626 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 26/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0422969  -0.20322338  0.01655046  0.3563163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03823243 -0.00834059  0.02367678  0.06889778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03806562  0.18643405  0.02505474 -0.21622196]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0417943   0.38118902  0.0207303  -0.5008974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04941808  0.18578108  0.01071235 -0.20175421]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05313371  0.3807482   0.00667727 -0.49103874]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06074867  0.1855327  -0.00314351 -0.1962589 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06445932 -0.00954415 -0.00706869  0.09543072]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06426844  0.1856784  -0.00516007 -0.19947395]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06798201 -0.00936937 -0.00914955  0.09157676]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06779462  0.18588252 -0.00731802 -0.20397876]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07151227  0.38110834 -0.01139759 -0.49896115]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07913444  0.18614893 -0.02137681 -0.20989183]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08285742 -0.00866094 -0.02557465  0.07597192]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0826842  -0.20340711 -0.02405521  0.3604775 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07861605 -0.00795162 -0.01684566  0.06030755]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07845702 -0.20282803 -0.01563951  0.34762833]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07440046 -0.3977241  -0.00868694  0.6353388 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06644598 -0.20248206  0.00401983  0.3399329 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06239634 -0.39766097  0.01081849  0.63388073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05444312 -0.2026916   0.0234961   0.34462434]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05038929 -0.39813977  0.03038859  0.6446229 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04242649 -0.20345421  0.04328105  0.36166224]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03835741 -0.00897331  0.0505143   0.08293476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03817794  0.1853895   0.05217299 -0.1933927 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04188573 -0.01043846  0.04830514  0.11528146]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04167696 -0.20621808  0.05061077  0.4228045 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0375526  -0.01184829  0.05906685  0.14649637]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03731564  0.18238023  0.06199678 -0.12698278]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04096324 -0.01357255  0.05945713  0.18459709]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04069179  0.1806505   0.06314907 -0.08875258]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0443048  -0.0153171   0.06137402  0.2231664 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04399846 -0.21126013  0.06583735  0.5345602 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03977326 -0.01712279  0.07652855  0.26332727]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0394308  -0.21324894  0.0817951   0.57913285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03516582 -0.01936279  0.09337775  0.31329697]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03477857 -0.21568233  0.09964369  0.63390654]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03046492 -0.02208116  0.11232182  0.37419206]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.0300233  0.17128101 0.11980566 0.11892974]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03344892  0.36450085  0.12218426 -0.13368383]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04073893  0.5576801   0.11951058 -0.38545948]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05189253  0.7509206   0.11180139 -0.6382002 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06691094  0.9443207   0.09903739 -0.89368796]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08579736  1.1379701   0.08116363 -1.1536692 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10855676  1.3319451   0.05809024 -1.4198381 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13519566  1.5263021   0.02969348 -1.6938128 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 27/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164655  0.3820622  -0.01143824 -0.5200025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06928779  0.18710311 -0.02183829 -0.23094575]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07302985 -0.00770008 -0.02645721  0.05476942]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07287586  0.18779103 -0.02536182 -0.24614215]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07663167 -0.00695968 -0.03028466  0.03843441]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07649248  0.18858317 -0.02951597 -0.2636477 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08026414 -0.00610532 -0.03478893  0.0195814 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08014204 -0.20071153 -0.0343973   0.3010882 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07612781 -0.00511663 -0.02837553 -0.0022414 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07602548 -0.19982038 -0.02842036  0.28135538]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07202907 -0.39452565 -0.02279325  0.56494087]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06413855 -0.58932054 -0.01149444  0.85035664]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05235215 -0.39404374  0.0055127   0.5540815 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04447127 -0.19899963  0.01659433  0.26314053]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04049128 -0.39435446  0.02185714  0.56101096]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03260419 -0.5897762   0.03307736  0.8604991 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02080866 -0.7853327   0.05028734  1.1633962 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00510201 -0.98107207  0.07355526  1.4714124 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01451943 -1.1770124   0.10298351  1.786134  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03805968 -0.9831869   0.13870619  1.5271597 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05772342 -0.78998446  0.16924939  1.2807884 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0735231  -0.9868094   0.19486515  1.6213294 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0932593  -0.7944431   0.22729175  1.3951695 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 28/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0579058   0.18717085 -0.00685163 -0.23240055]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164922 -0.00785253 -0.01149964  0.0581133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06149217 -0.20280772 -0.01033737  0.34714594]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05743602 -0.39778113 -0.00339445  0.63655126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04948039 -0.5928556   0.00933657  0.9281633 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03762328 -0.7881023   0.02789984  1.2237656 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02186123 -0.59335065  0.05237515  0.9399532 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00999422 -0.3989723   0.07117421  0.66417676]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00201478 -0.20490885  0.08445775  0.39472616]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0020834  -0.01108049  0.09235227  0.12982102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00230501 -0.2073957   0.09494869  0.45015195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00645292 -0.40372333  0.10395173  0.7711899 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01452739 -0.6001105   0.11937553  1.0946872 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0265296  -0.7965851   0.14126927  1.4223149 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0424613  -0.9931432   0.16971557  1.75561   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06232416 -1.1897343   0.20482777  2.0959172 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08611885 -0.9971797   0.24674612  1.8729222 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 29/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04587036  0.18660757  0.01334879 -0.2199951 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04960252  0.3815362   0.00894889 -0.5084376 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05723324  0.18628931 -0.00121986 -0.21294801]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06095903 -0.00881517 -0.00547882  0.07934986]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06078272  0.18638489 -0.00389182 -0.2150566 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06451042  0.38156226 -0.00819296 -0.50896466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07214166  0.5767987  -0.01837225 -0.8042181 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08367763  0.38193336 -0.03445661 -0.51737064]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09131631  0.5775231  -0.04480403 -0.82070935]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10286677  0.7732286  -0.06121821 -1.127141  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11833134  0.57895976 -0.08376103 -0.8542705 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12991053  0.77511734 -0.10084644 -1.1720723 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14541288  0.5814406  -0.12428789 -0.9126318 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1570417   0.3881994  -0.14254053 -0.66145253]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16480568  0.19531828 -0.15576957 -0.41683024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16871205  0.39226508 -0.16410618 -0.7542849 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17655735  0.5892232  -0.17919187 -1.0937837 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18834181  0.7861945  -0.20106755 -1.4369107 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20406571  0.98314583 -0.22980577 -1.7851025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 30/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03801544  0.18541493  0.02616089 -0.19373725]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04172374 -0.0100713   0.02228614  0.10708216]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04152231  0.18472432  0.02442778 -0.17848712]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0452168   0.37948832  0.02085804 -0.46336493]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05280657  0.5743094   0.01159074 -0.74940115]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06429276  0.3790295  -0.00339728 -0.45309338]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07187334  0.18395576 -0.01245915 -0.16148326]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07555246  0.37925383 -0.01568881 -0.45807052]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08313753  0.18435715 -0.02485022 -0.1703738 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08682468  0.3798258  -0.0282577  -0.47079146]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0944212   0.57533526 -0.03767353 -0.7722452 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1059279   0.3807514  -0.05311843 -0.49164993]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11354293  0.18641737 -0.06295143 -0.2161695 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11727128  0.38238013 -0.06727482 -0.52802694]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12491888  0.188266   -0.07783536 -0.25727823]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12868421 -0.00566341 -0.08298092  0.00987464]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12857093 -0.19950332 -0.08278343  0.2752656 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12458087 -0.00330382 -0.07727812 -0.04233632]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12451479 -0.19723746 -0.07812484  0.22499879]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12057004 -0.39116096 -0.07362487  0.49205145]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11274682 -0.19508193 -0.06378384  0.17710355]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10884518 -0.38923585 -0.06024177  0.44900244]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10106046 -0.19331583 -0.05126172  0.13795516]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09719415 -0.3876675  -0.04850262  0.41403538]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0894408  -0.19189285 -0.04022191  0.10646454]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08560294  0.00378173 -0.03809262 -0.19863199]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08567858  0.19942723 -0.04206526 -0.5030839 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08966712  0.00492263 -0.05212694 -0.22394858]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08976557  0.20074935 -0.05660591 -0.5326081 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09378056  0.39661986 -0.06725807 -0.8425765 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10171296  0.5925922  -0.0841096  -1.1556293 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1135648   0.39866146 -0.10722218 -0.89046067]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12153803  0.20514487 -0.1250314  -0.63331574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12564093  0.01196844 -0.13769771 -0.38247657]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1258803  -0.18095721 -0.14534724 -0.13618259]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12226115 -0.37373063 -0.1480709   0.10734675]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11478654 -0.56645465 -0.14592396  0.34989554]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10345744 -0.7592326  -0.13892604  0.59323955]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0882728  -0.9521645  -0.12706126  0.8391362 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06922951 -0.7555579  -0.11027853  0.5093481 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05411835 -0.55906916 -0.10009158  0.18404919]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04293697 -0.75262713 -0.09641059  0.44355518]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02788442 -0.55628264 -0.08753949  0.12210535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01675877 -0.3600227  -0.08509738 -0.19686107]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 30, Return: 50.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 31/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01480714 -0.39951223  0.05906051  0.67564946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00681689 -0.20525856  0.0725735   0.40213072]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00271172 -0.01123697  0.08061612  0.1331823 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00248698  0.18264322  0.08327976 -0.13301802]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00613985 -0.01356679  0.0806194   0.18473293]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00586851  0.18031465  0.08431406 -0.08146819]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00947481  0.3741331   0.0826847  -0.34640414]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01695747  0.56798756  0.07575661 -0.61191076]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02831722  0.3718931   0.0635184  -0.29636163]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03575508 0.17592588 0.05759117 0.0156583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0392736   0.37017664  0.05790433 -0.2583123 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04667713 0.17427789 0.05273809 0.05205769]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05016269  0.36860552  0.05377924 -0.22353068]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0575348   0.5629192   0.04930862 -0.49877638]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06879319  0.36713803  0.0393331  -0.19097064]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07613595  0.56167585  0.03551368 -0.47099075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08736946  0.75627863  0.02609387 -0.75227207]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10249504  0.95103127  0.01104843 -1.0366307 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12151566  1.1460046  -0.00968419 -1.3258247 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14443575  1.3412474  -0.03620068 -1.6215224 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1712607   1.1465701  -0.06863113 -1.3403383 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1941921   0.95237595 -0.0954379  -1.0698946 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21323963  0.7586367  -0.11683579 -0.80862445]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22841236  0.9551493  -0.13300829 -1.1356554 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.24751534  0.7619942  -0.15572138 -0.88747096]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.26275522  0.9588478  -0.17347081 -1.2247758 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.28193218  0.76633114 -0.19796632 -0.99107987]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2972588   0.5743297  -0.21778792 -0.76652545]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 32/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04600646  0.38086733  0.01264031 -0.49371535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0536238   0.1855694   0.00276601 -0.1970757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05733519  0.38065168 -0.00117551 -0.4888848 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06494822  0.5757902  -0.0109532  -0.78193796]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07646403  0.771061   -0.02659196 -1.0780467 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09188525  0.57630014 -0.0481529  -0.79382586]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10341125  0.38187107 -0.06402942 -0.51667184]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11104868  0.5778334  -0.07436285 -0.82882476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12260534  0.3838026  -0.09093934 -0.5604254 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13028139  0.5800753  -0.10214785 -0.8803173 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1418829   0.77642554 -0.1197542  -1.2032866 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15741141  0.58303785 -0.14381993 -0.9504064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16907217  0.39011386 -0.16282806 -0.7061455 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17687444  0.5870724  -0.17695098 -1.0453357 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18861589  0.39468405 -0.19785768 -0.8130119 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19650957  0.20274208 -0.21411791 -0.58850753]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 33/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04229395  0.18668123  0.01661599 -0.22163221]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04602758  0.3815618   0.01218335 -0.50902784]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05365882  0.57651     0.00200279 -0.7978466 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06518902  0.38136062 -0.01395414 -0.5045343 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07281623  0.18643807 -0.02404483 -0.21628135]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07654499 -0.00833204 -0.02837046  0.06872085]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07637835 -0.203036   -0.02699604  0.35231948]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07231763 -0.39776385 -0.01994965  0.636369  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06436235 -0.592602   -0.00722227  0.9227032 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05251031 -0.3973832   0.01123179  0.62775934]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04456265 -0.2024198   0.02378698  0.33863476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04051425 -0.00764427  0.03055967  0.05354681]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04036137  0.18702647  0.03163061 -0.22933982]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04410189  0.3816825   0.02704381 -0.5118799 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05173555  0.18619023  0.01680622 -0.21079887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05545935 -0.00916793  0.01259024  0.08713779]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05527599  0.18577132  0.01433299 -0.20154646]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05899142 -0.00955266  0.01030206  0.09562316]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05880037 -0.20482074  0.01221453  0.3915385 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05470395 -0.40011388  0.0200453   0.68804735]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04670167 -0.20527582  0.03380625  0.40174186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04259615 -0.01064927  0.04184108  0.119906  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04238317  0.183849    0.0442392  -0.15928839]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04606015 -0.01187747  0.04105343  0.14701617]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0458226   0.18263327  0.04399376 -0.1324378 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04947527 -0.01309035  0.041345    0.17379357]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04921346  0.18141623  0.04482087 -0.105565  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05284178  0.37586817  0.04270957 -0.38377705]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06035915  0.5703585   0.03503403 -0.6626938 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07176632  0.37476712  0.02178016 -0.3591888 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07926166  0.5695728   0.01459638 -0.6449251 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09065311  0.3742505   0.00169788 -0.34768173]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09813812  0.5693483  -0.00525576 -0.63982874]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10952509  0.37429997 -0.01805233 -0.34880558]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11701109  0.56967396 -0.02502844 -0.6471259 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12840457  0.37490952 -0.03797096 -0.3624283 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13590276  0.57055    -0.04521953 -0.6668383 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14731376  0.37608516 -0.05855629 -0.3887292 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15483546  0.18184112 -0.06633088 -0.11506826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15847228  0.3778477  -0.06863225 -0.42791855]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16602924  0.57387114 -0.07719062 -0.74142355]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17750667  0.37989488 -0.09201908 -0.47399703]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18510456  0.5761877  -0.10149903 -0.79420596]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19662832  0.38259447 -0.11738314 -0.53510016]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2042802   0.1893018  -0.12808515 -0.28158784]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20806624  0.38599595 -0.13371691 -0.61176634]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 34/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01480714 -0.00934462  0.05906051  0.09118061]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01462025  0.18488325  0.06088412 -0.18229952]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01831791 -0.01105466  0.05723813  0.12895164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01809682 -0.20694786  0.05981717  0.43912905]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01395786 -0.40286317  0.06859975  0.75005215]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0059006  -0.59886086  0.08360079  1.0635097 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00607662 -0.794984    0.10487098  1.381217  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0219763  -0.60131544  0.13249533  1.1230857 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03400261 -0.7979018   0.15495704  1.4542205 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04996064 -0.9945495   0.18404146  1.791033  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06985164 -0.8019071   0.2198621   1.5607527 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 35/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04229395  0.18668123  0.01661599 -0.22163221]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04602758 -0.00867424  0.01218335  0.0762454 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0458541   0.18627095  0.01370825 -0.21256886]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04957952 -0.00904428  0.00945688  0.08440655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04939863 -0.20430051  0.01114501  0.38005808]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04531262 -0.00933858  0.01874617  0.09090991]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04512585  0.18550973  0.02056437 -0.19580014]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04883604 -0.00990024  0.01664836  0.10329834]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04863804 -0.20525678  0.01871433  0.4011869 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0445329  -0.40063912  0.02673807  0.6997108 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03652012 -0.5961214   0.04073229  1.0006894 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02459769 -0.7917633   0.06074607  1.3058809 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00876243 -0.59746176  0.08686369  1.0328146 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00318681 -0.4035958   0.10751998  0.7686178 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01125873 -0.6000205   0.12289234  1.0931034 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02325914 -0.7965283   0.14475441  1.4216815 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0391897  -0.60346293  0.17318805  1.1775217 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05125896 -0.80035836  0.19673847  1.5191097 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06726613 -0.99723905  0.22712067  1.8662025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 36/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01234624 -0.9848512   0.09879284  1.5576583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03204326 -1.1810076   0.129946    1.8794562 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05566342 -0.98751956  0.16753513  1.6297678 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07541381 -1.1841673   0.20013048  1.9696347 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.09909716 -1.3807625   0.23952317  2.317088  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 37/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03801544  0.18541493  0.02616089 -0.19373725]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04172374  0.38015306  0.02228614 -0.4780541 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0493268   0.18472368  0.01272506 -0.17843123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05302127  0.37966123  0.00915643 -0.4670728 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 6.0614500e-02  5.7465261e-01 -1.8502203e-04 -7.5685573e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07210755  0.7697771  -0.01532214 -1.0495968 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0875031   0.965099   -0.03631407 -1.3470497 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10680507  1.1606581  -0.06325506 -1.6508691 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13001823  0.9663297  -0.09627245 -1.378544  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14934483  1.1625129  -0.12384333 -1.6997166 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17259508  0.96901685 -0.15783766 -1.4480124 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19197543  1.1656879  -0.1867979  -1.7855637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21528919  0.9730914  -0.22250918 -1.5562943 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 38/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00697355 -0.01034828  0.07142465  0.11344658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00676658  0.18368141  0.07369358 -0.1558749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01044021 -0.01241407  0.07057608  0.15911628]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01019193  0.18163021  0.07375841 -0.1104934 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01382453 -0.01446691  0.07154854  0.20451784]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0135352  -0.21053524  0.0756389   0.5188852 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00932449 -0.01655513  0.0860166   0.25096363]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00899339  0.17723991  0.09103587 -0.0133965 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01253819  0.37094635  0.09076794 -0.27602538]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01995711  0.56466395  0.08524743 -0.5387565 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03125039  0.36845356  0.0744723  -0.22047722]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03861947  0.56243634  0.07006276 -0.4887703 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04986819  0.3663995   0.06028735 -0.17485484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.05719618 0.17046888 0.05679026 0.13622092]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06060556  0.36473337  0.05951468 -0.13801867]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.06790023 0.16881172 0.0567543  0.17283046]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07127646  0.3630774   0.06021091 -0.10142206]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.07853801 0.16714653 0.05818247 0.20963296]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08188094 -0.02875699  0.06237513  0.520087  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.0813058  0.16543396 0.07277687 0.24769345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08461448  0.35944515  0.07773074 -0.0211755 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09180338  0.5533713   0.07730722 -0.28835654]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10287081  0.7473106   0.07154009 -0.5556909 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11781702  0.55126095  0.06042628 -0.24135384]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.12884223 0.35533023 0.0555992  0.06976037]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.13594885 0.15945709 0.05699441 0.37945387]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.13913798 0.35372528 0.06458349 0.10527198]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14621249  0.54786515  0.06668893 -0.16635631]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.15716979 0.3518551  0.0633618  0.1465972 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.16420689 0.15588568 0.06629375 0.45857736]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16732462 -0.04010771  0.07546529  0.77139777]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.16652246 0.15389913 0.09089325 0.5033815 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.16960044 0.34763035 0.10096087 0.24067003]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17655304  0.541176    0.10577428 -0.01853802]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.18737657 0.3447087  0.10540351 0.30555564]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.19427074 0.5381831  0.11151463 0.04788598]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2050344   0.731544    0.11247235 -0.20763648]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.21966529 0.5350085  0.10831962 0.11830134]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23036546  0.72842515  0.11068565 -0.13833958]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.24493395 0.53190625 0.10791885 0.18711008]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.25557208  0.72533196  0.11166105 -0.06967399]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.27007872 0.52880096 0.11026757 0.25604677]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [2.8065473e-01 7.2219002e-01 1.1538851e-01 7.8017481e-05]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.29509854  0.9154843   0.11539007 -0.25408635]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.31340823  1.1087857   0.11030834 -0.50826097]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 39/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106766 -0.5943515   0.06362975  0.96259123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-8.1936747e-04 -7.9026806e-01  8.2881577e-02  1.2745659e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01662473 -0.59629536  0.1083729   1.008945  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02855064 -0.7926839   0.1285518   1.3335989 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04440431 -0.5993953   0.15522377  1.0837486 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05639222 -0.7961865   0.17689875  1.4208391 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07231595 -0.60363865  0.20531553  1.188262  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08438872 -0.80074275  0.22908077  1.537648  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 40/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0147878  -0.40000218  0.05949304  0.68648887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00678776 -0.20575435  0.07322282  0.41311327]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00267267 -0.4018337   0.08148509  0.7279525 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.005364   -0.20792712  0.09604413  0.4619878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00952254 -0.01428446  0.10528389  0.20105603]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00980823 -0.21074231  0.10930501  0.52500737]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01402308 -0.01731458  0.11980516  0.268669  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01436937  0.17591204  0.12517853  0.0160452 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01085113  0.36903718  0.12549944 -0.23466939]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00347039  0.5621635   0.12080605 -0.48528042]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00777288  0.755392    0.11110044 -0.7375802 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02288072  0.94881856  0.09634884 -0.99333483]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0418571   0.7525489   0.07648214 -0.672013  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05690807  0.94652903  0.06304188 -0.9396688 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07583866  0.75061655  0.04424851 -0.62786174]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09085099  0.5549059   0.03169127 -0.32157826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1019491   0.7495625   0.02525971 -0.60410094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11694036  0.5540966   0.01317769 -0.30357003]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12802228  0.74902827  0.00710629 -0.592068  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14300285  0.55380756 -0.00473507 -0.29715514]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.154079    0.7489967  -0.01067818 -0.59132767]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16905893  0.9442665  -0.02250473 -0.88735497]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18794426  0.7494571  -0.04025183 -0.6018308 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20293342  0.5549207  -0.05228845 -0.32209325]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21403182  0.7507467  -0.05873031 -0.6307967 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22904676  0.94663686 -0.07134625 -0.94138205]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.24797949  0.7525452  -0.09017389 -0.6719432 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2630304   0.9487972  -0.10361275 -0.99160045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.28200635  0.75520295 -0.12344476 -0.7331741 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2971104   0.56198317 -0.13810824 -0.48175064]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.30835006  0.3690532  -0.14774325 -0.23558608]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.31573114  0.5659432  -0.15245497 -0.5709822 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.32705     0.76283723 -0.16387461 -0.9075461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.34230673  0.57026756 -0.18202554 -0.6705253 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.35371208  0.37808028 -0.19543605 -0.44022748]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.3612737   0.18618312 -0.20424059 -0.21495457]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.36499736  0.38355017 -0.20853968 -0.564482  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.37266836  0.19186983 -0.21982932 -0.34405804]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 40, Return: 43.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 41/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0579058   0.18717085 -0.00685163 -0.23240055]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06164922  0.38239002 -0.01149964 -0.5272368 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06929702  0.18743175 -0.02204438 -0.23819955]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07304566  0.38286158 -0.02680837 -0.5377536 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08070289  0.1881266  -0.03756344 -0.25363678]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08446542 -0.00643943 -0.04263617  0.02696532]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08433663  0.1892672  -0.04209687 -0.27885908]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08812197 -0.00522972 -0.04767405  0.00025501]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08801738  0.19054238 -0.04766895 -0.30708036]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09182823  0.38631004 -0.05381056 -0.6144076 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09955443  0.582141   -0.0660987  -0.9235415 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11119725  0.7780906  -0.08456954 -1.236243  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12675907  0.9741913  -0.1092944  -1.5541766 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14624289  1.1704403  -0.14037792 -1.8788624 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16965169  0.9771003  -0.17795518 -1.6328408 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1891937   1.1738094  -0.21061198 -1.9752878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 42/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04600646  0.38086733  0.01264031 -0.49371535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0536238   0.57580876  0.00276601 -0.782388  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06513998  0.77089256 -0.01288175 -1.0741993 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08055783  0.57594323 -0.03436574 -0.7855867 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0920767   0.77152    -0.05007747 -1.0888803 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10750709  0.5770928  -0.07185508 -0.81232166]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11904895  0.3830248  -0.08810151 -0.54307806]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12670945  0.18924429 -0.09896307 -0.27940297]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13049433 -0.00433689 -0.10455114 -0.01949962]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1304076   0.19211698 -0.10494112 -0.3432534 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13424994 -0.00136769 -0.11180619 -0.0854191 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13422258  0.19516458 -0.11351457 -0.41117942]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13812587  0.00181948 -0.12173817 -0.15632994]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13816226 -0.1913682  -0.12486476  0.09560424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13433489 -0.38449988 -0.12295268  0.3464314 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1266449  -0.57767797 -0.11602405  0.597954  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11509134 -0.38114008 -0.10406497  0.2710946 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10746854 -0.574635   -0.09864308  0.5292269 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09597584 -0.7682409  -0.08805854  0.78927   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08061102 -0.9620503  -0.07227314  1.0530019 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06137001 -0.7660484  -0.0512131   0.7385366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04604905 -0.57025796 -0.03644237  0.43018606]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03464388 -0.76484543 -0.02783865  0.7111618 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01934698 -0.5693493  -0.01361541  0.40984762]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00795999 -0.37403697 -0.00541846  0.1129035 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00047925 -0.17883779 -0.00316039 -0.18148398]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0030975  -0.37391436 -0.00679007  0.11020029]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01057579 -0.5689384  -0.00458607  0.40073326]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02195456 -0.763995    0.0034286   0.6919668 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03723446 -0.9591643   0.01726793  0.98572713]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05641774 -0.7642779   0.03698248  0.6985175 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0717033  -0.56968766  0.05095283  0.41770217]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08309706 -0.7654933   0.05930687  0.72600305]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.09840692 -0.9613829   0.07382693  1.036747  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.11763458 -0.7673158   0.09456187  0.76812464]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.1329809  -0.573614    0.10992436  0.5066297 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.14445317 -0.7700992   0.12005696  0.83183146]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.15985516 -0.96663934  0.13669358  1.1597315 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.17918794 -1.1632514   0.15988822  1.4919615 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.20245297 -0.97039557  0.18972746  1.2531747 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.22186089 -0.77814114  0.21479094  1.0254136 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 43/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106766 -0.5943515   0.06362975  0.96259123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-8.1936747e-04 -7.9026806e-01  8.2881577e-02  1.2745659e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01662473 -0.9863437   0.1083729   1.5920091 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0363516  -1.1825722   0.14021307  1.9164245 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06000305 -1.3788975   0.17854157  2.24911   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08758099 -1.5751965   0.22352377  2.5910907 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 44/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06571045  0.5774066  -0.01855621 -0.817635  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07725858  0.77277756 -0.03490891 -1.1160963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09271413  0.9683399  -0.05723084 -1.4195222 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11208093  1.1641215  -0.08562128 -1.7295305 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13536336  0.9700758  -0.12021189 -1.46467   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15476488  1.1664475  -0.14950529 -1.7923591 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17809382  1.3628961  -0.18535247 -2.1275358 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20535176  1.1700374  -0.22790319 -1.897385  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 45/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08490668  0.9680484  -0.0454587  -1.4124637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10426765  0.7735184  -0.07370798 -1.1343304 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11973801  0.9695234  -0.09639458 -1.4491901 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13912848  0.7757095  -0.12537839 -1.188115  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15464267  0.97221375 -0.14914069 -1.5173224 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17408694  1.1687917  -0.17948714 -1.852601  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19746278  0.9760406  -0.21653916 -1.6206056 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 46/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00326062 -0.5946414   0.0754039   0.9696814 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0086322  -0.7906901   0.09479753  1.2850666 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02444601 -0.9868823   0.12049886  1.6058624 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04418365 -0.793374    0.15261611  1.3530456 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06005113 -0.99004674  0.17967702  1.6893209 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07985207 -0.79739916  0.21346344  1.4575429 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 47/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00454311 -0.5947744   0.087106    0.9734366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01643859 -0.4009223   0.10657474  0.7093377 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02445704 -0.20742524  0.12076149  0.45201254]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02860554 -0.01419956  0.12980174  0.19970252]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02888953  0.17885005  0.1337958  -0.04938124]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02531253  0.37182507  0.13280816 -0.29703802]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01787603  0.17508458  0.12686741  0.03440487]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01437434  0.36818048  0.1275555  -0.21571212]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00701073  0.17148764  0.12324126  0.1143299 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00358098 -0.02516491  0.12552786  0.44321272]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00408428  0.16797811  0.13439211  0.19258608]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00072472 -0.02878477  0.13824384  0.5244615 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00130041  0.16414861  0.14873306  0.27833682]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00198256 0.35687038 0.1542998  0.03601223]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.00911997 0.1599111  0.15502004 0.3731255 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01231819 -0.03703411  0.16248256  0.7103944 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01157751 -0.23398872  0.17669044  1.049495  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00689773 -0.04159442  0.19768034  0.8170738 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00606585 0.15035199 0.21402182 0.59250176]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 48/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04600646  0.77108794  0.01264031 -1.0788078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06142822  0.96604073 -0.00893584 -1.3674974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08074903  0.77103174 -0.03628579 -1.0776228 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09616967  0.5764074  -0.05783825 -0.7965441 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10769781  0.38212478 -0.07376913 -0.5226025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11534031  0.5782033  -0.08422118 -0.8375888 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12690437  0.7743682  -0.10097296 -1.155525  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14239174  0.9706512  -0.12408345 -1.4780855 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16180477  1.1670506  -0.15364516 -1.8068064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18514578  1.3635174  -0.1897813  -2.1430292 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21241613  1.5599395  -0.23264188 -2.4878337 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 49/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06571045  0.96765006 -0.01855621 -1.4029963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08506345  0.7727635  -0.04661614 -1.116172  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10051872  0.96846527 -0.06893958 -1.4231058 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11988802  0.7742602  -0.09740169 -1.1527425 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13537323  0.97050834 -0.12045655 -1.4743103 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1547834   0.77704686 -0.14994276 -1.2215494 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17032434  0.5841407  -0.17437373 -0.9793551 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18200715  0.39173058 -0.19396085 -0.7461231 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18984176  0.19973828 -0.2088833  -0.52020246]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19383653  0.00807395 -0.21928735 -0.29992133]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 50/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06164655  0.7723054  -0.01143824 -1.1053604 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07709266  0.9675759  -0.03354545 -1.4016098 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09644417  1.1630982  -0.06157764 -1.7045888 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11970614  0.9687365  -0.09566942 -1.4316912 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13908087  1.1649002  -0.12430324 -1.752675  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16237888  1.3611941  -0.15935674 -2.0812953 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18960276  1.5575306  -0.20098265 -2.4187214 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.22075337  1.3646371  -0.24935707 -2.1938972 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 50, Return: 13.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 51/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04603718  0.38189536  0.01197105 -0.516387  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05367509  0.5768467   0.00164331 -0.80527365]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06521202  0.38170227 -0.01446216 -0.51207423]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07284606  0.57702494 -0.02470364 -0.8092793 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08438656  0.38225    -0.04088923 -0.52446824]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09203156  0.18772665 -0.0513786  -0.24494512]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09578609  0.38354334 -0.0562775  -0.55338126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10345697  0.5794085  -0.06734512 -0.8632508 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11504514  0.77537954 -0.08461014 -1.1763254 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13055272  0.5814525  -0.10813665 -0.91132116]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14218177  0.7778584  -0.12636307 -1.2359394 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15773894  0.58456624 -0.15108186 -0.9853649 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16943027  0.39175522 -0.17078915 -0.74369216]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17726538  0.19935027 -0.185663   -0.50925195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18125238  0.39653575 -0.19584803 -0.85421884]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18918309  0.5937103  -0.21293242 -1.2015386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 52/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03041884 -0.00883249  0.03558456  0.07978204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03024219 -0.20444602  0.0371802   0.38347623]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02615327 -0.00987113  0.04484972  0.10274406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02595585 -0.20560618  0.0469046   0.4092328 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02184373 -0.40136066  0.05508926  0.7163265 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01381651 -0.20704272  0.06941579  0.4414798 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00967566 -0.01296823  0.07824539  0.171461  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00941629  0.18095168  0.08167461 -0.09554822]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01303533  0.37481382  0.07976364 -0.36138642]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0205316   0.56871676  0.07253592 -0.6278904 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03190594  0.7627554   0.05997811 -0.89687616]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04716105  0.56687385  0.04204058 -0.5859592 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05849852  0.7613825   0.0303214  -0.86510813]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07372618  0.95607895  0.01301924 -1.1481054 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09284776  1.1510285  -0.00994287 -1.4366775 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11586832  1.3462716  -0.03867642 -1.7324507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14279376  1.5418131  -0.07332543 -2.0369117 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17363001  1.3475187  -0.11406367 -1.7677916 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20058039  1.1538551  -0.1494195  -1.5126457 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22365749  1.350437   -0.17967242 -1.8479993 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.25066623  1.5470259  -0.2166324  -2.1906755 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 53/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03448966  0.1866822   0.02831649 -0.22171655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0382233   0.3813882   0.02388216 -0.50533456]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04585107  0.5761656   0.01377547 -0.79039663]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05737438  0.7710957  -0.00203246 -1.0787143 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07279629  0.57600063 -0.02360675 -0.7866698 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08431631  0.38121083 -0.03934014 -0.50150615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09194052  0.18666485 -0.04937027 -0.22147602]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09567382 -0.00771793 -0.05379979  0.05523391]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09551946 -0.20202886 -0.05269511  0.33046907]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09147888 -0.00619793 -0.04608573  0.02164525]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09135492 -0.20062971 -0.04565282  0.29943883]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08734233 -0.3950722  -0.03966404  0.57738143]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07944088 -0.5896164  -0.02811642  0.85731006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06764856 -0.39412293 -0.01097022  0.5559206 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.9766099e-02 -1.9884868e-01  1.4819535e-04  2.5980166e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05578912 -0.39397275  0.00534423  0.5525313 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04790967 -0.5891693   0.01639486  0.84689325]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03612628 -0.39427483  0.03333272  0.55941063]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02824079 -0.5898484   0.04452093  0.862406  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01644382 -0.39536     0.06176905  0.5840473 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00853662 -0.20115526  0.07345     0.31144434]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00451351 -0.3972426   0.07967889  0.6263572 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00343134 -0.59338105  0.09220603  0.9430322 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01529896 -0.7896163   0.11106668  1.2632048 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03109128 -0.59607553  0.13633077  1.007269  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04301279 -0.7927282   0.15647615  1.3394682 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05886736 -0.5998838   0.18326552  1.0995522 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07086504 -0.7968819   0.20525657  1.4436799 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08680268 -0.9938524   0.23413016  1.7928562 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 54/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01480714 -0.39951223  0.05906051  0.67564946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00681689 -0.595403    0.0725735   0.986327  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00509117 -0.7914179   0.09230004  1.3008933 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02091952 -0.59758043  0.1183179   1.0384724 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03287113 -0.40421253  0.13908735  0.7851528 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04095538 -0.21124767  0.15479042  0.5392607 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04518034 -0.01860166  0.16557562  0.2990735 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04555237 -0.21564898  0.1715571   0.63905925]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04986535 -0.02328162  0.18433829  0.40493485]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05033098 -0.22047368  0.19243698  0.74960065]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05474046 -0.41765526  0.20742899  1.0961413 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06309356 -0.2257776   0.22935182  0.8750423 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 55/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00454311 -0.5947744   0.087106    0.9734366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01643859 -0.4009223   0.10657474  0.7093377 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02445704 -0.5973463   0.12076149  1.0335757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03640397 -0.4040192   0.141433    0.78111464]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04448435 -0.21109524  0.1570553   0.53606105]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04870626 -0.40803635  0.16777653  0.8738215 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05686698 -0.21554403  0.18525295  0.6382325 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06117786 -0.41269976  0.1980176   0.9830594 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06943186 -0.60984415  0.21767879  1.3308347 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 56/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106376  0.18562292  0.06371707 -0.19868816]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01477622 -0.01034977  0.05974331  0.11339449]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01456922 -0.20627463  0.0620112   0.42431155]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01044373 -0.4022177   0.07049743  0.7358808 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00239938 -0.20813668  0.08521505  0.4661915 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00176336 -0.40435278  0.09453887  0.78447056]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00985041 -0.21064821  0.11022829  0.52296424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01406338 -0.01723622  0.12068757  0.26694852]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0144081   0.1759751   0.12602654  0.01463571]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0108886  -0.02070779  0.12631926  0.34427404]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01130276  0.17241205  0.13320474  0.09394153]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00785451  0.36539826  0.13508357 -0.15392686]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00054655  0.16862673  0.13200504  0.17813544]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00282599 -0.02811299  0.13556774  0.50937444]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00226373 0.16486475 0.14575523 0.2622987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00556102 0.35763803 0.1510012  0.01890533]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01271378  0.5503082   0.1513793  -0.2225855 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02371995 0.35338324 0.1469276  0.11375964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03078761 0.15649496 0.1492028  0.44895148]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.03391751 0.34922636 0.15818182 0.20676935]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04090204 0.15223771 0.1623172  0.544875  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04394679 -0.04474819  0.1732147   0.88398194]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04305183 0.14765221 0.19089435 0.6503709 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04600487 -0.04954367  0.20390177  0.9965743 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.045014   0.14235488 0.22383325 0.77422214]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 57/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00082953 -0.4006148   0.08311619  0.70108664]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00884183 -0.59678453  0.09713792  1.0187335 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02077752 -0.40308207  0.11751259  0.75806314]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02883916 -0.20975865  0.13267384  0.5045472 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03303434 -0.0167315   0.14276479  0.2564428 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03336896 -0.2135725   0.14789365  0.5905308 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03764042 -0.02079691  0.15970427  0.34784645]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03805635  0.1717359   0.16666119  0.10947611]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03462164 -0.02533344  0.16885072  0.44975615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0351283   0.16704834  0.17784584  0.21469395]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03178734  0.35924098  0.18213972 -0.01703638]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02460252  0.16203776  0.181799    0.32712886]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02136176 -0.03514389  0.18834157  0.671182  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02206464 -0.23231547  0.20176521  1.0167549 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02671095 -0.04037115  0.2221003   0.79359627]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 58/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00082953 -0.01053289  0.08311619  0.11762639]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00104019 -0.20674129  0.08546872  0.43533102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00517502 -0.01292668  0.09417533  0.17076753]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00543355  0.18073     0.09759068 -0.09078366]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00181895  0.3743275   0.09577502 -0.35115203]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0056676   0.5679662   0.08875197 -0.61216414]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01702692  0.37172335  0.07650869 -0.29289955]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02446139 0.17559867 0.0706507  0.0228985 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02797336  0.36964002  0.07110867 -0.2466834 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03536616 0.17357835 0.066175   0.06755424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03883773  0.3676922   0.06752609 -0.203538  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04619158 0.1716728  0.06345532 0.1096586 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04962503 -0.02429835  0.0656485   0.42166704]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04913906 -0.220286    0.07408184  0.73430204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04473335 -0.02626144  0.08876788  0.46582362]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04420812 -0.22251815  0.09808435  0.7851131 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03975775 -0.41884112  0.11378662  1.1069727 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03138093 -0.22538374  0.13592607  0.85204434]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02687326 -0.4220709   0.15296696  1.1841931 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01843184 -0.6188099   0.17665082  1.5206528 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00605564 -0.42620775  0.20706387  1.2879113 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00246852 -0.623273    0.23282209  1.6376375 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 59/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00454311 -0.5947744   0.087106    0.9734366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01643859 -0.4009223   0.10657474  0.7093377 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02445704 -0.20742524  0.12076149  0.45201254]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02860554 -0.01419956  0.12980174  0.19970252]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02888953 -0.21091603  0.1337958   0.53034955]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03310785 -0.40764123  0.14440279  0.8620188 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04126068 -0.604403    0.16164316  1.1963966 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05334874 -0.41169956  0.18557109  0.95842594]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06158273 -0.21949172  0.20473962  0.7293095 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06597257 -0.02769926  0.2193258   0.5074033 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 60/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02668534 -0.20369677  0.04001765  0.36697814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0226114  -0.39936385  0.04735721  0.67200583]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01462412 -0.595111    0.06079733  0.9792152 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0027219  -0.40085447  0.08038163  0.70623213]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00529519 -0.20693277  0.09450628  0.43989557]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00943384 -0.01326649  0.10330418  0.17843597]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00969917  0.18023708  0.10687291 -0.07995456]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00609443  0.37367758  0.10527381 -0.33709896]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00137912  0.5671562   0.09853183 -0.5948168 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01272225  0.3708033   0.0866355  -0.2727945 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02013831 0.17455886 0.08117961 0.04590711]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02362949  0.3684286   0.08209775 -0.22009917]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03099806 0.17223492 0.07769577 0.09730974]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03444276 -0.02390964  0.07964196  0.4134586 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.03396457 0.1699984  0.08791114 0.14690843]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03736454  0.36375862  0.0908493  -0.11679694]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04463971  0.55746937  0.08851337 -0.3794918 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05578909  0.7512302   0.08092353 -0.6430062 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0708137   0.5550791   0.06806341 -0.32597694]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08191528  0.7491693   0.06154387 -0.59644294]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09689867  0.94337845  0.04961501 -0.8691226 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11576623  0.7476179   0.03223255 -0.5612623 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13071859  0.942273    0.02100731 -0.84361863]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14956406  0.74687076  0.00413493 -0.5444042 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16450147  0.55169094 -0.00675315 -0.25042135]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17553529  0.74690866 -0.01176158 -0.5452267 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19047347  0.551954   -0.02266611 -0.25627264]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20151255  0.35716283 -0.02779156  0.02917572]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2086558   0.16245022 -0.02720805  0.31296217]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21190481  0.357949   -0.02094881  0.01182437]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21906379  0.55336505 -0.02071232 -0.28739384]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23013109  0.74877614 -0.02646019 -0.58653665]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.24510661  0.5540346  -0.03819093 -0.30230492]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2561873   0.74967945 -0.04423703 -0.6067836 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2711809   0.555203   -0.0563727  -0.32835585]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.28228495  0.36092702 -0.06293982 -0.05396957]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.28950348  0.16676137 -0.06401921  0.21821015]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.29283872 -0.02738978 -0.05965501  0.49003148]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.29229093  0.16852076 -0.04985438  0.17916025]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.29566133 -0.02585363 -0.04627117  0.45570847]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.29514426 -0.22029187 -0.037157    0.73345476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.29073843 -0.02467677 -0.0224879   0.429313  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.29024488 -0.21947315 -0.01390165  0.7148228 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 2.8585541e-01 -4.1439995e-01  3.9481136e-04  1.0030978e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.27756742 -0.6095272   0.02045677  1.2959046 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2653769  -0.41467097  0.04637486  1.0096954 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 60, Return: 50.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 61/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08490668  0.5778198  -0.0454587  -0.82727844]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09646308  0.383348   -0.06200428 -0.5492323 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10413004  0.1891493  -0.07298892 -0.2767116 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10791302  0.38523257 -0.07852315 -0.59149307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11561768  0.58136094 -0.09035301 -0.90784127]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12724489  0.38757065 -0.10850984 -0.64486915]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13499631  0.19411477 -0.12140723 -0.3882314 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1388786   0.390732   -0.12917185 -0.71659243]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14669324  0.5873826  -0.1435037  -1.0469819 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15844089  0.7840872  -0.16444334 -1.3810513 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17412263  0.5913546  -0.19206436 -1.1439812 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18594973  0.78839475 -0.21494399 -1.4902271 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 62/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01480714 -0.00934462  0.05906051  0.09118061]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01462025 -0.2052612   0.06088412  0.40189677]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01051502 -0.4011915   0.06892206  0.71313655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00249119 -0.20708805  0.08318479  0.44291967]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00165057 -0.01323562  0.09204318  0.17757463]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00191528 -0.20954598  0.09559467  0.4978162 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0061062 -0.4058766  0.105551   0.8190296]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01422373 -0.6022726   0.12193159  1.1429585 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02626918 -0.40893635  0.14479077  0.8908664 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03444791 -0.21604413  0.16260809  0.6469755 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03876879 -0.41301337  0.1755476   0.9861283 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04702906 -0.22062132  0.19527017  0.7533218 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05144149 -0.41782242  0.21033661  1.1005471 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 63/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04600646  0.77108794  0.01264031 -1.0788078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06142822  0.96604073 -0.00893584 -1.3674974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08074903  0.77103174 -0.03628579 -1.0776228 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09616967  0.9666137  -0.05783825 -1.3814683 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11550194  1.1624078  -0.08546761 -1.6916633 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13875009  0.96837074 -0.11930088 -1.426767  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15811752  1.1647476  -0.14783622 -1.7542298 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18141246  1.3612049  -0.18292081 -2.089007  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20863655  1.1683406  -0.22470096 -1.8580164 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 64/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00697355 -0.40047646  0.07142465  0.69745135]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00103598 -0.5965123   0.08537367  1.0117364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01296623 -0.4026268   0.1056084   0.74703676]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02101876 -0.599035    0.12054913  1.0709989 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03299946 -0.40569526  0.14196911  0.81844956]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04111337 -0.6024453   0.1583381   1.1522043 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05316228 -0.4097028   0.1813822   0.9130643 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06135633 -0.21743652  0.19964348  0.68243045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06570506 -0.02556435  0.21329209  0.45864487]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 65/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01234624 -0.9848512   0.09879284  1.5576583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03204326 -1.1810076   0.129946    1.8794562 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05566342 -0.98751956  0.16753513  1.6297678 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07541381 -1.1841673   0.20013048  1.9696347 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.09909716 -1.3807625   0.23952317  2.317088  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 66/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04587036 -0.20363224  0.01334879  0.36532268]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04179772 -0.00870251  0.02065525  0.07687856]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04162367  0.18611734  0.02219282 -0.20921667]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04534601 -0.00931479  0.01800848  0.0903835 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04515972  0.18554446  0.01981615 -0.19656378]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04887061 -0.00985524  0.01588488  0.1023038 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0486735   0.1850355   0.01793095 -0.18532547]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05237421  0.37989637  0.01422444 -0.47229835]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05997214  0.57481456  0.00477848 -0.76046425]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07146844  0.3796271  -0.01043081 -0.4662815 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07906097  0.18465407 -0.01975644 -0.17690448]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08275405  0.3800531  -0.02329453 -0.47575384]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09035512  0.18526769 -0.0328096  -0.19050297]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09406047 -0.00936989 -0.03661966  0.09165198]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09387308  0.18625729 -0.03478662 -0.21235582]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09759822 -0.00835049 -0.03903374  0.06915402]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09743121  0.18730871 -0.03765066 -0.23558423]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10117739  0.3829478  -0.04236234 -0.53990155]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10883634  0.18844613 -0.05316038 -0.26086164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11260526  0.38428506 -0.05837761 -0.56982684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12029096  0.19002834 -0.06977414 -0.29609138]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12409153  0.386072   -0.07569597 -0.6099383 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13181297  0.19208525 -0.08789474 -0.34202477]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13565467  0.3883406  -0.09473523 -0.66107917]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14342149  0.19465564 -0.10795682 -0.399665  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1473146   0.00121747 -0.11595012 -0.14287466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14733896  0.19779263 -0.11880761 -0.4697705 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1512948   0.39437488 -0.12820302 -0.7974125 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1591823   0.59100085 -0.14415127 -1.1275195 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17100231  0.78768647 -0.16670166 -1.4617218 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18675604  0.9844129  -0.1959361  -1.8015044 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20644431  1.1811116  -0.23196618 -2.1481442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 67/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03041884 -0.00883249  0.03558456  0.07978204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03024219  0.18576175  0.0371802  -0.20146489]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03395743  0.38033277  0.0331509  -0.48219126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04156408  0.5749715   0.02350708 -0.7642444 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05306351  0.769762    0.00822219 -1.049439  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06845875  0.5745319  -0.01276659 -0.7541864 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07994939  0.76982754 -0.02785032 -1.0508592 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09534594  0.9653076  -0.0488675  -1.3521526 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1146521   1.161008   -0.07591055 -1.659714  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13787225  0.9668484  -0.10910483 -1.3916097 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15720922  0.77324104 -0.13693702 -1.1349386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17267404  0.58015025 -0.1596358  -0.88814753]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18427704  0.38751283 -0.17739874 -0.64959925]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1920273   0.19524744 -0.19039074 -0.41760942]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19593225  0.39248532 -0.19874293 -0.7637575 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20378196  0.58970815 -0.21401808 -1.111815  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 68/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06926793  0.57666624 -0.0214023  -0.80132866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08080126  0.38184425 -0.03742888 -0.5154544 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08843814  0.57747275 -0.04773797 -0.81969315]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0999876   0.3830355  -0.06413183 -0.5423992 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10764831  0.5789974  -0.07497981 -0.8545796 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11922825  0.7750568  -0.09207141 -1.1698657 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13472939  0.9712476  -0.11546873 -1.4899364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15415435  0.77770543 -0.14526744 -1.2354287 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16970845  0.584718   -0.16997603 -0.99155515]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1814028   0.3922278  -0.18980713 -0.75671357]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18924737  0.20015793 -0.20494139 -0.52925175]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19325052  0.39748383 -0.21552643 -0.8788783 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 69/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00326062 -0.5946414   0.0754039   0.9696814 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0086322  -0.40060818  0.09479753  0.7016064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01664437 -0.5969074   0.10882965  1.0225619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02858252 -0.40339017  0.1292809   0.76593614]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03665032 -0.21026279  0.14459962  0.5165654 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04085558 -0.40709317  0.15493092  0.85109395]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04899744 -0.6039496   0.1719528   1.1882095 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06107643 -0.80083185  0.19571699  1.5294828 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07709306 -0.9977009   0.22630665  1.8763229 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 70/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01234624 -0.594763    0.09879284  0.9741247 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0242415  -0.7910615   0.11827533  1.2961347 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04006273 -0.59762377  0.14419802  1.0426971 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05201521 -0.40468034  0.16505197  0.79853314]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06010881 -0.2121608   0.18102263  0.56198364]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06435203 -0.40929952  0.19226229  0.9057903 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07253802 -0.21722762  0.21037811  0.6791685 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 70, Return: 11.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 71/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01888104 -0.593896    0.05171815  0.95197964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00700312 -0.78967434  0.07075775  1.2604529 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00879037 -0.9856264   0.0959668   1.574431  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0285029  -1.1817528   0.12745541  1.8954377 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05213795 -0.98822224  0.16536418  1.6448672 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07190239 -0.7953767   0.19826151  1.4079407 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08780993 -0.9923281   0.22642033  1.7554883 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 72/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 5.3832039e-02  3.8155851e-01  4.8094502e-04 -5.0888926e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06146321  0.5766737  -0.00969684 -0.80142057]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07299668  0.38168603 -0.02572525 -0.5118037 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08063041  0.18693572 -0.03596133 -0.22733727]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08436912 -0.00765434 -0.04050807  0.05378876]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08421603 -0.20217276 -0.03943229  0.3334209 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08017258 -0.39671192 -0.03276388  0.61341274]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07223834 -0.5913611  -0.02049562  0.8955989 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06041112 -0.3959673  -0.00258364  0.5965445 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05249177 -0.2008093   0.00934725  0.30304888]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04847559 -0.3960632   0.01540822  0.598665  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04055432 -0.59139735  0.02738152  0.89616126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02872637 -0.3966571   0.04530475  0.61220956]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02079323 -0.20219661  0.05754894  0.33413318]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0167493  -0.00793894  0.0642316   0.0601391 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01659052 -0.20392023  0.06543439  0.37237594]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01251212 -0.00978591  0.07288191  0.10102163]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0123164   0.18421996  0.07490233 -0.16780631]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0160008  -0.01188978  0.07154621  0.14753419]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.015763   -0.20795947  0.0744969   0.46190298]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01160381 -0.40405092  0.08373495  0.7771061 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00352279 -0.21017419  0.09927708  0.5118993 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00068069 -0.01658049  0.10951506  0.25207815]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0010123  -0.21308199  0.11455663  0.57719773]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00527394 -0.40960747  0.12610058  0.9036598 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01346609 -0.2163983   0.14417377  0.6531226 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01779406 -0.41320232  0.15723623  0.9875063 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0260581  -0.6100401   0.17698635  1.3251579 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03825891 -0.41753876  0.20348951  1.0926793 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04660968 -0.22559345  0.2253431   0.87011564]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 73/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03041884 -0.00883249  0.03558456  0.07978204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03024219 -0.20444602  0.0371802   0.38347623]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02615327 -0.40007558  0.04484972  0.6876462 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01815176 -0.2056039   0.05860265  0.40941343]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01403968 -0.01135965  0.06679092  0.13576655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01381249 -0.20737155  0.06950625  0.44875076]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00966506 -0.01329806  0.07848126  0.17876184]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0093991  -0.20945026  0.0820565   0.4951343 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00521009 -0.01557555  0.09195919  0.22939828]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00489858 -0.21188307  0.09654716  0.5496133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 6.60919992e-04 -4.08219159e-01  1.07539415e-01  8.71086657e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00750346 -0.21471126  0.12496115  0.6140573 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01179769 -0.4113374   0.1372423   0.94333977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02002444 -0.6080148   0.1561091   1.2758046 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03218473 -0.41518968  0.18162519  1.035794  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04048853 -0.22288564  0.20234106  0.8051873 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04494624 -0.42012092  0.21844481  1.1540877 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 74/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0579058   0.18717085 -0.00685163 -0.23240055]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164922 -0.00785253 -0.01149964  0.0581133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06149217 -0.20280772 -0.01033737  0.34714594]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05743602 -0.00754027 -0.00339445  0.05122127]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05728521  0.18763019 -0.00237003 -0.24253069]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06103781  0.38278592 -0.00722064 -0.53596026]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06869353  0.57800865 -0.01793985 -0.83090955]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08025371  0.38313642 -0.03455804 -0.5439223 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08791643  0.18851672 -0.04543649 -0.2623251 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09168677 -0.0059282  -0.05068299  0.01568754]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0915682  -0.20028804 -0.05036924  0.2919585 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08756244 -0.394657   -0.04453007  0.5683398 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0796693  -0.589127   -0.03316327  0.8466682 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06788676 -0.3935687  -0.01622991  0.5437438 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06001539 -0.58845884 -0.00535503  0.83126915]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04824621 -0.3932641   0.01127035  0.5369069 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04038093 -0.19830242  0.02200849  0.24779636]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03641488 -0.00350159  0.02696442 -0.03786416]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03634485 -0.19899963  0.02620714  0.26320288]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03236486 -0.39448565  0.03147119  0.56403524]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02447515 -0.5900347   0.0427519   0.8664646 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01267445 -0.7857116   0.06008119  1.172277  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00303978 -0.59142     0.08352673  0.8990186 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01486818 -0.78756857  0.1015071   1.2167441 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03061955 -0.9838425   0.12584198  1.5394313 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0502964 -0.790439   0.1566306  1.2885202]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06610518 -0.9871678   0.18240102  1.6258622 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08584854 -1.1839066   0.21491826  1.9694039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 75/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06571045  0.5774066  -0.01855621 -0.817635  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07725858  0.38254347 -0.03490891 -0.5308459 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08490945  0.57813865 -0.04552583 -0.834321  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09647223  0.38366726 -0.06221225 -0.55629617]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10414557  0.18947138 -0.07333817 -0.28384447]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.107935    0.38555855 -0.07901506 -0.5987267 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11564617  0.581692   -0.0909896  -0.91521513]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12728001  0.38791057 -0.1092939  -0.6524595 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13503821  0.1944667  -0.12234309 -0.3960935 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13892755  0.3910928  -0.13026495 -0.7247073 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1467494   0.19798984 -0.1447591  -0.47569665]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15070921  0.0051768  -0.15427303 -0.2319128 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15081275 -0.18744285 -0.15891129  0.00840864]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14706388  0.00955897 -0.15874313 -0.32989433]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14725506 -0.18298915 -0.165341   -0.09117831]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14359528  0.01406911 -0.16716458 -0.43112364]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14387666 -0.17834027 -0.17578705 -0.19545053]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14030986  0.01880341 -0.17969605 -0.5380245 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14068593 -0.17339778 -0.19045655 -0.30691168]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13721797 -0.3653679  -0.19659477 -0.07981967]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12991062 -0.16805065 -0.19819118 -0.42752153]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1265496  -0.359895   -0.2067416  -0.20327573]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1193517  -0.16250868 -0.21080711 -0.55340016]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 76/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01234624 -0.9848512   0.09879284  1.5576583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03204326 -0.7910413   0.129946    1.297359  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04786409 -0.98755175  0.15589318  1.6277361 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06761513 -1.1841253   0.18844791  1.9646679 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.09129763 -1.3806762   0.22774126  2.309351  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 77/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00454311 -0.5947744   0.087106    0.9734366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01643859 -0.79095024  0.10657474  1.2921615 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0322576  -0.5973322   0.13241796  1.0346559 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04420424 -0.40419564  0.15311109  0.78630453]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05228816 -0.6010522   0.16883717  1.1229739 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0643092  -0.79793626  0.19129665  1.4635034 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08026792 -0.6056028   0.22056672  1.2361647 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 78/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06164655  0.7723054  -0.01143824 -1.1053604 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07709266  0.5773357  -0.03354545 -0.8162878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08863937  0.38268873 -0.04987121 -0.53434205]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09629315  0.18830228 -0.06055805 -0.25778097]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1000592  -0.00590518 -0.06571367  0.01520302]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09994109  0.19009463 -0.06540961 -0.29746807]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10374298 -0.00403688 -0.07135897 -0.02611021]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10366224 -0.19806679 -0.07188118  0.2432322 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09970091 -0.00199564 -0.06701653 -0.07122989]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.099661    0.1940199  -0.06844112 -0.3842813 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1035414   0.3900434  -0.07612675 -0.6977341 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11134227  0.5861338  -0.09008144 -1.0133775 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12306494  0.3923214  -0.11034898 -0.7502858 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13091137  0.19888037 -0.1253547  -0.4942659 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13488898  0.3955266  -0.13524002 -0.82367855]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14279951  0.5922137  -0.1517136  -1.1556547 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15464377  0.39935967 -0.17482668 -0.91412944]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16263098  0.20697829 -0.19310927 -0.68109375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16677053  0.014988   -0.20673114 -0.45488325]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1670703   0.21234117 -0.2158288  -0.8049566 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 79/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00082953 -0.01053289  0.08311619  0.11762639]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00104019  0.1833059   0.08546872 -0.14772001]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00262593  0.37710655  0.08251432 -0.41226265]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01016806  0.5709678   0.07426906 -0.67783344]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02158741  0.76498365  0.06071239 -0.9462399 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03688709  0.9592377   0.04178759 -1.2192457 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05607184  1.1537968   0.01740268 -1.4985476 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07914778  0.9584678  -0.01256827 -1.2004822 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09831713  1.1537501  -0.03657791 -1.4970775 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12139213  0.95909125 -0.06651946 -1.2160362 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14057395  1.1550051  -0.09084018 -1.5287994 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16367406  1.3510978  -0.12141617 -1.8483958 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19069602  1.1575042  -0.15838408 -1.595752  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2138461   1.3541101  -0.19029912 -1.9333401 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.24092829  1.5506914  -0.22896594 -2.2784998 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 80/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0422969  -0.20322338  0.01655046  0.3563163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03823243 -0.00834059  0.02367678  0.06889778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03806562  0.18643405  0.02505474 -0.21622196]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0417943  -0.00903695  0.0207303   0.08425784]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04161356  0.1857818   0.02241546 -0.20181324]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0453292  -0.00965344  0.01837919  0.09785558]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04513613 -0.20503391  0.0203363   0.39627996]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04103545 -0.01020632  0.0282619   0.11007746]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04083133 -0.20572162  0.03046345  0.4115412 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03671689 -0.01104446  0.03869428  0.12861581]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03649601  0.18350244  0.04126659 -0.1516128 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04016605 -0.01218538  0.03823433  0.15379794]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03992235  0.18236886  0.0413103  -0.12658197]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04356973  0.3768754   0.03877866 -0.4059511 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05110723  0.18122561  0.03065963 -0.10129899]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05473175  0.37589505  0.02863365 -0.3841534 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06224965  0.570599    0.02095059 -0.6676725 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07366163  0.3751921   0.00759714 -0.3684674 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08116547  0.17996302  0.00022779 -0.07339869]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08476473 -0.0151622  -0.00124019  0.21935609]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08446149  0.17997746  0.00314694 -0.07371778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08806103 -0.01518946  0.00167258  0.21995635]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08775724  0.17990854  0.00607171 -0.0721985 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09135541  0.37494293  0.00462774 -0.36295962]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09885427  0.5699988  -0.00263145 -0.65417975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11025425  0.37491357 -0.01571505 -0.3623266 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11775252  0.57025534 -0.02296158 -0.6599231 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12915763  0.37546033 -0.03616004 -0.37455767]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13666683  0.57107675 -0.0436512  -0.67841923]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14808837  0.37658754 -0.05721958 -0.3997927 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15562011  0.18232198 -0.06521544 -0.12568463]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15926656 -0.01180801 -0.06772913  0.14573158]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1590304   0.1842152  -0.06481449 -0.16752625]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1627147   0.38020217 -0.06816502 -0.4799315 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17031875  0.5762168  -0.07776365 -0.7932957 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18184309  0.77231526 -0.09362957 -1.1093938 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19728939  0.9685344  -0.11581744 -1.4299203 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21666008  1.1648802  -0.14441586 -1.7564402 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.23995768  0.9716603  -0.17954466 -1.5119383 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2593909   1.1684448  -0.20978342 -1.854873  ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 80, Return: 44.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 81/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01480714 -0.39951223  0.05906051  0.67564946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00681689 -0.595403    0.0725735   0.986327  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00509117 -0.40132406  0.09230004  0.7172928 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01311765 -0.59759396  0.1066459   1.0375421 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02506953 -0.40403858  0.12739673  0.78015304]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0331503 -0.6006602  0.1429998  1.1100488]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0451635  -0.4076767   0.16520077  0.865427  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05331703 -0.2151419   0.18250932  0.6289052 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05761987 -0.41227838  0.19508742  0.97305894]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06586544 -0.6094066   0.2145486   1.3201349 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 82/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02667528 -0.20403126  0.04023969  0.37436375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259465 -0.00950332  0.04772697  0.09463533]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02240459 -0.20527568  0.04961967  0.40198588]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01829907 -0.40106502  0.05765939  0.7098906 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01027777 -0.20678703  0.0718572   0.4359004 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00614203 -0.01275195  0.08057521  0.16670671]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00588699  0.18112971  0.08390934 -0.09950874]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00950959  0.37495512  0.08191917 -0.36458352]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01700869  0.17877029  0.0746275  -0.04723584]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0205841   0.3727472   0.07368279 -0.31547123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02803904  0.17665726  0.06737336 -0.00049127]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03157219 -0.019363    0.06736353  0.31266484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03118492 -0.2153767   0.07361683  0.6258085 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02687739 -0.02135542  0.086133    0.35718858]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.02645028 0.17244312 0.09327677 0.09286022]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02989914 -0.0238834   0.09513398  0.41345343]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.02942148 0.16977039 0.10340304 0.15221347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03281688  0.36327142  0.10644731 -0.1061414 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04008231  0.55671966  0.10432448 -0.36343488]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05121671  0.36028168  0.09705579 -0.0397641 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.05842234 0.16391158 0.0962605  0.2818944 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.06170057 0.3575381  0.10189839 0.02105609]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.06885134 0.16111365 0.10231952 0.34407076]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.07207361 0.35464248 0.10920093 0.08532527]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.07916646 0.1581384  0.11090744 0.41036698]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.08232922 0.3515276  0.11911477 0.15460537]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08935978  0.5447605   0.12220688 -0.09825317]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.10025498 0.3481183  0.12024182 0.23034978]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10721735  0.54133517  0.12484881 -0.02211691]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.11804406 0.34466445 0.12440647 0.30720326]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.12493735 0.5378144  0.13055053 0.0561987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.13569364 0.3410857  0.13167451 0.38705352]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.14251535 0.53411674 0.13941559 0.13861403]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.15319768 0.33730185 0.14218786 0.47182742]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.15994371 0.5301594  0.15162441 0.227123  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1705469   0.722826    0.15616688 -0.01415421]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18500343  0.91540366  0.15588379 -0.2537788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2033115   1.1079961   0.15080822 -0.49352017]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22547142  1.3007053   0.1409378  -0.73513335]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.25148553  1.1039469   0.12623514 -0.4016235 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.27356446  1.2970735   0.11820267 -0.6519936 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.29950595  1.4903682   0.1051628  -0.9052418 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.3293133   1.683921    0.08705797 -1.163106  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.36299172  1.8778082   0.06379585 -1.4272727 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.4005479   1.6819587   0.03525039 -1.1153529 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.43418705  1.4863921   0.01294333 -0.81182384]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 83/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106766 -0.5943515   0.06362975  0.96259123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00081937 -0.40013957  0.08288158  0.6905575 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00882216 -0.20625953  0.09669273  0.425075  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01294735 -0.0126307   0.10519423  0.16437183]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01319996  0.18084042  0.10848166 -0.09336053]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00958315  0.37425378  0.10661445 -0.34994406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00209808  0.17778988  0.09961557 -0.02563617]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00145772 -0.01860908  0.09910285  0.29674035]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00108554 -0.21499385  0.10503765  0.6189607 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00321434 -0.41141397  0.11741687  0.94279134]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01144262 -0.6079055   0.1362727   1.2699406 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02360073 -0.4147609   0.1616715   1.0228498 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03189595 -0.6116237   0.1821285   1.3616179 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04412842 -0.41919068  0.20936087  1.1309928 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05251224 -0.6163457   0.23198071  1.481371  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 84/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106376  0.18562292  0.06371707 -0.19868816]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01477622 -0.01034977  0.05974331  0.11339449]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01456922  0.1838675   0.0620112  -0.15985757]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01824657 -0.01208489  0.05881405  0.15172568]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01800487  0.18214774  0.06184856 -0.12183815]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02164783  0.37633157  0.05941179 -0.39438507]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02917446  0.18041912  0.0515241  -0.0835781 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03278284  0.37476608  0.04985253 -0.3595703 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04027816  0.17897221  0.04266113 -0.0515938 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04385761  0.3734573   0.04162925 -0.33051744]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05132676  0.17776826  0.0350189  -0.02500272]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05488212 -0.01783794  0.03451885  0.27852014]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05452536 -0.21343489  0.04008925  0.5818873 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025666 -0.40909493  0.051727    0.8869246 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04207477 -0.21471184  0.06946549  0.61094093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03778053 -0.02062606  0.08168431  0.34092087]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03736801 -0.21680947  0.08850273  0.65820324]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03303182 -0.4130445   0.10166679  0.97738963]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02477093 -0.21942194  0.12121458  0.718294  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02038249 -0.41599414  0.13558047  1.0465387 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01206261 -0.22290656  0.15651123  0.7993039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00760447 -0.41978946  0.17249732  1.1368452 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-7.9131534e-04 -6.1669558e-01  1.9523422e-01  1.4782841e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01312523 -0.42441887  0.2247999   1.2523832 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 85/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01234624 -0.9848512   0.09879284  1.5576583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03204326 -1.1810076   0.129946    1.8794562 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05566342 -1.3772845   0.16753513  2.209486  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0832091  -1.573571    0.21172485  2.5488186 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 86/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03801544 -0.20481206  0.02616089  0.3914299 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0339192  -0.01007097  0.03398949  0.10710862]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03371778  0.18454783  0.03613166 -0.17465998]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03740874 -0.0110721   0.03263846  0.12919879]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0371873  -0.20664603  0.03522243  0.43199745]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03305437 -0.01204005  0.04386238  0.15062279]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03281358 -0.20776173  0.04687484  0.45681435]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02865834 -0.40351397  0.05601113  0.76389635]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02058806 -0.59936076  0.07128905  1.0736644 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00860085 -0.79534876  0.09276234  1.387841  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00730613 -0.9914962   0.12051916  1.7080303 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02713605 -0.79794854  0.15467976  1.4551616 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04309503 -0.6050265   0.183783    1.2145282 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05519556 -0.41268831  0.20807356  0.9846081 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06344932 -0.6098978   0.22776572  1.334775  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 87/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04587036 -0.20363224  0.01334879  0.36532268]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04179772 -0.3989413   0.02065525  0.66218466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03381889 -0.5943445   0.03389894  0.96129906]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.021932   -0.78990525  0.05312492  1.264436  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0061339  -0.595501    0.07841364  0.9888525 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00577612 -0.4015114   0.09819069  0.72179216]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01380635 -0.20787494  0.11262653  0.46155828]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01796385 -0.40439352  0.1218577   0.78751075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02605172 -0.6009598   0.13760792  1.1159112 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03807091 -0.40788567  0.15992613  0.869367  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04622863 -0.60477954  0.17731348  1.207757  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05832422 -0.8016922   0.20146862  1.5503523 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07435806 -0.9985798   0.23247567  1.8985423 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 88/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00454311 -0.20468625  0.087106    0.38990307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00863683 -0.01090164  0.09490407  0.12590498]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00885486  0.18274164  0.09742217 -0.13539292]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00520003  0.37634295  0.09471431 -0.39582103]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00232683  0.5700024   0.08679789 -0.6572038 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01372688  0.3737863   0.07365381 -0.33850113]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0212026   0.5677871   0.06688379 -0.60707873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03255834  0.3717969   0.05474221 -0.29410157]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03999428 0.17593895 0.04886018 0.01533145]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04351306 -0.01984843  0.04916681  0.3230212 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04311609 0.17454018 0.05562723 0.04623988]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04660689  0.3688222   0.05655203 -0.22838637]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05398334  0.5630923   0.05198431 -0.50270826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06524519  0.36727762  0.04193014 -0.19410655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.07259074 0.17158175 0.03804801 0.11150312]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07602237 -0.02406416  0.04027807  0.41594303]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.07554109 0.17046449 0.04859693 0.13622525]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07895038 -0.02531864  0.05132144  0.4438351 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.078444   0.16904101 0.06019814 0.16776136]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08182482 -0.02688864  0.06355336  0.47881106]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.08128706 0.16728121 0.07312959 0.20681639]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08463268  0.3612854   0.07726592 -0.06193113]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09185839  0.55521935  0.07602729 -0.32927063]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10296278  0.3591021   0.06944188 -0.01361399]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11014482  0.553163    0.0691696  -0.28360426]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12120807  0.7472337   0.06349751 -0.55369455]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13615274  0.55128026  0.05242362 -0.24170104]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.14717835 0.35545018 0.0475896  0.06704601]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.15428735 0.15967938 0.04893052 0.3743558 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15748094 -0.03610222  0.05641764  0.68205655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1567589  -0.2319604   0.07005877  0.9919544 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1521197  -0.42794636  0.08989786  1.3057926 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14356077 -0.23407172  0.11601371  1.0425494 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13887933 -0.04066556  0.13686469  0.7884218 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.13806602 0.15233782 0.15263313 0.5417367 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14111277 -0.04456269  0.16346787  0.8783539 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.14022152 0.14800575 0.18103495 0.6411978 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.14318164 0.3402041  0.1938589  0.4105457 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.14998572 0.5321255  0.20206982 0.18469441]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.16062823 0.3347721  0.2057637  0.53370625]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.16732368 0.1374413  0.21643783 0.88352704]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 89/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02667528 -0.20403126  0.04023969  0.37436375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259465 -0.00950332  0.04772697  0.09463533]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02240459 -0.20527568  0.04961967  0.40198588]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01829907 -0.40106502  0.05765939  0.7098906 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01027777 -0.20678703  0.0718572   0.4359004 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00614203 -0.01275195  0.08057521  0.16670671]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00588699 -0.2089293   0.08390934  0.4836815 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00170841 -0.40512902  0.09358297  0.80158603]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00639417 -0.60140127  0.10961469  1.1221795 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0184222  -0.40787387  0.13205828  0.8657928 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02657968 -0.604522    0.14937414  1.1969091 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03867012 -0.8012277   0.17331232  1.5324348 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05469467 -0.608566    0.20396101  1.2984707 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06686599 -0.8056079   0.22993043  1.6474569 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 90/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06945409  0.38238686 -0.02320687 -0.5272419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07710183  0.187599   -0.0337517  -0.24196094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08085381 -0.00702497 -0.03859092  0.03988764]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08071331 -0.2015729  -0.03779317  0.32014933]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07668185 -0.3961368  -0.03139018  0.60067827]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06875911 -0.5908059  -0.01937662  0.88331085]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.056943   -0.39542624 -0.0017104   0.5846    ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04903447 -0.5905242   0.0099816   0.8767436 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03722399 -0.3955393   0.02751647  0.5872154 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0293132  -0.20081331  0.03926078  0.30332583]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02529694 -0.00627225  0.0453273   0.02327882]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02517149  0.18817134  0.04579287 -0.25476533]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02893492 -0.00757351  0.04069757  0.05200255]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02878345 -0.20325467  0.04173762  0.35724303]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02471836 -0.39894438  0.04888248  0.6627892 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01673947 -0.5947111   0.06213826  0.97045434]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00484525 -0.7906096   0.08154735  1.2819918 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01096695 -0.59661555  0.10718719  1.0159161 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02289926 -0.40307343  0.12750551  0.75872177]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03096073 -0.20991747  0.14267994  0.508725  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03515908 -0.01706352  0.15285444  0.2641893 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03550035 -0.21399921  0.15813823  0.600913  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03978033 -0.02140158  0.1701565   0.36192045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04020836  0.17094497  0.1773949   0.12735228]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03678946 -0.02621601  0.17994194  0.4703336 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03731378 -0.22336257  0.18934862  0.81389666]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04178103 -0.42050293  0.20562655  1.1596584 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05019109 -0.22856516  0.22881971  0.9378461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 90, Return: 33.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 91/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01888104 -0.593896    0.05171815  0.95197964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00700312 -0.78967434  0.07075775  1.2604529 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00879037 -0.5955252   0.0959668   0.9907433 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02070087 -0.40180948  0.11578166  0.7296777 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02873706 -0.5983252   0.13037522  1.0564415 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04070356 -0.79491115  0.15150405  1.3870397 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05660179 -0.6019672   0.17924485  1.1453109 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06864113 -0.40958047  0.20215106  0.9137705 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07683274 -0.606778    0.22042647  1.2625719 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 92/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04600646  0.38086733  0.01264031 -0.49371535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0536238   0.57580876  0.00276601 -0.782388  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06513998  0.77089256 -0.01288175 -1.0741993 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08055783  0.57594323 -0.03436574 -0.7855867 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0920767   0.77152    -0.05007747 -1.0888803 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10750709  0.9672652  -0.07185508 -1.3968465 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1268524   0.77310675 -0.09979201 -1.1274676 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14231454  0.9693843  -0.12234136 -1.4497094 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16170222  0.77595997 -0.15133555 -1.1976203 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17722142  0.9726812  -0.17528796 -1.5336524 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19667505  1.1694279  -0.205961   -1.8755213 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2200636   1.366117   -0.24347143 -2.2244449 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 93/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259123  0.38033074  0.04780285 -0.4824741 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03019784  0.57474655  0.03815337 -0.759716  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04169277  0.37912026  0.02295905 -0.4552757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04927517  0.5739102   0.01385354 -0.74063426]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06075338  0.37859973 -0.00095915 -0.44362387]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06832538  0.18349138 -0.00983162 -0.15124343]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07199521  0.3787527  -0.01285649 -0.44701174]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07957026  0.5740542  -0.02179673 -0.7437194 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09105134  0.3792397  -0.03667111 -0.45797488]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09863614  0.18465485 -0.04583061 -0.1770727 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10232923  0.3804017  -0.04937207 -0.48385414]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10993727  0.18601003 -0.05904915 -0.20713146]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11365747 -0.00822001 -0.06319178  0.06635546]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11349307 -0.20238172 -0.06186467  0.3384507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10944543 -0.3965713  -0.05509565  0.61100125]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.101514   -0.59088165 -0.04287563  0.8858346 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08969637 -0.39520466 -0.02515894  0.5799872 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08179228 -0.19973935 -0.01355919  0.27948615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07779749 -0.00442663 -0.00796947 -0.0174423 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07770896  0.1908087  -0.00831832 -0.31262898]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08152513 -0.00419376 -0.0145709  -0.02258098]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08144126 -0.19910376 -0.01502252  0.26546928]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07745919 -0.3940081  -0.00971313  0.5533764 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06957902 -0.5889923   0.0013544   0.8429833 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05779918 -0.7841327   0.01821406  1.1360918 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04211652 -0.9794882   0.0409359   1.434431  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02252676 -0.78489435  0.06962452  1.1548166 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00682887 -0.5907459   0.09272085  0.88475263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00498605 -0.39699695  0.1104159   0.6225984 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01292599 -0.20357585  0.12286787  0.3666299 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0169975  -0.40021002  0.13020046  0.6953899 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0250017  -0.20711122  0.14410827  0.44636425]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02914393 -0.40394646  0.15303555  0.78077793]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03722286 -0.21122219  0.1686511   0.53988713]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0414473  -0.4082633   0.17944886  0.8806041 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04961257 -0.60530955  0.19706094  1.2239026 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06171876 -0.41319388  0.22153899  0.998868  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 94/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00082953 -0.4006148   0.08311619  0.70108664]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00884183 -0.59678453  0.09713792  1.0187335 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02077752 -0.79305756  0.11751259  1.3402687 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03663867 -0.98944634  0.14431797  1.6672895 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0564276  -1.185922    0.17766376  2.0012217 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08014604 -0.99304503  0.21768819  1.7684189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 95/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03801544 -0.20481206  0.02616089  0.3914299 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0339192  -0.40029535  0.03398949  0.6922449 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02591329 -0.20566103  0.04783438  0.41045302]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02180007 -0.40142733  0.05604344  0.7178245 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01377153 -0.2071239   0.07039993  0.44329512]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00962905 -0.4031677   0.07926583  0.7573129 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00156569 -0.20922245  0.0944121   0.49058822]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00261875 -0.01555028  0.10422385  0.22908969]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00292976 -0.21199524  0.10880565  0.55274457]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00716966 -0.40846345  0.11986054  0.87762994]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01533893 -0.21515645  0.13741314  0.62490445]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01964206 -0.02219323  0.14991122  0.3784605 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02008593 -0.21909083  0.15748043  0.7144034 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02446774 -0.41600165  0.1717685   1.0522227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03278778 -0.6129332   0.19281296  1.393525  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04504644 -0.8098603   0.22068347  1.7397782 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 96/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0147878  -0.00983522  0.05949304  0.10202754]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0145911   0.1843859   0.06153359 -0.17130806]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01827882 -0.01156033  0.05810743  0.14013447]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01804761  0.18268332  0.06091012 -0.1336653 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02170128  0.3768823   0.05823681 -0.40652704]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02923892  0.18098497  0.05010627 -0.09606727]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03285862 -0.01481798  0.04818493  0.21199362]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03256226 -0.21059455  0.0524248   0.51947856]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02835037 -0.01624833  0.06281437  0.24376553]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02802541 -0.21220861  0.06768968  0.55558157]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02378123 -0.4082124   0.07880131  0.8687995 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01561699 -0.6043129   0.0961773   1.185181  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00353073 -0.41056094  0.11988092  0.9241288 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00468049 -0.21724449  0.1383635   0.67139596]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00902538 -0.02428957  0.15179142  0.42527884]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00951117  0.16839308  0.16029699  0.18403363]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00614331 -0.02861607  0.16397767  0.5226846 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00671563  0.16386433  0.17443135  0.28583065]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00343835  0.35612524  0.18014798  0.05283867]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00368416  0.54826826  0.18120475 -0.17803426]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.01464952 0.35107827 0.17764406 0.16589107]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02167109 0.15391713 0.18096188 0.5089317 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.02474943 0.34609014 0.19114052 0.2782921 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03167123 0.14882852 0.19670635 0.6246495 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0346478  -0.0484166   0.20919934  0.972271  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03367947 -0.24563807  0.22864477  1.3227103 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 97/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02668534 -0.20369677  0.04001765  0.36697814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0226114  -0.00916565  0.04735721  0.08717711]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02242809 -0.20493332  0.04910076  0.3944172 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01832942 -0.01054122  0.0569891   0.1176103 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0181186   0.18371984  0.0593413  -0.15656252]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02179299  0.3779442   0.05621006 -0.42995012]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02935188  0.57222694  0.04761105 -0.70439696]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04079642  0.3764787   0.03352311 -0.39711496]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04832599  0.1808976   0.02558081 -0.09405415]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05194394  0.37564373  0.02369973 -0.37855786]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05945682  0.18019336  0.01612857 -0.07849756]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06306069  0.37508044  0.01455862 -0.36604854]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0705623   0.5699925   0.00723765 -0.6541056 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08196215  0.7650129  -0.00584446 -0.94450074]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0972624   0.9602131  -0.02473447 -1.2390143 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11646666  0.76541746 -0.04951476 -0.9541813 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13177502  0.57099533 -0.06859838 -0.677457  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14319491  0.37689012 -0.08214752 -0.40713605]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15073273  0.18302324 -0.09029025 -0.14144059]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15439318  0.37931454 -0.09311906 -0.46118745]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16197948  0.5756208  -0.10234281 -0.7817085 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1734919   0.38204324 -0.11797698 -0.52289784]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18113276  0.57861084 -0.12843493 -0.8503045 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19270498  0.7752282  -0.14544103 -1.1804575 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20820954  0.58226234 -0.16905017 -0.93667376]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21985479  0.77921087 -0.18778364 -1.2773497 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.235439    0.58691233 -0.21333064 -1.0488572 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 98/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106766 -0.5943515   0.06362975  0.96259123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-8.1936747e-04 -7.9026806e-01  8.2881577e-02  1.2745659e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01662473 -0.59629536  0.1083729   1.008945  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02855064 -0.40277374  0.1285518   0.75216484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03660611 -0.59941185  0.14359508  1.0823784 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04859435 -0.40644664  0.16524266  0.83798164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05672328 -0.6033929   0.18200229  1.1777375 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06879114 -0.41104004  0.20555703  0.9471875 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07701194 -0.21918991  0.22450079  0.7254798 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 99/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00082953 -0.4006148   0.08311619  0.70108664]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00884183 -0.59678453  0.09713792  1.0187335 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02077752 -0.40308207  0.11751259  0.75806314]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02883916 -0.59961045  0.13267384  1.0852919 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04083137 -0.40646428  0.15437968  0.8370105 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04896066 -0.6033196   0.1711199   1.1739893 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06102705 -0.41078365  0.19459969  0.9394675 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06924272 -0.60792065  0.21338904  1.2864435 ]\n",
      "Reward: 1.0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:20:44.845582Z",
     "start_time": "2024-06-01T08:20:44.843047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"****************************************************\")\n",
    "print(\"Final reward after episode: {}, Return: {}\".format(num_episodes, RETURN))\n",
    "print(\"****************************************************\")"
   ],
   "id": "b65a5a460ce10c3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Final reward after episode: 100, Return: 13.0\n",
      "****************************************************\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "`",
   "id": "55e33b8fc5ac58f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
