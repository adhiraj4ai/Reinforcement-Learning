{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.675214Z",
     "start_time": "2024-09-19T16:30:34.672093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# set seed\n",
    "SEED = 106"
   ],
   "id": "19aff06715285ff6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.682060Z",
     "start_time": "2024-09-19T16:30:34.678936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode='ansi')"
   ],
   "id": "822faed246655ae8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.686695Z",
     "start_time": "2024-09-19T16:30:34.683724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env.reset(seed=SEED)\n",
    "print(env.render())"
   ],
   "id": "1939223f94cf4230",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.691478Z",
     "start_time": "2024-09-19T16:30:34.687735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_policy_value(env, policy, gamma=1.0, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    Evaluate the value function for a given policy.\n",
    "    :param env: environment for an agent\n",
    "    :param policy: policy to evaluate\n",
    "    :param gamma: discount factor\n",
    "    :param threshold: convergence threshold\n",
    "    :return: value table for the given policy\n",
    "    \"\"\"\n",
    "    num_of_states = env.observation_space.n\n",
    "    value_table = np.zeros(num_of_states)\n",
    "    \n",
    "    while True:\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        # Iterate through each state to evaluate its value\n",
    "        for state in range(num_of_states):\n",
    "            action = policy[state]\n",
    "            q_value = 0\n",
    "            \n",
    "            # For each action, compute its value using transition probabilities\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                bellman_backup = reward + gamma * updated_value_table[next_state]\n",
    "                q_value += prob * bellman_backup\n",
    "                \n",
    "            value_table[state] = q_value\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
    "            break\n",
    "\n",
    "    return value_table"
   ],
   "id": "d38b2555a809950a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.696868Z",
     "start_time": "2024-09-19T16:30:34.693015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_policy(env, value_table, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Extract the policy from the given value table.\n",
    "    :param env: environment for an agent\n",
    "    :param value_table: value table computed from value iteration\n",
    "    :param gamma: discount factor\n",
    "    :return: policy table\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize policy as an integer array\n",
    "    policy = np.zeros(num_of_states, dtype=int)\n",
    "    \n",
    "    # Iterate through each state\n",
    "    for state in range(num_of_states):\n",
    "        q_values = []\n",
    "        \n",
    "        # For each action in the state, compute q value\n",
    "        for action in range(num_of_actions):\n",
    "            q_value = 0\n",
    "            \n",
    "            # Calculate q value using the transition probabilities\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                bellman_backup = reward + gamma * value_table[next_state]\n",
    "                q_value += prob * bellman_backup\n",
    "            \n",
    "            # Append q value to the list\n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        # Select the action with the highest q value\n",
    "        policy[state] = np.argmax(q_values)\n",
    "        \n",
    "    return policy"
   ],
   "id": "677a89e61a1d08be",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.701020Z",
     "start_time": "2024-09-19T16:30:34.697841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def policy_iteration(env, gamma=1.0, num_of_iterations=1000, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm to compute the optimal policy and value table.\n",
    "    :param env: environment for an agent\n",
    "    :param gamma: discount factor\n",
    "    :param num_of_iterations: max number of iterations for the algorithm\n",
    "    :param threshold: convergence threshold\n",
    "    :return: optimal policy and value table\n",
    "    \"\"\"\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "\n",
    "    # Initialize a random policy (or all zeros)\n",
    "    policy = np.random.choice(num_of_actions, num_of_states)\n",
    "    \n",
    "    for i in range(num_of_iterations):\n",
    "        # Step 1: Policy Evaluation (compute the value of the current policy)\n",
    "        value_table = compute_policy_value(env, policy, gamma, threshold)\n",
    "        \n",
    "        # Step 2: Policy Improvement (extract new policy based on the current value table)\n",
    "        new_policy = extract_policy(env, value_table, gamma)\n",
    "        \n",
    "        # Check for convergence (if policy doesn't change, stop)\n",
    "        if np.all(policy == new_policy):\n",
    "            print(f\"Policy converged at iteration {i}.\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy, value_table"
   ],
   "id": "99441d78214d27f9",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.715984Z",
     "start_time": "2024-09-19T16:30:34.702491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run policy iteration\n",
    "optimal_policy, optimal_value_table = policy_iteration(env, gamma=0.90)\n",
    "\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal Value Table:\", optimal_value_table)"
   ],
   "id": "84dff21280842d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy converged at iteration 2.\n",
      "Optimal Policy: [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "Optimal Value Table: [0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0.\n",
      " 0.11220821 0.         0.14543635 0.24749695 0.29961759 0.\n",
      " 0.         0.3799359  0.63902015 0.        ]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:30:34.722040Z",
     "start_time": "2024-09-19T16:30:34.717035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's decode the action to be take in each state\n",
    "# map numbers to action\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"down\",\n",
    "    2: \"right\",\n",
    "    3: \"up\"\n",
    "}\n",
    "\n",
    "# Number of times the agent moves\n",
    "num_timestep = 100\n",
    "\n",
    "# Reset the environment\n",
    "current_state, info = env.reset(seed=SEED)\n",
    "\n",
    "for i in range(num_timestep):\n",
    "    print(\"------- Step: {} --------\".format(i+1))\n",
    "        # Let's take a random action now from the action space\n",
    "    # Random action means we are taking random policy at the moment.\n",
    "    action = optimal_policy[current_state]\n",
    "    \n",
    "    # # Take the action and get the new observation space\n",
    "    next_state, reward, done, info, transition_prob = env.step(action)\n",
    "    \n",
    "    print(\"Current State: {}\".format(current_state))\n",
    "    print(\"Action: {}\".format(action_map[action]))\n",
    "    print(\"Next State: {}\".format(next_state))\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    current_state = next_state\n",
    "    \n",
    "    # if the agent moves to hole state, then terminate\n",
    "    if done: \n",
    "        break"
   ],
   "id": "9a2bdc68475ad1da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Step: 1 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 2 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 3 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 4 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 5 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 6 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 7 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 8 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 9 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 10 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 11 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 12 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 13 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 14 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 15 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 16 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 17 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 18 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 19 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 20 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 21 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 22 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 23 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 24 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 25 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 26 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 27 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 28 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 29 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 30 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 13\n",
      "Reward: 0.0\n",
      "------- Step: 31 --------\n",
      "Current State: 13\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 32 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 33 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 34 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 10\n",
      "Reward: 0.0\n",
      "------- Step: 35 --------\n",
      "Current State: 10\n",
      "Action: left\n",
      "Next State: 14\n",
      "Reward: 0.0\n",
      "------- Step: 36 --------\n",
      "Current State: 14\n",
      "Action: down\n",
      "Next State: 15\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
