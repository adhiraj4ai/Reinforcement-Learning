{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19aff06715285ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.230215Z",
     "start_time": "2024-09-21T04:04:48.000409Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# set seed\n",
    "SEED = 106\n",
    "\n",
    "# set discount factor\n",
    "GAMMA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822faed246655ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.250843Z",
     "start_time": "2024-09-21T04:04:48.241985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1939223f94cf4230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.261840Z",
     "start_time": "2024-09-21T04:04:48.255116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset(seed=SEED)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38b2555a809950a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.270298Z",
     "start_time": "2024-09-21T04:04:48.264132Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_policy_value(env, policy, gamma=1.0, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    Evaluate the value function for a given policy.\n",
    "    :param env: environment for an agent\n",
    "    :param policy: policy to evaluate\n",
    "    :param gamma: discount factor\n",
    "    :param threshold: convergence threshold\n",
    "    :return: value table for the given policy\n",
    "    \"\"\"\n",
    "    num_of_states = env.observation_space.n\n",
    "    value_table = np.zeros(num_of_states)\n",
    "    \n",
    "    while True:\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        # Iterate through each state to evaluate its value\n",
    "        for state in range(num_of_states):\n",
    "            action = policy[state]\n",
    "            q_value = 0\n",
    "            \n",
    "            # For each action, compute its value using transition probabilities\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                bellman_backup = reward + gamma * updated_value_table[next_state]\n",
    "                q_value += prob * bellman_backup\n",
    "                \n",
    "            value_table[state] = q_value\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
    "            break\n",
    "\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677a89e61a1d08be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.282704Z",
     "start_time": "2024-09-21T04:04:48.278672Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_policy(env, value_table, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Extract the policy from the given value table.\n",
    "    :param env: environment for an agent\n",
    "    :param value_table: value table computed from value iteration\n",
    "    :param gamma: discount factor\n",
    "    :return: policy table\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize policy as an integer array\n",
    "    policy = np.zeros(num_of_states, dtype=int)\n",
    "    \n",
    "    # Iterate through each state\n",
    "    for state in range(num_of_states):\n",
    "        q_values = []\n",
    "        \n",
    "        # For each action in the state, compute q value\n",
    "        for action in range(num_of_actions):\n",
    "            q_value = 0\n",
    "            \n",
    "            # Calculate q value using the transition probabilities\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                bellman_backup = reward + gamma * value_table[next_state]\n",
    "                q_value += prob * bellman_backup\n",
    "            \n",
    "            # Append q value to the list\n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        # Select the action with the highest q value\n",
    "        policy[state] = np.argmax(q_values)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99441d78214d27f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.287207Z",
     "start_time": "2024-09-21T04:04:48.283829Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=1.0, num_of_iterations=1000, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm to compute the optimal policy and value table.\n",
    "    :param env: environment for an agent\n",
    "    :param gamma: discount factor\n",
    "    :param num_of_iterations: max number of iterations for the algorithm\n",
    "    :param threshold: convergence threshold\n",
    "    :return: optimal policy and value table\n",
    "    \"\"\"\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "\n",
    "    # Initialize a random policy (or all zeros)\n",
    "    policy = np.random.choice(num_of_actions, num_of_states)\n",
    "    \n",
    "    for i in range(num_of_iterations):\n",
    "        # Step 1: Policy Evaluation (compute the value of the current policy)\n",
    "        value_table = compute_policy_value(env, policy, gamma, threshold)\n",
    "        \n",
    "        # Step 2: Policy Improvement (extract new policy based on the current value table)\n",
    "        new_policy = extract_policy(env, value_table, gamma)\n",
    "        \n",
    "        # Check for convergence (if policy doesn't change, stop)\n",
    "        if np.all(policy == new_policy):\n",
    "            print(f\"Policy converged at iteration {i}.\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy, value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dff21280842d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.292783Z",
     "start_time": "2024-09-21T04:04:48.288074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy converged at iteration 6.\n",
      "Optimal Policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "Optimal Value Table: [0.32768 0.4096  0.512   0.4096  0.4096  0.      0.64    0.      0.512\n",
      " 0.64    0.8     0.      0.      0.8     1.      0.     ]\n"
     ]
    }
   ],
   "source": [
    "# Run policy iteration\n",
    "optimal_policy, optimal_value_table = policy_iteration(env, gamma=GAMMA)\n",
    "\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"Optimal Value Table:\", optimal_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2bdc68475ad1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.298266Z",
     "start_time": "2024-09-21T04:04:48.293708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Step: 1 --------\n",
      "Current State: 0\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 2 --------\n",
      "Current State: 4\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 3 --------\n",
      "Current State: 8\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 4 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 13\n",
      "Reward: 0.0\n",
      "------- Step: 5 --------\n",
      "Current State: 13\n",
      "Action: right\n",
      "Next State: 14\n",
      "Reward: 0.0\n",
      "------- Step: 6 --------\n",
      "Current State: 14\n",
      "Action: right\n",
      "Next State: 15\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's decode the action to be take in each state\n",
    "# map numbers to action\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"down\",\n",
    "    2: \"right\",\n",
    "    3: \"up\"\n",
    "}\n",
    "\n",
    "# Number of times the agent moves\n",
    "num_timestep = 100\n",
    "\n",
    "# Reset the environment\n",
    "current_state, info = env.reset(seed=SEED)\n",
    "\n",
    "for i in range(num_timestep):\n",
    "    print(\"------- Step: {} --------\".format(i+1))\n",
    "        # Let's take a random action now from the action space\n",
    "    # Random action means we are taking random policy at the moment.\n",
    "    action = optimal_policy[current_state]\n",
    "    \n",
    "    # # Take the action and get the new observation space\n",
    "    next_state, reward, done, info, transition_prob = env.step(action)\n",
    "    \n",
    "    print(\"Current State: {}\".format(current_state))\n",
    "    print(\"Action: {}\".format(action_map[action]))\n",
    "    print(\"Next State: {}\".format(next_state))\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    current_state = next_state\n",
    "    \n",
    "    # if the agent moves to hole state, then terminate\n",
    "    if done: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e859a262441cb09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:48.300968Z",
     "start_time": "2024-09-21T04:04:48.299389Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rajan Adhikari"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "title": "Implementation of Policy Iteration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
