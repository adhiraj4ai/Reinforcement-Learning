{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19aff06715285ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.569338Z",
     "start_time": "2024-09-21T04:04:16.282045Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# set seed\n",
    "SEED = 106\n",
    "\n",
    "# set discount factor\n",
    "GAMMA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822faed246655ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.574851Z",
     "start_time": "2024-09-21T04:04:16.570570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1939223f94cf4230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.578509Z",
     "start_time": "2024-09-21T04:04:16.575824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset(seed=SEED)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38b2555a809950a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.584547Z",
     "start_time": "2024-09-21T04:04:16.579734Z"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, num_of_iterations=10, gamma=1.0, threshold=1e-40) -> list:\n",
    "    \"\"\"\n",
    "    Value iteration algorithm to compute the value table.\n",
    "    :param env: environment for an agent\n",
    "    :param num_of_iterations: number of iterations\n",
    "    :param gamma: discount factor\n",
    "    :param threshold: threshold value to stop iterations\n",
    "    :return: value table\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "    \n",
    "    print('Number of states:', num_of_states)\n",
    "    print('Number of actions:', num_of_actions)\n",
    "    \n",
    "    # Initialize the value table with zeros for each state\n",
    "    value_table = np.zeros(num_of_states)\n",
    "    \n",
    "    # Perform value iteration for num_of_iterations\n",
    "    for i in range(num_of_iterations):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        # Compute q value for each state\n",
    "        for state in range(num_of_states):\n",
    "            # Initialize q values\n",
    "            q_values = []\n",
    "            \n",
    "            # For each action in the state, compute q value\n",
    "            for action in range(num_of_actions):\n",
    "                q_value = 0\n",
    "                # Loop through each transition (prob, next_state, reward, done)\n",
    "                for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                    # Compute Bellman backup\n",
    "                    bellman_backup = reward + gamma * updated_value_table[next_state]\n",
    "                    # Compute q value\n",
    "                    q_value += prob * bellman_backup\n",
    "                \n",
    "                # Append q value to the list of q values\n",
    "                q_values.append(q_value)\n",
    "            \n",
    "            # Update the value table with the maximum q value\n",
    "            value_table[state] = max(q_values)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
    "            print(\"Execution halted in iteration {}.\".format(i))\n",
    "            break\n",
    "                \n",
    "    return value_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62cda843340685b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.589968Z",
     "start_time": "2024-09-21T04:04:16.586755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of actions: 4\n",
      "Execution halted in iteration 6.\n",
      "[0.32768 0.4096  0.512   0.4096  0.4096  0.      0.64    0.      0.512\n",
      " 0.64    0.8     0.      0.      0.8     1.      0.     ]\n"
     ]
    }
   ],
   "source": [
    "# Print Value Table\n",
    "optimal_value_table = value_iteration(env, num_of_iterations = 10, gamma=GAMMA)\n",
    "print(optimal_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99441d78214d27f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.594483Z",
     "start_time": "2024-09-21T04:04:16.590860Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_policy(env, value_table, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Extract the policy from the given value table.\n",
    "    :param env: environment for an agent\n",
    "    :param value_table: value table computed from value iteration\n",
    "    :param gamma: discount factor\n",
    "    :return: policy table\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize policy as an integer array\n",
    "    policy = np.zeros(num_of_states, dtype=int)\n",
    "    \n",
    "    # Iterate through each state\n",
    "    for state in range(num_of_states):\n",
    "        q_values = []\n",
    "        \n",
    "        # For each action in the state, compute q value\n",
    "        for action in range(num_of_actions):\n",
    "            q_value = 0\n",
    "            \n",
    "            # Calculate q value using the transition probabilities\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                bellman_backup = reward + gamma * value_table[next_state]\n",
    "                q_value += prob * bellman_backup\n",
    "            \n",
    "            # Append q value to the list\n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        # Select the action with the highest q value\n",
    "        policy[state] = np.argmax(q_values)\n",
    "        \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dff21280842d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.597729Z",
     "start_time": "2024-09-21T04:04:16.595370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = extract_policy(env, optimal_value_table, gamma=GAMMA)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2bdc68475ad1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T04:04:16.602937Z",
     "start_time": "2024-09-21T04:04:16.598919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Step: 1 --------\n",
      "Current State: 0\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 2 --------\n",
      "Current State: 4\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 3 --------\n",
      "Current State: 8\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 4 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 13\n",
      "Reward: 0.0\n",
      "------- Step: 5 --------\n",
      "Current State: 13\n",
      "Action: right\n",
      "Next State: 14\n",
      "Reward: 0.0\n",
      "------- Step: 6 --------\n",
      "Current State: 14\n",
      "Action: right\n",
      "Next State: 15\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's decode the action to be take in each state\n",
    "# map numbers to action\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"down\",\n",
    "    2: \"right\",\n",
    "    3: \"up\"\n",
    "}\n",
    "\n",
    "# Number of times the agent moves\n",
    "num_timestep = 100\n",
    "\n",
    "# Reset the environment\n",
    "current_state, info = env.reset(seed=SEED)\n",
    "\n",
    "for i in range(num_timestep):\n",
    "    print(\"------- Step: {} --------\".format(i+1))\n",
    "        # Let's take a random action now from the action space\n",
    "    # Random action means we are taking random policy at the moment.\n",
    "    action = optimal_policy[current_state]\n",
    "    \n",
    "    # # Take the action and get the new observation space\n",
    "    next_state, reward, done, info, transition_prob = env.step(action)\n",
    "    \n",
    "    print(\"Current State: {}\".format(current_state))\n",
    "    print(\"Action: {}\".format(action_map[action]))\n",
    "    print(\"Next State: {}\".format(next_state))\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    current_state = next_state\n",
    "    \n",
    "    # if the agent moves to hole state, then terminate\n",
    "    if done: \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rajan Adhikari"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "title": "Implementation of Value Iteration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
