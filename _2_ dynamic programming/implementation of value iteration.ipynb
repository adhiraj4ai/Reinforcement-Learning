{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:21:54.712857Z",
     "start_time": "2024-09-19T16:21:54.709756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# set seed\n",
    "SEED = 106"
   ],
   "id": "19aff06715285ff6",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:22:19.605364Z",
     "start_time": "2024-09-19T16:22:19.600852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode='ansi')"
   ],
   "id": "822faed246655ae8",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:22:20.149575Z",
     "start_time": "2024-09-19T16:22:20.146828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env.reset(seed=SEED)\n",
    "print(env.render())"
   ],
   "id": "1939223f94cf4230",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:22:20.923361Z",
     "start_time": "2024-09-19T16:22:20.918224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def value_iteration(env, num_of_iterations=100, gamma=1.0, threshold=1e-40) -> list:\n",
    "    \"\"\"\n",
    "    Value iteration algorithm to compute the value table.\n",
    "    :param env: environment for an agent\n",
    "    :param num_of_iterations: number of iterations\n",
    "    :param gamma: discount factor\n",
    "    :param threshold: threshold value to stop iterations\n",
    "    :return: value table\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "    \n",
    "    print('Number of states:', num_of_states)\n",
    "    print('Number of actions:', num_of_actions)\n",
    "    \n",
    "    # Initialize the value table with zeros for each state\n",
    "    value_table = np.zeros(num_of_states)\n",
    "    \n",
    "    # Perform value iteration for num_of_iterations\n",
    "    for i in range(num_of_iterations):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        # Compute q value for each state\n",
    "        for state in range(num_of_states):\n",
    "            # Initialize q values\n",
    "            q_values = []\n",
    "            \n",
    "            # For each action in the state, compute q value\n",
    "            for action in range(num_of_actions):\n",
    "                q_value = 0\n",
    "                # Loop through each transition (prob, next_state, reward, done)\n",
    "                for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                    # Compute Bellman backup\n",
    "                    bellman_backup = reward + gamma * updated_value_table[next_state]\n",
    "                    # Compute q value\n",
    "                    q_value += prob * bellman_backup\n",
    "                \n",
    "                # Append q value to the list of q values\n",
    "                q_values.append(q_value)\n",
    "            \n",
    "            # Update the value table with the maximum q value\n",
    "            value_table[state] = max(q_values)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
    "            print(\"Execution halted in iteration {}.\".format(i))\n",
    "            break\n",
    "                \n",
    "    return value_table\n"
   ],
   "id": "d38b2555a809950a",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:22:21.776327Z",
     "start_time": "2024-09-19T16:22:21.687777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print Value Table\n",
    "optimal_value_table = value_iteration(env, num_of_iterations = 1000)\n",
    "print(optimal_value_table)"
   ],
   "id": "62cda843340685b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of actions: 4\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:22:22.930634Z",
     "start_time": "2024-09-19T16:22:22.925519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_policy(env, value_table, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Extract the policy from the given value table.\n",
    "    :param env: environment for an agent\n",
    "    :param value_table: value table computed from value iteration\n",
    "    :param gamma: discount factor\n",
    "    :return: policy table\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_of_states = env.observation_space.n\n",
    "    num_of_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize policy as an integer array\n",
    "    policy = np.zeros(num_of_states, dtype=int)\n",
    "    \n",
    "    # Iterate through each state\n",
    "    for state in range(num_of_states):\n",
    "        q_values = []\n",
    "        \n",
    "        # For each action in the state, compute q value\n",
    "        for action in range(num_of_actions):\n",
    "            q_value = 0\n",
    "            \n",
    "            # Calculate q value using the transition probabilities\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[state][action]:\n",
    "                bellman_backup = reward + gamma * value_table[next_state]\n",
    "                q_value += prob * bellman_backup\n",
    "            \n",
    "            # Append q value to the list\n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        # Select the action with the highest q value\n",
    "        policy[state] = np.argmax(q_values)\n",
    "        \n",
    "    return policy\n"
   ],
   "id": "99441d78214d27f9",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:22:23.706528Z",
     "start_time": "2024-09-19T16:22:23.702336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimal_policy = extract_policy(env, optimal_value_table)\n",
    "print(optimal_policy)"
   ],
   "id": "84dff21280842d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:23:30.477055Z",
     "start_time": "2024-09-19T16:23:30.470678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's decode the action to be take in each state\n",
    "# map numbers to action\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"down\",\n",
    "    2: \"right\",\n",
    "    3: \"up\"\n",
    "}\n",
    "\n",
    "# Number of times the agent moves\n",
    "num_timestep = 100\n",
    "\n",
    "# Reset the environment\n",
    "current_state, info = env.reset(seed=SEED)\n",
    "\n",
    "for i in range(num_timestep):\n",
    "    print(\"------- Step: {} --------\".format(i+1))\n",
    "        # Let's take a random action now from the action space\n",
    "    # Random action means we are taking random policy at the moment.\n",
    "    action = optimal_policy[current_state]\n",
    "    \n",
    "    # # Take the action and get the new observation space\n",
    "    next_state, reward, done, info, transition_prob = env.step(action)\n",
    "    \n",
    "    print(\"Current State: {}\".format(current_state))\n",
    "    print(\"Action: {}\".format(action_map[action]))\n",
    "    print(\"Next State: {}\".format(next_state))\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    current_state = next_state\n",
    "    \n",
    "    # if the agent moves to hole state, then terminate\n",
    "    if done: \n",
    "        break"
   ],
   "id": "9a2bdc68475ad1da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Step: 1 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 2 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 3 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 4 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 5 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 6 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 7 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 8 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 9 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 10 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 11 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 12 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 13 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 14 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 15 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 16 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 17 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 18 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 19 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 20 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 21 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 22 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 23 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 24 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 25 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "------- Step: 26 --------\n",
      "Current State: 0\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 27 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "------- Step: 28 --------\n",
      "Current State: 4\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 29 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 30 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 13\n",
      "Reward: 0.0\n",
      "------- Step: 31 --------\n",
      "Current State: 13\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 32 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "------- Step: 33 --------\n",
      "Current State: 8\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "------- Step: 34 --------\n",
      "Current State: 9\n",
      "Action: down\n",
      "Next State: 10\n",
      "Reward: 0.0\n",
      "------- Step: 35 --------\n",
      "Current State: 10\n",
      "Action: left\n",
      "Next State: 14\n",
      "Reward: 0.0\n",
      "------- Step: 36 --------\n",
      "Current State: 14\n",
      "Action: down\n",
      "Next State: 15\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:10:26.227934Z",
     "start_time": "2024-09-19T16:10:26.226128Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cb7ca1f9051e850d",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ecd2f295736f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
