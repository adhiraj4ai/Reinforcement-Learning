{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e10714cb5d017e9",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44198bdc560a3b",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "Monte Carlo Techniques are particularly useful in such situation where traditional deterministic approach are impractical or impossible to apply. Monte Carlo is computational technique which rely on random sampling to obtain complex numerical results. They are very commonly used across various fields of Physics, Finance, Gaming, Engineering, Chemistry etc.\n",
    "\n",
    "As said above, the Monte Carlo method approximates the expectation of a random variable from sampling. And, when the sampling size is greater, the approximate value becomes better. Say, X be the random variable and we need to compute the expected value of X, such that we compute the sum of values of X multiplied by the respective probabilities and is given, matematically, as:\n",
    "\n",
    "\\begin{equation}\n",
    "E(X) = \\sum_{i=1}^{N}{x_ip(x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "However, we are trying to approximate it using Monte Carlo method by sampling the values of X for some large number N times and compute the expected value of X as:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{x \\sim p(x)}[X] \\approx 1/N\\sum_{i=1}^{N}{x_i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e9eb641732272",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods for RL\n",
    "\n",
    "Monte Carlo is a model-free methods in Reinforcement Learning and model free methods do not require the model dynamics of the environment. They provide ways of solving the RL problem based on averaging sample returns. Unlike Dynamic Programming technique, here we consider a learning method for estimating value functions and discovering optimal policies as we also do not assume complete knowledge of environment dynamics. Due to the lack of knowledge on dynamics of environment, this technique solely rely on experience.\n",
    "\n",
    "Here, We take sample sequences of states, actions and rewards from actual or simulated interaction with an environment by agent to predict on value function and finding optimal policies. Having said that, we still gonna need a model which needs only the sample distribution instead of complete probability distributions of all possible transitions.\n",
    "\n",
    "Monte Carlo methods are particularly suited for episodic tasks, where an episode is a sequence of states, actions, and rewards starting from an initial state and ending in a terminal state. The value of states or state-action pairs is estimated based on the average return (sum of rewards) received over many episodes.\n",
    "\n",
    "The fundamental concept underlying Monte Carlo methods revolves around estimating the value of states or state-action pairs by averaging the returns gained from various trajectories or episodes. This process involves computing the anticipated return for each state or state-action pair and adjusting the value estimates as more experience is accumulated.\n",
    "\n",
    "When using Monte Carlo in RL, we sample and take average returns for each state-action pair as there are multiple states in any given problems. Then the return after taking an action in preceding state depends on the actions taken in the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdd0db669c419e",
   "metadata": {},
   "source": [
    "## Advantages of Monte Carlo Methods Over Dynamic Programming\n",
    "* MC are useful to learn optimal behaviour directly from interaction with the environment without the need of environment dynamics.\n",
    "* MC can be used with simulation or sample models or real environment whereas DP requires explicit model of transition probabilities.\n",
    "* It is easy and efficient to focus MC on small required subset of the states without evaluating on the rest of the states set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521ced7b593c29b",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction\n",
    "`MC Prediction` is a method used to estimate the value function of a given policy while using Monte Carlo method. As we already know the vlaue function represents the expected cumulative reward an agent can receive by following policy $\\pi$. \n",
    "\n",
    "In Monte Carlo Prediction, an agent generates complete episodes by following some policy and through interaction with the environment. Each episodes starts from some initial state and ends at the terminal state. Agent, then, computes the returns for each state in the episode. The concept behind prediction in Monte Carlo is to use the average of the returns as an estimate of the value funciton for the corresponding states. As the agent generates more and more episodes, the estimate of the value function becomes more and more accurate (Statistically, the law of large number)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d51a9f3952dbc",
   "metadata": {},
   "source": [
    "### MC Prediction of State Values\n",
    "\n",
    "As Monte Carlo is a learning technique involving estimating value functions and discovering optimal policies, we use the Monte Carlo Prediction for it. The value function represents the expected cumulative reward that an agent can receive by following a particular policy, starting from a given state. In Monte Carlo Methods, an episode refers to a sequence of states, actions and rewards that an agent experiences through interaction with the environment. The episode starts from an initial state and continues until terminal state. \n",
    "\n",
    "The key idea behind Monte Carlo Prediction is to use actual cumulative rewards obtained from complete episodes to estimate the value function in contrast to using a recursive relationship as in dynamic programming methods.\n",
    "\n",
    "An obvious way to estimate it from experience, is to simply average the returns observed after visits to that state.\n",
    "\n",
    "A state s may be visited multiple times in the same episode. Thus, there are two approaches commonly used.\n",
    "\n",
    "1. **First Visit Monte Carlo (FVMC):** In the first Visit Monte Carlo method, the value function estimate for a state is updated based on the return observed only from the first time visit of that state. Any subsequent visit of the same state within the same episode are ignored. It es\n",
    "    \n",
    "2. **Every Visit Monte Carlo (EVMC):** In the every visit Monte Carlo method, the value function estimate for a state is updated based on the return observed from all the visit of that state within an episode. Thus, all the subsequent visit of the state within an episode contributes to the value function estimate.\n",
    "    \n",
    "The major difference, from the above, between FVMC and EVMC lies on how multiple visits of same state within a same episode are considered. However, th preference between FVMC and EVMC depends on the specific problem and the trade-off between computation efficiency and convergence speed. FVMC is often preferred for episodic tasks with relatively short episodes, while EVMC can be more suitable for continuing tasks or problems with long episodes where revisiting states is common.\n",
    "\n",
    "Hereâ€™s how Monte Carlo Prediction works, in generalized fashion:\n",
    "1. The agent generates episodes by interacting with the environment following a given policy.\n",
    "2. For each episode, the agent computes the cumulative return for each visited state. The return is calculated as the sum of rewards received after visiting that state until the end of the episode. \n",
    "3. The value function for a visited state is estimated by averaging the returns obtained from all the visits to that state across multiple episodes.\n",
    "\n",
    "\\begin{equation}\n",
    "    V(S) = 1/N * \\sum_{i=1}^N{G(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Where, G(i) is the return observed after i-th visit to states S.\n",
    "\n",
    "4. With more episodes, the Monte Carlo estimate of value function converge to actual value function for the given policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8eb182ffe9ca5",
   "metadata": {},
   "source": [
    "### Monte Carlo Prediction of Action Values\n",
    "\n",
    "If a model of the environment is available, state values are enough to determine a policy, which is exactly we did in Dynamic Programming. But without a model, state values alone are not sufficient, and it is particularly useful to estimate action values. One must explicitly estimate the value of each action for an agent in a state to find optimal policy. Thus, with Monte Carlo we aim to estimate $Q*$. Policy evaluation problem for action values is to estimate A_\\pi(s, a), the expected return when starting in state $s$, taking action and thereafter following policy $\\pi$. \n",
    "\n",
    "A state-action pair ($s$, $a$) is said to be visited in an episode if ever the state $s$is visited and action $a$ is taken being in state $s$. Similar to state values, this give rise to two scenarios - **First Visit MC method** and **Every Visit MC method**. The **FVMC** method computes the average returns only after the first occurrence of visiting a state and selecting an action within each episode. Succeeding visit of state-action pairs are completely ignored. In contrast, the **EVMC** method computes the value of state-action pair by averaging the returns observed after every instance it is visited throughout all episodes.  These method converge quadratically to the true expected values as the number of visits to each-action pair becomes very large.\n",
    "\n",
    "In deterministic policies, where actions are chosen consistently for each state, many state-action pairs may never be visited, making it impossible to improve estimates for those actions. This poses a serious problem because the purpose of learning action values is to help in choosing among all possible actions in each state, not just the favored one. This is the general problem of exploration vs exploitation (or maintaining exploration ). In order to insure continual exploration and approximately accurate evaluation of action values, one approach is to guarantee that all state-action pairs are visited infinitely often by starting episodes from randomly chosen state action pairs i.e. every state-action pair has some non-zero probability of being selected as initial of episode. This strategy is known as exploring starts. However, it is not always practically possible to control the starting conditions for some environment, particularly when learning directly from action interaction with an environment.\n",
    "\n",
    "The most common alternative approach to ensure encountering all state-action pairs is to employ stochastic policies, where thereâ€™s a chance of selection any action in each state. This approach atleast guarantees exploration by allowing every action to be chosen with some probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb58771692c5d77",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "`MC Control` method is used to find an optimal policy in Monte Carlo. The goal of Control method is to find an optimal control policy for an agent to act in the environment. Rather than just  predicting or estimating the value function for a given policy, it focus on the goal of finding an optimal control policy, thus called `Control`. The agent aims to learn an optimal policy through iterative policy evaluation and policy improvement steps. The policy evaluation step estimates the value function for the current policy, which represent the expected future reward for each state when following that policy. THe policy improvement step then updates the policy by choosing actions which maximize the expected future reward in each state. By repeating this iterative process of policy evaluation and policy improvement, the agent gradually refines its policy, ultimately converging to optimal policy that maximizes the expected long term reward. \n",
    "\n",
    "Unlike MC Prediction, here, we are not given any policy. Thus, we start off by initializing a random policy and use it to generate an episode in the first iteration. Based on this we compute the Q value as the average return of state-action pair and we extract a new policy. With the new policy, we proceed with f\n",
    "\n",
    "There are two types of control:\n",
    "1. On-Policy Control\n",
    "2. Off-Policy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d7b4f55b307a9",
   "metadata": {},
   "source": [
    "### On-Policy Control\n",
    " In the Monte Carlo control algorithm, the agent learns the optimal policy by simultaneously following and updating the same policy during the learning process. This method directly evaluates and improves the policy that the agent uses to make decisions, meaning the learning agent gains experience from its current policy. Consequently, the behavior policy and the target policy are different iterations of the same policy. This approach ensures that the target policy is at least as good as, if not better than, the behavior policy, thereby gradually refining the agent's decision-making over time.\n",
    " \n",
    "There are two types of on-policy control methods:\n",
    "1. Monte Carlo Exploring Starts\n",
    "2. Monte Carlo with $\\epsilon$-Greedy Policy\n",
    "\n",
    "Let's explore both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a479ce02531fc4",
   "metadata": {},
   "source": [
    "#### Monte Carlo Exploring Starts\n",
    "Monte Carlo methods involve learning directly from experience, where an episode is the sequence of states, actions and rewards achieve through interaction of agent and environment. The episodes begins through some initial state and goes through various states till terminal state through the interaction of agent and environment. \n",
    "\n",
    "**Exploring Starts** is a RL concept where each episodes starts from different initial values in order to explore the environment thoroughly. In RL, an agent learns to make decisions by interacting with an environment by maximizing some cumulative reward. If the agent always starts from the same initial state, it might not explore the full range of states and actions available across different parts of the environment. It ensures that an agent interacts with various parts of the environments and experience various situation of the environment. This helps agent to learn a more comprehensive understanding of the environment and make a robust decision. \n",
    "\n",
    "It involves:\n",
    "\n",
    "1. **Random Initialization** of initial state and a randomly chosen action format hat state\n",
    "2. **Exploration** of all possible state-action pairs, thereby avoiding scenarios where some pairs are never visited.\n",
    "3. Overtime, through multiple episodes, an agent experience all aspects (**Coverage**) of state-action space, leading to robust estimates to reach to the goal.\n",
    "\n",
    "Now, let's go through the algorithm of Monte Carlo Exploring Starts:\n",
    "\n",
    "In this approach, the agent starts learning from various initial states. Exploring starts help agent to learn robust policy that performs well in various situation and may start from different initial states.  Its working mechanism follows as below:\n",
    "\n",
    "---\n",
    "\n",
    "###### Algorithm - Monte Carlo with Exploring Starts\n",
    "\n",
    "---\n",
    "\n",
    "1. **Initialize $Q$-values**: Initialize the state-action value function $Q(s,a)$ arbitrarily for all states $s$ and actions $a$.\n",
    "2. **Initialize the Policy:** Initialize the policy $\\pi$ arbitrarily for all states $s$.\n",
    "3. **Exploring Start:** Ensure each state-action pair $(s,a)$ is visited by starting each episode with a randomly chosen state-action pair.\n",
    "4. **Generate episodes:** Generate episodes by based on current policy.\n",
    "5. **Process Episode:** For each episode:\n",
    "    1. **Random Start:** Select a random state-action pair $(s_0, a_0)$ from the episode to start processing.\n",
    "    2. **Compute Returns:** For each time step $t$, calculate the return $G_t$, the cumulative discounted reward from time step $t$ to the end of the episode:  \n",
    "           \n",
    "    \\begin{equation}\n",
    "      G_t = R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 R_{t+3}+....... = \\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+1}}\n",
    "    \\end{equation}\n",
    "    \n",
    "    3. **Update Q-Values:** Update the state-action value.\n",
    "    \n",
    "    \\begin{equation}\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha[G_t - Q(s_t, a_t)]\n",
    "    \\end{equation}\n",
    "    \n",
    "   where \\alpha is the learning rate whose value lies between 0 and 1.\n",
    "\n",
    "    4. **Policy Improvement:** Improve the policy to be greedy with respect to updated state-action value.\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\pi(s) = arg max _a Q(s, a)\n",
    "    \\end{equation}\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9301353196deb",
   "metadata": {},
   "source": [
    "#### Monte Carlo with $\\epsilon$-Greedy Policy\n",
    "In this approach, the agent starts learning from a fixed initial state or a set of predefined initial states. The agent generates episodes by following a fixed policy (e.g., the current estimate of the optimal policy) from these initial states. The agent updates its value function and policy based on the complete episodes, using techniques like Monte Carlo policy evaluation and policy improvement. This method can be effective when the problem domain has a well-defined starting point or set of starting points.\n",
    "\n",
    "Let's now understand the concept of greedy policy before we use epsilon-greedy policy.\n",
    "\n",
    "A policy that selects the best action available at the moment is known as a greedy policy. For example in any state we have an option to choose left, right, up and down in grid world and value for these actions is 3, 4, 2, 1 respectively. The greedy policy selects the best action having maximum Q value i.e. *right*. Problem with this approach is that agent never explores the other possible actions. But it is likely that the other possible action may lead to optimal policy. So, this gives rise to the question whether the agent should explore other actions as well or follow the same action with maximum $Q$ value always. This is known as the ***Exploration-Exploitation problem***. \n",
    "\n",
    "The exploration vs. exploitation problem is a fundamental trade-off in reinforcement learning that arises when an agent needs to balance between exploring new actions or states to gather more information about the environment, and exploiting the current knowledge to maximize the expected reward based on what it has already learned.\n",
    "\n",
    "> **Exploration:**\n",
    "Exploration refers to the process of trying new actions or visiting unexplored states in the environment. This is necessary because the agent initially has incomplete knowledge about the environment dynamics and rewards. By exploring, the agent can discover new information, potentially leading to better policies or higher rewards in the long run.\n",
    "\n",
    "> **Exploitation:**\n",
    "Exploitation refers to the process of utilizing the agent's current knowledge to select actions that are expected to maximize the reward based on the information gathered so far. This is important because the agent's objective is to maximize its cumulative reward, and exploiting the current knowledge can lead to higher rewards in the short term. \n",
    "\n",
    "The exploration vs. exploitation dilemma arises because these two objectives are often in conflict. If the agent explores too much, it may miss out on opportunities to exploit its current knowledge and maximize immediate rewards. On the other hand, if the agent exploits too much, it may get stuck in a sub-optimal policy or fail to discover potentially better actions or states.\n",
    "\n",
    "To avoid this, among various technique, we use MC technique without exploration starts which decides on exploration and exploitation probabilistically. This is known as **MC with $\\epsilon$-greedy policy** where all actions are assigned some non-zero probability i.e. different actions other than one with maximum $Q$ value are assigned $\\epsilon$ probability value and action with maximum $Q$ value is assigned $1-\\epsilon$ probability. With this setup we always end up making a choice between exploration, with probability of $\\epsilon$, and exploitation with probability of $1-\\epsilon$. In this approach, the agent starts learning from a fixed or predefined initial state or a set of predefined initial states. It works as follows:\n",
    "\n",
    "---\n",
    "\n",
    "###### Algorithm - Monte Carlo with $\\epsilon$-Greedy Policy\n",
    "\n",
    "---\n",
    "\n",
    "1. **Initialize $Q$-values**: Initialize the state-action value function $Q(s,a)$ arbitrarily for all states $s$ and actions $a$.\n",
    "\n",
    "2. **Define Exploration Strategy:** Choose an exploration strategy, such as an epsilon-greedy policy where with probability $Ïµ$ ,a random action is chosen, and with probability $1 âˆ’ Ïµ$, the action with the highest $Q$-value is chosen.\n",
    "\n",
    "3. **Generate episodes:** Generate episodes by based on current policy.\n",
    "\n",
    "4. **Process Episode:** For each episode:\n",
    "\n",
    "    1. **Random Start:** Select a random state-action pair $(s_0, a_0)$ from the episode to start processing.\n",
    "    2. **Compute Returns:** For each time step $t$, calculate the return $G_t$, the cumulative discounted reward from time step $t$ to the end of the episode:  \n",
    "           \n",
    "    \\begin{equation}\n",
    "      G_t = R_{t+1}+ \\gamma R_{t+2} + \\gamma^2 R_{t+3}+....... = \\sum_{k=0}^{T-t-1}{\\gamma^k R_{t+k+1}}\n",
    "    \\end{equation}\n",
    "    \n",
    "    3. **Update Q-Values:** Update the state-action value.\n",
    "    \n",
    "    \\begin{equation}\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha[G_t - Q(s_t, a_t)]\n",
    "    \\end{equation}\n",
    "\n",
    "    4. **Policy Improvement:** Improve the policy to be greedy with respect to updated state-action value.\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\pi(s) = arg max _a Q(s, a)\n",
    "    \\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "In the $\\epsilon$-greedy policy, if $\\epsilon$ is 0, then the policy becomes greedy which only chooses exploitation, and when $\\epsilon$ is 1, then the policy only chooses exploration. Hence, the value of policy must be choosen optimally between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae2078548e058d",
   "metadata": {},
   "source": [
    "### Off-Policy Control\n",
    "In this, the agent learns the optimal policy from the experience generated by some other policy. This method evaluate and improve a different policy from the one used to make current decisions i.e. the learning algorithm learns from any other policy. Thus, the behaviour policy and target policy are different in this type of learning. These can be more sample efficient because they can reuse the data collected from various policies. But, as a cons, this may lead to the complexity or distributional shift.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f7af7b69f6797c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
