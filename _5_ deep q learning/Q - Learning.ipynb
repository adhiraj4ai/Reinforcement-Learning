{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T20:22:59.950481Z",
     "start_time": "2024-06-24T20:22:59.947468Z"
    }
   },
   "source": [
    "#import required libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set seed\n",
    "SEED = 106"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:22:59.955216Z",
     "start_time": "2024-06-24T20:22:59.952004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\n",
    "    'FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode = 'ansi'\n",
    ")\n",
    "env.reset(seed=SEED)\n",
    "print(env.render())"
   ],
   "id": "6785088fc7bde621",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:22:59.963468Z",
     "start_time": "2024-06-24T20:22:59.956590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env, alpha = 0.01, gamma = 0.9, epsilon=0.9):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Learning Agent agent.\n",
    "        \n",
    "        :param env: The environment to learn from.\n",
    "        :type env: gym.Env\n",
    "        :param alpha: The learning rate.\n",
    "        :type alpha: float\n",
    "        :param gamma: The discount factor.\n",
    "        :type gamma: float\n",
    "        :param epsilon: The exploration rate for the epsilon-greedy policy.\n",
    "        :type epsilon: float\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = self.init_q_values()\n",
    "        \n",
    "    def init_q_values(self):\n",
    "        \"\"\"\n",
    "        Initialize the Q-values to zero for all state-action pairs.\n",
    "        \n",
    "        :return: A dictionary with state-action pairs as keys and Q-values as values.\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        q = {}\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                q[(state, action)] = 0.0\n",
    "        return q\n",
    "    \n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using the epsilon-greedy policy.\n",
    "        \n",
    "        :param state: The current state.\n",
    "        :type state: int\n",
    "        :return: The selected action.\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(\n",
    "                range(self.env.action_space.n), key=lambda x: self.q_values[(state, x)]\n",
    "            )\n",
    "    \n",
    "    def compute_policy(self, num_of_timesteps, num_of_episodes):\n",
    "        \"\"\"\n",
    "        Learn the policy by running SARSA for a specified number of episodes and timesteps.\n",
    "        \n",
    "        :param num_of_timesteps: The number of timesteps in each episode.\n",
    "        :type num_of_timesteps: int\n",
    "        :param num_of_episodes: The number of episodes to run.\n",
    "        :type num_of_episodes: int\n",
    "        :return: DataFrame containing Q-values for each episode.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        q_values_df = pd.DataFrame()\n",
    "\n",
    "        for i in range(num_of_episodes):\n",
    "            # Initialize states\n",
    "            state, _ = self.env.reset()\n",
    "            \n",
    "            for t in range(num_of_timesteps):\n",
    "                # Select the action using epsilon-greedy policy\n",
    "                action = self.epsilon_greedy(state)\n",
    "                \n",
    "                # Take the action and observe the next state and reward\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                \n",
    "                # Select next action for the next state using -greedy policy\n",
    "                next_action = np.argmax(\n",
    "                    [self.q_values[(next_state, a)] for a in range(env.action_space.n)]\n",
    "                )\n",
    "                \n",
    "                # Update Q-value\n",
    "                self.q_values[(state, action)] += self.alpha * (\n",
    "                    reward + self.gamma * self.q_values[(next_state, next_action)] - self.q_values[(state, action)]\n",
    "                )\n",
    "                \n",
    "                # Move to the next state and action\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Store Q-values in DataFrame\n",
    "            q_values_df = pd.concat([q_values_df, pd.DataFrame(self.q_values, index=[i])])\n",
    "        \n",
    "        return q_values_df"
   ],
   "id": "e98c34f141524baa",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T20:23:09.147939Z",
     "start_time": "2024-06-24T20:22:59.964364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "alpha = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.5\n",
    "num_of_episodes = 100000\n",
    "num_of_timesteps = 100\n",
    "\n",
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)\n",
    "env.reset()\n",
    "\n",
    "# Create SARSA model\n",
    "model = QLearning(env, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "# Compute policy and store Q-values\n",
    "q_values_df = model.compute_policy(num_of_timesteps, num_of_episodes)"
   ],
   "id": "dd90336ce92fc4de",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate average Q-values over episodes\n",
    "avg_q_values = q_values_df.mean(axis=1)\n",
    "\n",
    "# Plot the average Q-values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(avg_q_values.index, avg_q_values.values, label='Average Q-value', color='r')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q-value')\n",
    "plt.title('Average Q-value Evolution Over Episodes through Q-Learning')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c4eea1799bc99d19",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "57c63afbc2248eb7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the learned policy\n",
    "policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "for state in range(env.observation_space.n):\n",
    "    policy[state] = np.argmax([model.q_values[(state, a)] for a in range(env.action_space.n)])\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "print(policy.reshape((4, 4)))  # Reshape according to FrozenLake's 4x4 grid"
   ],
   "id": "701bcc2ef84f5f19",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the learned Q-values\n",
    "print(\"Learned Q-values:\")\n",
    "for state in range(env.observation_space.n):\n",
    "    for action in range(env.action_space.n):\n",
    "        print(f\"Q[{state}, {action}] = {model.q_values[(state, action)]:.2f}\")"
   ],
   "id": "c1667340c56cb80d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get learned Policy\n",
    "policy = policy.reshape((4, 4))\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "\n",
    "# Function to run an episode\n",
    "def run_episode(env, policy):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    \n",
    "    while not done:\n",
    "        frames.append(env.render())\n",
    "        action = policy[state]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Run an episode\n",
    "frames = run_episode(env, policy)\n",
    "\n",
    "# Display the animation\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(frames[0])\n",
    "\n",
    "def animate(i):\n",
    "    img.set_data(frames[i])\n",
    "    return (img,)\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "anim = FuncAnimation(fig, animate, frames=len(frames), interval=500, blit=True)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ],
   "id": "3ca18268b496cbe6",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fab476e782e668c",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
