{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Value Function\n",
    "\n",
    "The value function refers to the value of the state. The value of the state is the return obtained by agent from the state by following policy $\\pi$. It is denoted by $V(s)$ and is given as:\n",
    "\n",
    "Using Deterministic Policy, where the particular state has only one action to be taken,\n",
    "\n",
    "$$\n",
    "V^\\pi {(s)} = \\mathbb{E}[R(\\tau|s_0 = s]\n",
    "$$\n",
    "\n",
    "and using Stochastic Policy, where the action has probabilistic distribution in particular state,\n",
    "\n",
    "$$\n",
    "V^\\pi {(s)} = \\mathbb{E}[R(\\tau|s_0 = s]\n",
    "$$\n",
    "\n",
    "which means the expected return obtained by following the trajectory ($\\tau$) guided by the policy ($\\pi$) with the starting state (s). The reason behind expected return, while using stochastic policy, is that the return is the random variable as it takes different values determined by probability distribution over the action space. Thus, expected return is basically the weighted average of the returns."
   ],
   "id": "8b2d63c588facd09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Optimal Value Function\n",
    "The value function depends on the policy i.e. there can be different value functions determined by different policies. Some policy are likely to return good value function and some worse. Thus, the optimal value function of the state, denoted by $V^*(s)$, is the maximum value function among all the value function determined by examining various policies. It is given as:\n",
    "\n",
    "$$\n",
    "V^*(s) = max_\\pi V^\\pi(s)\n",
    "$$\n",
    "\n",
    "Here, $^*$ denotes the optimality.\n",
    "\n",
    "The policy that yields the optimal value function is known as **Optimal Policy** denoted by $\\pi^*$."
   ],
   "id": "f5fd311cd7a7c707"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# $Q$ Function\n",
    "A $Q$ function is the value obtained from the state-action pair. The value of state-action pair is the value obtained by the agent as a return from the state $s$ by performing action a while following through policy $\\pi$. It is denoted by $Q(s,a)$ and given as:\n",
    "\n",
    "Using deterministic policy,\n",
    "$$\n",
    "Q^\\pi(s, a) = [R(\\tau)|s_0 = s, a_0 =a]\n",
    "$$\n",
    "\n",
    "and using stochastic policy,\n",
    "$$\n",
    "Q^\\pi(s, a) = \\mathbb{E}[R(\\tau)|s_0 = s, a_0 =a]\n",
    "$$\n",
    "\n",
    "It implies that the $Q$ value is the expected return the agent would obtain starting from state $s$ and performing action $a$ following policy $\\pi$."
   ],
   "id": "6ec452d56c5ecc5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Optimal $Q$ Function\n",
    "The optimal $Q$ function is the one with maximum $Q$ value over all other $Q$ functions, denoted by $Q^*(s,a)$ and is given as:\n",
    "$$\n",
    "Q^*(s,a) = max_\\pi Q^\\pi(s,a)\n",
    "$$\n",
    "\n",
    "The policy that yields the optimal $Q$ function is known as **Optimal Policy** denoted by $\\pi^*$."
   ],
   "id": "6d367eb0e91a8fb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Difference with Value Function\n",
    "The key distinction between a value function and a $Q$ function lies in what they compute: a value function evaluates the expected return of a state, while a $Q$ function assesses the value of a state-action pair."
   ],
   "id": "285b34d1bc92690f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
