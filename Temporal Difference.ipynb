{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759268c70b16c73a",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD)\n",
    "Temporal Difference is most popular model-free methods in Reinforcement Learning. It combines the advantages of both Dynamic Programming and Monte Carlo techniques. Like as in DP, we perform bootstrapping and just like MC, it is a model-free technique. TD learning updates value estimates based on other learned estimates, without waiting for a final outcome. It bootstraps, meaning it bases its update in part on an existing estimate. TD learning methods are used to estimate value functions and improve policies based on these estimates. Unlike Monte Carlo methods, which wait until the end of an episode to update the value estimates, TD learning updates the value estimates at each time step.\n",
    "\n",
    "Like as Monte Carlo, TD also consists of prediction and control. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb8ccaae30f6e9",
   "metadata": {},
   "source": [
    "## Temporal Difference Prediction\n",
    "\n",
    "TD Prediction is used to estimate the value function $V(s)$ of a given policy $\\pi(s)$. The value function represents the expected cumulative reward starting from state $s$ and following policy $\\pi$ thereafter. In the TD prediction method, the policy is given as input and we try to estimate the value function using the given policy. \n",
    "\n",
    "Given a policy $\\pi$, both MC and TD update their estimate V for the non-terminal states $s_t$. MC update can be given for non-stationary environments as:\n",
    "\n",
    "\\begin{equation}\n",
    "V(s_t) \\leftarrow V(s_t) + \\alpha[G_t - V(s_t)]\n",
    "\\end{equation}\n",
    "\n",
    "where $G_t$ is the actual return following time $t$, and $\\alpha$ is the learning rate which is constant size parameter. This is called $constant-\\alpha$ MC. The $G_t$ is known only after the end of the episode. Recall the computation of $G_t$ as:\n",
    "\n",
    "$$\n",
    "G_t = R_t + \\gamma R_(t+1) + \\gamma^2 R_(t+2) + ...\n",
    "$$\n",
    "\n",
    "However, TD methods need to wait only until the next step. So, the simplest TD makes the update\n",
    "\n",
    "\\begin{equation}\n",
    "V(s_t) \\leftarrow V(s_t) + \\alpha[R_(t+1) + \\gamma V(S_(t+1)) - V(s_t)]\n",
    "\\end{equation}\n",
    "\n",
    "immediately on next step $S_(t+1)$ and receiving $R_(t+1)$. This TD method is called $TD(0)$, or one-step TD. This is the special case of $TD(\\lambda)$, which we will discuss later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423eafa7ad8b24b8",
   "metadata": {},
   "source": [
    "Here, the quantity within the brackets [] i.e. $R_(t+1) + \\gamma V(S_(t+1)) - V(s_t)$ measures the discrepancy between the estimated value of $s_t$ and the better estimate $R_(t+1) + \\gamma V(S_(t+1))$. This is known as ***TD Error***. Note that, the TD error at each time is the error in the estimate made at that time. It is represented by $\\delta_t$ and written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_t = R_(t+1) + \\gamma V(S_(t+1)) - V(s_t)\n",
    "\\end{equation}\n",
    "or,\n",
    "\\begin{equation}\n",
    "\\delta_t = R_(t+1) + \\gamma Q(s_(t+1), a_(t+1)) - Q(s_t, a_t)\n",
    "\\end{equation}\n",
    "\n",
    "Since, the TD error depends on the next state and next reward, it is available only after one time step later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cbcf04aef59bc",
   "metadata": {},
   "source": [
    "## Temporal Difference Control\n",
    "TD Control, or Temporal Difference Control, extends the ideas of TD Prediction to learn not just the value function, but also the optimal policy in reinforcement learning. It combines the TD learning approach with control to find the best actions an agent should take in each state. The goal of TD Control is to learn the optimal policy that maximizes the cumulative reward in a Markov Decision Process (MDP) by directly estimating the value functions and improving the policy over time. Thus, TD aims to learn the optimal action-value function $Q*(s, a)$ and, consequently, the optimal policy $\\pi*(s)$, by iteratively improving estimates of aciton values using TD methods.\n",
    "\n",
    "TD Control methods are iterative and involve updating estimates of value functions based on the observed rewards and estimated future values, without needing to wait for the end of an episode (unlike Monte Carlo methods).\n",
    "\n",
    "There are two primary TD Control Methods\n",
    "1. SARSA\n",
    "2. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af78e654547452e",
   "metadata": {},
   "source": [
    "### TD Control using SARSA\n",
    "Like as MC, we will encounter the trade off between exploration and exploitation in TD. Thus, we again have two approaches of control viz. On-Policy and Off_policy. SARSA is an on-policy TD control method. \n",
    "\n",
    "Temporal Difference (TD) methods can be applied to learn action-value functions, extending the concept we previously discussed for state value functions. In on-policy methods, our goal is to estimate $q_\\pi(s, a)$ - the action-value function for the current behavior policy $\\pi$ - for all possible states $s$ and actions $a$.\n",
    "\n",
    "While we earlier focused on state-to-state transitions to learn state values, we now shift our attention to transitions between state-action pairs to learn the values of these pairs. This shift is natural because both approaches fundamentally deal with Markov chains that have an associated reward process.\n",
    "\n",
    "The key difference is that instead of estimating the expected return from being in a particular state, we're now estimating the expected return from taking a specific action in a particular state and then following the current policy. This action-value focus is particularly useful for control problems where we need to decide which actions to take in each state.\n",
    "\n",
    "This approach allows us to directly learn the value of actions, which is crucial for improving the policy. By estimating action-values, we can make informed decisions about which actions are most promising in each state, facilitating the process of policy improvement in reinforcement learning algorithms.  The update of state-action pairs, thus, can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "This update is done after every transition from a non-terminal state $s_t$. If $s_{t+1}$ is a terminal state, then Q(s_{t+1}, a_{t+1}) is defined as 0. Since this update rule is the transition form one state-action pair to the next state-action pair it can be represented using quintuple of events as $(s_t, a_t, r_t,, s_{t+1}, a_{t+1})$. This is where the approach is named Sarsa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bcf6b1294a7bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Algorithm - SARSA (State - Action - Reward - State - Action)\n",
    "\n",
    "---\n",
    "1. **Initialize Q-Values:** Initialize the state-action value function $Q(s, a)$ arbitrarily for all states $s$ and actions $a$.\n",
    "2. **Initialize the Policy:** Initialize the policy $\\pi$ arbitrarily.\n",
    "3. **Generate Episodes:** Generate episodes based on the current policy $\\pi$.\n",
    "4. **For each episode, Loop until Convergence**,\n",
    "    1. **Random Start:** Select a random initial state $s_0$.\n",
    "    1. **Select Action:** Chose the first action $a_0$ using policy derived from Q, say $\\pi$.\n",
    "    1. Loop for each episode:\n",
    "       * **Select Action:** Choose the first action $a_t$ using policy derived from Q, say $\\pi$.\n",
    "       * **Take Action:** execute action a_t and observe reward $r_t$, $s_{t+1}$.\n",
    "       * **Update Q-value:** Update the Q-value for the current state-action pair $(s_t, a_t)$\n",
    "       $$\n",
    "       Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [R_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "       $$\n",
    "       * **Policy Improvement:** Update the policy $\\pi$ with respect to the udpated Q-values. \n",
    "       $$\n",
    "            \\pi(s_t) = \\arg \\max_{a}Q(s_t,a)\n",
    "       $$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25007852cca6d31",
   "metadata": {},
   "source": [
    "The convergence properties of the SARSA algorithm depend on the nature of the policy's dependence on Q. For example, we could possibly use $\\epsilon$-greedy or $\\epsilon$-soft policies. SARSA converges with probability 1 to an optimal policy and action-value function as long as all state-action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy. Because SARSA updates the Q-values based on the actions actually taken, it tends to converge more safely in scenarios where the environment is stochastic. This is because it accounts for the actual behavior policy, including exploratory actions.\n",
    "\n",
    "SARSA balances exploration and exploitation by following the same policy for both generating behavior and updating Q-values. This means it considers the exploration actions when updating the Q-values.\n",
    "\n",
    "This SARSA algorithm follows an on-policy TD control approach, ensuring that the policy being evaluated and improved is the same policy used to generate actions during training. This makes SARSA well-suited for learning in environments where the policy must be followed throughout training, such as in scenarios with safety constraints or when exploring dangerous or costly actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab8c870fc056f8",
   "metadata": {},
   "source": [
    "### TD using Q-Learning\n",
    "It aims to find the optimal action-selection policy that maximizes the cumulative reward. Unlike SARSA, which is an on-policy algorithm, Q-Learning updates its Q-values based on the best possible action (greedy action) that can be taken from the next state, regardless of the policy that was actually followed to get there.\n",
    "\n",
    "Q-Learning is an off-policy TD control algorithm, meaning it learns the value of the optimal policy independently of the agent's actions. This allows Q-Learning to converge to the optimal policy even if the agent is following a different policy during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c585c8f831bdbe4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Algorithm - Q-Learning\n",
    "\n",
    "---\n",
    "1. **Initialize Q-Values:** Initialize the state-action value function $Q(s, a)$ arbitrarily for all states $s$ and actions $a$.\n",
    "2. **Initialize the Policy:** Initialize the policy $\\pi$ arbitrarily.\n",
    "3. **Generate Episodes:** Generate episodes based on the current policy $\\pi$.\n",
    "4. **For each episode, Loop until Convergence**,\n",
    "    1. **Random Start:** Select a random initial state $s_0$.\n",
    "    1. **Loop for each episode, loop until convergence**:\n",
    "       * **Select Action:** Choose the first action $a_t$ using an exploratory policy derived from Q, say $\\pi$.\n",
    "       * **Take Action:** execute action a_t and observe reward $r_t$, $s_{t+1}$.\n",
    "       * **Update Q-value:** Update the Q-value for the current state-action pair $(s_t, a_t)$\n",
    "       $$\n",
    "       Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [R_{t+1} + \\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_t=, a_t)]\n",
    "       $$\n",
    "       * **Policy Improvement:** Update the policy $\\pi$ with respect to the updated Q-values. \n",
    "       $$\n",
    "            \\pi(s_t) = \\arg \\max_{a}Q(s_t,a)\n",
    "       $$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb22a1e639eee49",
   "metadata": {},
   "source": [
    "Q-Learning explicitly separates exploration and exploitation by using the greedy action for updating Q-values, even if the agent follows an exploratory policy for generating behavior. Q-Learning typically converges faster to the optimal policy because it always updates the Q-values using the best possible action. However, in highly stochastic environments, this can sometimes lead to less stable learning because it doesn't account for the exploration actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c268b3f9f1cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
